{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f2aaf9",
   "metadata": {},
   "source": [
    "# Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306cb01",
   "metadata": {},
   "source": [
    "### Scrape code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL FIX: Use naive datetime for cutoff to avoid comparison errors\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from time import sleep\n",
    "import time\n",
    "import json\n",
    "\n",
    "# FIXED: Use naive datetime (no timezone) for consistent comparisons\n",
    "CUTOFF_DATE = datetime(2025, 10, 1, 0, 0, 0)  # Removed tzinfo=timezone.utc\n",
    "\n",
    "CATEGORIES = {\n",
    "    'Stocks': 'https://finance.yahoo.com/markets/stocks/most-active/',\n",
    "    'Crypto': 'https://finance.yahoo.com/markets/crypto/all/',\n",
    "    'Currencies': 'https://finance.yahoo.com/markets/currencies/',\n",
    "    'Private companies': 'https://finance.yahoo.com/markets/private-companies/highest-valuation/',\n",
    "    'Treasury bond': 'https://finance.yahoo.com/markets/bonds/'\n",
    "}\n",
    "\n",
    "def normalize_datetime(dt):\n",
    "    \"\"\"Convert timezone-aware datetime to naive (remove timezone info)\"\"\"\n",
    "    if dt is None:\n",
    "        return None\n",
    "    if isinstance(dt, str):\n",
    "        return dt\n",
    "    if hasattr(dt, 'replace'):\n",
    "        return dt.replace(tzinfo=None)\n",
    "    return dt\n",
    "\n",
    "def parse_date(date_str):\n",
    "    \"\"\"Parse date string and normalize to naive datetime\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    \n",
    "    date_str = date_str.strip()\n",
    "    \n",
    "    formats = [\n",
    "        \"%a, %B %d, %Y at %I:%M %p GMT%z\",\n",
    "        \"%a, %B %d, %Y at %I:%M %p\",\n",
    "        \"%B %d, %Y\",\n",
    "        \"%a, %B %d, %Y\",\n",
    "        \"%Y-%m-%d\",\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        fixed_str = re.sub(r'GMT([+-]\\d{1,2})$', lambda m: f\"GMT{m.group(1).zfill(3)}00\", date_str)\n",
    "        parsed = datetime.strptime(fixed_str, \"%a, %B %d, %Y at %I:%M %p GMT%z\")\n",
    "        return normalize_datetime(parsed)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    date_str_no_tz = re.sub(r'\\s+[A-Z]{3,4}$', '', date_str)\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str_no_tz, fmt)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def parse_relative_time(relative_str):\n",
    "    \"\"\"Parse relative time strings like '2h ago', '30m ago', '1d ago' into datetime\"\"\"\n",
    "    if not relative_str or 'ago' not in relative_str.lower():\n",
    "        return None\n",
    "    \n",
    "    relative_str = relative_str.lower().strip()\n",
    "    now = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        parts = relative_str.split()\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "        \n",
    "        amount = int(parts[0])\n",
    "        unit = parts[1].lower()\n",
    "        \n",
    "        if unit.startswith('h'):\n",
    "            return now - timedelta(hours=amount)\n",
    "        elif unit.startswith('m'):\n",
    "            return now - timedelta(minutes=amount)\n",
    "        elif unit.startswith('d'):\n",
    "            return now - timedelta(days=amount)\n",
    "        elif unit.startswith('w'):\n",
    "            return now - timedelta(weeks=amount)\n",
    "        elif unit.startswith('s'):\n",
    "            return now - timedelta(seconds=amount)\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def check_cutoff_reached(parsed_date):\n",
    "    \"\"\"Check if date is before cutoff - safely handles naive datetimes\"\"\"\n",
    "    if parsed_date is None:\n",
    "        return False\n",
    "    # Normalize to ensure comparison works\n",
    "    normalized = normalize_datetime(parsed_date)\n",
    "    return normalized < CUTOFF_DATE\n",
    "\n",
    "def handle_cookie_consent(driver):\n",
    "    \"\"\"Handle Yahoo cookie consent\"\"\"\n",
    "    try:\n",
    "        accept_button = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[name='agree'][value='agree'].accept-all\"))\n",
    "        )\n",
    "        accept_button.click()\n",
    "        sleep(1)\n",
    "        print(\"‚úì Cookie consent accepted!\\n\")\n",
    "        return True\n",
    "    except:\n",
    "        try:\n",
    "            scroll_button = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.ID, \"scroll-down-btn\"))\n",
    "            )\n",
    "            scroll_button.click()\n",
    "            sleep(1)\n",
    "            accept_button = WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[name='agree'][value='agree'].accept-all\"))\n",
    "            )\n",
    "            accept_button.click()\n",
    "            sleep(1)\n",
    "            print(\"‚úì Cookie consent accepted!\\n\")\n",
    "            return True\n",
    "        except:\n",
    "            print(\"‚ö† No consent dialog found (may be already accepted)\\n\")\n",
    "            return False\n",
    "\n",
    "def smart_scroll_and_load(driver, max_scroll_attempts=200):\n",
    "    \"\"\"Scroll intelligently and extract relative dates from homepage\"\"\"\n",
    "    print(f\"\\nüîÑ Smart scrolling to load articles with dates...\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    articles_found = {}\n",
    "    scroll_count = 0\n",
    "    no_new_content_count = 0\n",
    "    no_height_change_count = 0\n",
    "\n",
    "    while scroll_count < max_scroll_attempts:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(2)\n",
    "        driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "        sleep(1.5)\n",
    "\n",
    "        try:\n",
    "            article_sections = driver.find_elements(By.CSS_SELECTOR, 'section[data-testid=\"storyitem\"]')\n",
    "            new_articles_count = 0\n",
    "\n",
    "            for section in article_sections:\n",
    "                try:\n",
    "                    url = None\n",
    "                    try:\n",
    "                        link = section.find_element(By.CSS_SELECTOR, 'a.subtle-link.titles[href*=\"/news/\"]')\n",
    "                        url = link.get_attribute('href')\n",
    "                    except:\n",
    "                        try:\n",
    "                            link = section.find_element(By.CSS_SELECTOR, 'a.subtle-link.titles[href*=\"/m/\"]')\n",
    "                            url = link.get_attribute('href')\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    if url and (('/news/' in url) or ('/m/' in url)):\n",
    "                        last_part = url.split('/')[-1]\n",
    "                        if '.html' in last_part or any(c.isdigit() for c in last_part):\n",
    "                            if url not in articles_found:\n",
    "                                relative_date = \"N/A\"\n",
    "                                try:\n",
    "                                    publishing_div = section.find_element(By.CSS_SELECTOR, 'div.publishing.yf-m1e6lz')\n",
    "                                    text = publishing_div.text.strip()\n",
    "                                    if 'ago' in text.lower():\n",
    "                                        parts = text.split('‚Ä¢')\n",
    "                                        if len(parts) > 1:\n",
    "                                            relative_date = parts[1].strip()\n",
    "                                except:\n",
    "                                    pass\n",
    "                                \n",
    "                                articles_found[url] = relative_date\n",
    "                                new_articles_count += 1\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if new_articles_count > 0:\n",
    "                no_new_content_count = 0\n",
    "                no_height_change_count = 0  # Reset both counters when we find new articles\n",
    "                print(f\"  üìä Scroll {scroll_count + 1}: {len(articles_found)} articles found (+{new_articles_count} new)\")\n",
    "            else:\n",
    "                no_new_content_count += 1\n",
    "                print(f\"  ‚è∏ Scroll {scroll_count + 1}: No new articles (attempt {no_new_content_count}/10)\")\n",
    "\n",
    "            # Check if we've exhausted scrolling (both no new articles AND page height not changing)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                no_height_change_count += 1\n",
    "                print(f\"    ‚ö† Page height not changing (attempt {no_height_change_count}/3)\")\n",
    "                \n",
    "                # Only stop if BOTH conditions are true: no new articles AND no height change\n",
    "                if no_new_content_count >= 10 and no_height_change_count >= 3:\n",
    "                    print(f\"  ‚ö† No new content AND page not loading more. Stopping.\")\n",
    "                    break\n",
    "            else:\n",
    "                no_height_change_count = 0  # Reset if page is still growing\n",
    "                print(f\"    ‚úì Page height increased (new height: {new_height})\")\n",
    "\n",
    "            last_height = new_height\n",
    "            scroll_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Error during scroll: {e}\")\n",
    "            scroll_count += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nüîÑ Parsing relative times and sorting articles...\")\n",
    "    articles_with_dates = []\n",
    "    \n",
    "    for url, relative_str in articles_found.items():\n",
    "        parsed_date = parse_relative_time(relative_str)\n",
    "        articles_with_dates.append({\n",
    "            'url': url,\n",
    "            'relative_date': relative_str,\n",
    "            'parsed_date': parsed_date\n",
    "        })\n",
    "    \n",
    "    articles_with_dates.sort(\n",
    "        key=lambda x: x['parsed_date'] if x['parsed_date'] is not None else datetime.min, \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Sorted articles preview:\")\n",
    "    for i, article in enumerate(articles_with_dates[:3]):\n",
    "        date_str = article['relative_date'] if article['relative_date'] != \"N/A\" else 'N/A'\n",
    "        print(f\"  [{i+1}] {date_str}\")\n",
    "    if len(articles_with_dates) > 3:\n",
    "        print(f\"  ... ({len(articles_with_dates) - 3} more articles)\")\n",
    "    \n",
    "    print(f\"\\n‚úì Scrolling complete: {len(articles_with_dates)} unique articles found and sorted by date\\n\")\n",
    "    return articles_with_dates\n",
    "\n",
    "def scrape_article_details(driver, article_url):\n",
    "    \"\"\"Scrape article details\"\"\"\n",
    "    try:\n",
    "        driver.get(article_url)\n",
    "        sleep(1.5)\n",
    "\n",
    "        article_data = {\n",
    "            'url': article_url,\n",
    "            'title': '',\n",
    "            'author': '',\n",
    "            'date': '',\n",
    "            'datetime': '',\n",
    "            'parsed_date': None,\n",
    "            'text': '',\n",
    "            'article_type': '',\n",
    "            'stock_tickers': [],\n",
    "            'cutoff_reached': False\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            title_elem = WebDriverWait(driver, 3).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'h1.cover-title'))\n",
    "            )\n",
    "            article_data['title'] = title_elem.text.strip()\n",
    "        except:\n",
    "            article_data['title'] = \"N/A\"\n",
    "\n",
    "        is_video = False\n",
    "        try:\n",
    "            publishing_div = driver.find_element(By.CSS_SELECTOR, 'div.publishing.yf-m1e6lz')\n",
    "            if 'Yahoo Finance Video' in publishing_div.text:\n",
    "                is_video = True\n",
    "                article_data['article_type'] = 'video'\n",
    "        except:\n",
    "            article_data['article_type'] = 'text'\n",
    "\n",
    "        try:\n",
    "            author_elem = driver.find_element(By.CSS_SELECTOR, 'div.byline-attr-author a.primary-link')\n",
    "            article_data['author'] = author_elem.text.strip()\n",
    "        except:\n",
    "            try:\n",
    "                author_elem = driver.find_element(By.CSS_SELECTOR, 'a.primary-link[data-ylk*=\"author\"]')\n",
    "                article_data['author'] = author_elem.text.strip()\n",
    "            except:\n",
    "                try:\n",
    "                    author_div = driver.find_element(By.CSS_SELECTOR, 'div.byline-attr-author')\n",
    "                    try:\n",
    "                        author_link = author_div.find_element(By.TAG_NAME, 'a')\n",
    "                        article_data['author'] = author_link.text.strip()\n",
    "                    except:\n",
    "                        full_text = author_div.text.strip()\n",
    "                        if '¬∑' in full_text:\n",
    "                            article_data['author'] = full_text.split('¬∑')[0].strip()\n",
    "                        else:\n",
    "                            article_data['author'] = full_text\n",
    "                except:\n",
    "                    article_data['author'] = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            date_elem = driver.find_element(By.CSS_SELECTOR, 'time.byline-attr-meta-time, time[datetime]')\n",
    "            article_data['date'] = date_elem.text.strip()\n",
    "            article_data['datetime'] = date_elem.get_attribute('datetime') or \"N/A\"\n",
    "            \n",
    "            parsed_date = parse_date(article_data['date'])\n",
    "            # Normalize to naive datetime for consistency\n",
    "            article_data['parsed_date'] = normalize_datetime(parsed_date)\n",
    "            \n",
    "            if article_data['parsed_date'] is not None and check_cutoff_reached(article_data['parsed_date']):\n",
    "                print(f\"    ‚õî CUTOFF REACHED! Date: {article_data['parsed_date'].strftime('%d/%m/%Y %H:%M')}\")\n",
    "                article_data['cutoff_reached'] = True\n",
    "                return article_data\n",
    "        except:\n",
    "            article_data['date'] = \"N/A\"\n",
    "            article_data['datetime'] = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            ticker_elements = driver.find_elements(By.CSS_SELECTOR, 'span.symbol.yf-90gdtp')\n",
    "            tickers = [ticker.text.strip() for ticker in ticker_elements if ticker.text.strip()]\n",
    "            article_data['stock_tickers'] = tickers if tickers else []\n",
    "        except:\n",
    "            article_data['stock_tickers'] = []\n",
    "\n",
    "        if is_video:\n",
    "            try:\n",
    "                transcript_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[data-testid=\"accordionItem\"][aria-controls*=\"acccon\"]'))\n",
    "                )\n",
    "                if 'transcript' in transcript_button.text.lower():\n",
    "                    transcript_button.click()\n",
    "                    sleep(1)\n",
    "                    transcript_paragraphs = driver.find_elements(By.CSS_SELECTOR, 'div.transcript-content p.type-body-md-reg')\n",
    "                    if transcript_paragraphs:\n",
    "                        transcript_parts = [p.text.strip() for p in transcript_paragraphs if p.text.strip()]\n",
    "                        article_data['text'] = '\\n\\n'.join(transcript_parts)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                try:\n",
    "                    read_more_button = WebDriverWait(driver, 3).until(\n",
    "                        EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.readmore-button[data-ylk*=\"readmore\"]'))\n",
    "                    )\n",
    "                    read_more_button.click()\n",
    "                    sleep(1)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                paragraphs = driver.find_elements(By.CSS_SELECTOR, 'div.body[data-testid=\"article-body\"] p.yf-1090901')\n",
    "                text_parts = [p.text.strip() for p in paragraphs if p.text.strip() and len(p.text.strip()) > 20]\n",
    "                \n",
    "                if text_parts:\n",
    "                    article_data['text'] = '\\n\\n'.join(text_parts)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return article_data\n",
    "    except Exception as e:\n",
    "        print(f\"    ‚úó Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_category(driver, category_name, category_url):\n",
    "    \"\"\"Scrape articles from a single category\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üìÇ SCRAPING CATEGORY: {category_name}\")\n",
    "    print(f\"‚õî CUTOFF DATE: {CUTOFF_DATE.strftime('%d/%m/%Y %H:%M')}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nüåê Loading: {category_url}\")\n",
    "        driver.get(category_url)\n",
    "        sleep(1.2)\n",
    "        handle_cookie_consent(driver)\n",
    "        \n",
    "        articles_data = smart_scroll_and_load(driver)\n",
    "        \n",
    "        if not articles_data:\n",
    "            print(f\"‚ö† No articles found for {category_name}!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"‚úì Found {len(articles_data)} articles, starting scraping until cutoff date...\\n\")\n",
    "        print(f\"üîç Scraping articles:\\n\")\n",
    "        \n",
    "        all_articles = []\n",
    "        cutoff_reached = False\n",
    "        failed_count = 0\n",
    "        consecutive_before_cutoff = 0\n",
    "        \n",
    "        for idx, article_info in enumerate(articles_data, 1):\n",
    "            if cutoff_reached:\n",
    "                print(f\"\\n‚õî CUTOFF REACHED! Stopping scraping for category {category_name}\")\n",
    "                break\n",
    "            \n",
    "            article_url = article_info['url']\n",
    "            \n",
    "            if idx % 10 == 0 or idx == 1:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"\\nüìà Progress: {idx}/{len(articles_data)} | Elapsed: {elapsed:.1f}s | Articles scraped: {len(all_articles)}\")\n",
    "            \n",
    "            if not article_url or not isinstance(article_url, str):\n",
    "                print(f\"  ‚ö† Skipping invalid URL at index {idx}\")\n",
    "                continue\n",
    "            \n",
    "            data = scrape_article_details(driver, article_url)\n",
    "            \n",
    "            if data:\n",
    "                if data['cutoff_reached']:\n",
    "                    cutoff_reached = True\n",
    "                    break\n",
    "                \n",
    "                # Check if article is before cutoff - with proper datetime handling\n",
    "                if data['parsed_date'] is not None:\n",
    "                    normalized_date = normalize_datetime(data['parsed_date'])\n",
    "                    if normalized_date < CUTOFF_DATE:\n",
    "                        consecutive_before_cutoff += 1\n",
    "                        print(f\"\\n  ‚õî Article #{idx} is before cutoff ({normalized_date.strftime('%d/%m/%Y %H:%M')} < {CUTOFF_DATE.strftime('%d/%m/%Y %H:%M')})\")\n",
    "                        if consecutive_before_cutoff >= 3:\n",
    "                            print(f\"  ‚õî {consecutive_before_cutoff} consecutive articles before cutoff. Stopping scraping.\")\n",
    "                            cutoff_reached = True\n",
    "                            break\n",
    "                    else:\n",
    "                        consecutive_before_cutoff = 0\n",
    "                \n",
    "                if data['title'] != \"N/A\":\n",
    "                    data['category'] = category_name\n",
    "                    all_articles.append(data)\n",
    "                    if idx <= 3:\n",
    "                        date_str = data['parsed_date'].strftime('%d/%m/%Y %H:%M') if data['parsed_date'] is not None else 'N/A'\n",
    "                        print(f\"  ‚úì [{idx}] {data['title'][:50]}... ({date_str})\")\n",
    "                    \n",
    "                    if len(all_articles) % 20 == 0:\n",
    "                        current_date = data['parsed_date'].strftime('%d/%m/%Y %H:%M') if data['parsed_date'] is not None else 'N/A'\n",
    "                        print(f\"\\n  ‚úÖ [{len(all_articles)} articles] - Most recent: {current_date}\")\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "            \n",
    "            sleep(1.2)\n",
    "        \n",
    "        if not all_articles:\n",
    "            print(f\"\\n‚ö† No articles extracted for {category_name}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(all_articles)\n",
    "        expected_cols = ['category', 'title', 'author', 'date', 'datetime', 'parsed_date', 'text', 'url', 'stock_tickers']\n",
    "        for col in expected_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = [] if col == 'stock_tickers' else \"\"\n",
    "        df = df[expected_cols]\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n‚è± Category scraping time: {total_time:.1f} seconds\")\n",
    "        print(f\"üìä Articles kept: {len(all_articles)}\")\n",
    "        if failed_count > 0:\n",
    "            print(f\"‚ö† Failed: {failed_count} articles\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error scraping category {category_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def scrape_all_categories():\n",
    "    \"\"\"Main function to scrape all categories\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    \n",
    "    all_categories_data = []\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"üöÄ YAHOO FINANCE SCRAPER - WITH DATE CUTOFF (FIXED)\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nüìã Categories to scrape: {len(CATEGORIES)}\")\n",
    "        print(f\"‚õî CUTOFF DATE: {CUTOFF_DATE.strftime('%d/%m/%Y %H:%M')}\\n\")\n",
    "        \n",
    "        for idx, (category_name, category_url) in enumerate(CATEGORIES.items(), 1):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"üîÑ CATEGORY {idx}/{len(CATEGORIES)}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            try:\n",
    "                category_df = scrape_category(driver, category_name, category_url)\n",
    "                if not category_df.empty:\n",
    "                    all_categories_data.append(category_df)\n",
    "                    print(f\"\\n‚úÖ {category_name}: {len(category_df)} articles scraped\")\n",
    "                else:\n",
    "                    print(f\"\\n‚ö†Ô∏è {category_name}: No articles scraped\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "            \n",
    "            if idx < len(CATEGORIES):\n",
    "                sleep(3)\n",
    "        \n",
    "        if all_categories_data:\n",
    "            final_df = pd.concat(all_categories_data, ignore_index=True)\n",
    "            final_df['text_length'] = final_df['text'].apply(len)\n",
    "            final_df['word_count'] = final_df['text'].apply(lambda x: len(x.split()))\n",
    "            \n",
    "            try:\n",
    "                final_df = final_df.sort_values(by='parsed_date', ascending=False).reset_index(drop=True)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            total_time = time.time() - overall_start\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"‚úÖ SCRAPING COMPLETED!\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"‚è± Total time: {total_time:.1f}s ({total_time/60:.1f}m)\")\n",
    "            print(f\"üìä Total articles: {len(final_df)}\")\n",
    "            print(f\"üìã Articles per category:\")\n",
    "            for cat in CATEGORIES.keys():\n",
    "                count = len(final_df[final_df['category'] == cat])\n",
    "                print(f\"  ‚Ä¢ {cat}: {count}\")\n",
    "            \n",
    "            return final_df\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è No data scraped\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Main execution\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "df_all = scrape_all_categories()\n",
    "\n",
    "if not df_all.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä FINAL RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nüìã DataFrame Info:\")\n",
    "    df_all.info()\n",
    "    print(\"\\nüìÑ First 5 Articles:\")\n",
    "    print(df_all[['title', 'category', 'parsed_date']].head())\n",
    "    \n",
    "    # Prepare for export\n",
    "    df_export = df_all.copy()\n",
    "    df_export['parsed_date'] = df_export['parsed_date'].apply(\n",
    "        lambda x: x.isoformat() if x is not None else None\n",
    "    )\n",
    "    df_export['stock_tickers'] = df_export['stock_tickers'].apply(\n",
    "        lambda x: ','.join(x) if isinstance(x, list) and x else ''\n",
    "    )\n",
    "    \n",
    "    # Save CSV\n",
    "    df_export.to_csv('yahoo_finance_articles.csv', index=False, encoding='utf-8')\n",
    "    print(\"\\nüíæ Saved to 'yahoo_finance_articles.csv'\")\n",
    "    \n",
    "    # Save JSON\n",
    "    class CustomEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if obj is None:\n",
    "                return None\n",
    "            elif isinstance(obj, (pd.Timestamp, datetime)):\n",
    "                return obj.isoformat()\n",
    "            return str(obj)\n",
    "    \n",
    "    with open('yahoo_finance_articles.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(df_export.to_dict(orient='records'), f, cls=CustomEncoder, indent=2)\n",
    "    print(\"üíæ Saved to 'yahoo_finance_articles.json'\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No data to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ed350",
   "metadata": {},
   "source": [
    "### Set dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Conta quanti titoli sono duplicati\n",
    "duplicati = df_all[df_all.duplicated(subset='title', keep=False)]\n",
    "print(f\"Numero di record con titoli duplicati: {len(duplicati)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Show how many unique titles we have\n",
    "num_unici = df_all['title'].nunique()\n",
    "print(f\"\\nNumero di titoli unici: {num_unici}\")\n",
    "\n",
    "# 3. Delete duplicates\n",
    "df_all = df_all.drop_duplicates(subset='title', keep='first')\n",
    "\n",
    "# 4. Final check\n",
    "print(f\"\\nNumero di righe dopo la rimozione duplicati: {len(df_all)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display list of columns in df\n",
    "print(df_all.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 3 rows of df_all\n",
    "print(df_all.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f4270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'datetime','date' from df_all\n",
    "df_all = df_all.drop(columns=['datetime','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ecf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert to string type if not already\n",
    "df_all['stock_tickers'] = df_all['stock_tickers'].astype(str)\n",
    "\n",
    "# Remove brackets and quotes, replace ', ' with ','\n",
    "df_all['stock_tickers'] = df_all['stock_tickers'].str.replace(r'[\\[\\]\"]', '', regex=True)\n",
    "df_all['stock_tickers'] = df_all['stock_tickers'].str.replace(r\"', '\", ',', regex=True)\n",
    "df_all['stock_tickers'] = df_all['stock_tickers'].str.replace(\"'\", '', regex=True)\n",
    "\n",
    "# Rename column\n",
    "df_all = df_all.rename(columns={'stock_tickers': 'tickers'})\n",
    "\n",
    "# Check the result\n",
    "print(df_all['tickers'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 3 rows of df_all\n",
    "print(df_all.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85ef05",
   "metadata": {},
   "source": [
    "### Create a copy of 'df_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabdd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea copia e pulisci in un passaggio\n",
    "df_export = df_all.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd3472",
   "metadata": {},
   "source": [
    "### Remove problematic article and export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Crea copia\n",
    "df_export = df_export[df_export['title'] != \"Huntington (HBAN) Q3 2025 Earnings Call Transcript\"]\n",
    "\n",
    "# Pulisci tutte le colonne di tipo object/string\n",
    "df_export = df_export.apply(\n",
    "    lambda x: x.str.replace('\\t|\\r|\\n', ' ', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip() \n",
    "    if x.dtype == 'object' else x\n",
    ")\n",
    "\n",
    "# Esporta\n",
    "df_export.to_csv(\n",
    "    'output.csv', \n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8',\n",
    "    quoting=csv.QUOTE_NONNUMERIC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99de06a",
   "metadata": {},
   "source": [
    "### Check code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Filtra le prime 10 news nella categoria \"stocks\"\n",
    "# =====================================================\n",
    "\n",
    "# Filtra solo le righe in cui la categoria √® \"stocks\"\n",
    "df_stocks = df_export[df_export['category'].str.lower() == 'crypto']\n",
    "\n",
    "# Controlla se ci sono articoli in questa categoria\n",
    "if not df_stocks.empty:\n",
    "    # Converti a datetime se non √® gi√† stato fatto\n",
    "    df_stocks['parsed_date'] = pd.to_datetime(df_stocks['parsed_date'], errors='coerce')\n",
    "    \n",
    "    # Ordina per data decrescente\n",
    "    df_sorted = df_stocks.sort_values(by='parsed_date', ascending=False)\n",
    "\n",
    "    # Aggiungi una colonna formattata per una visualizzazione pi√π leggibile\n",
    "    df_sorted['formatted_date'] = df_sorted['parsed_date'].dt.strftime(\"%d %B %Y, %H:%M\")\n",
    "\n",
    "    # Mostra le prime 5 righe (pi√π recenti)\n",
    "    print(\"\\nüìà Le 5 news pi√π recenti nella categoria 'stocks':\")\n",
    "    print(df_sorted[['formatted_date', 'title', 'category', 'url']].head(10))\n",
    "else:\n",
    "    print(\"‚ùå Nessuna notizia trovata nella categoria 'stocks'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show extracted text from first article\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assicurati che Pandas non tronchi le stringhe lunghe\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Ciclo per stampare titolo e testo completo dei primi 5 articoli\n",
    "for i, row in enumerate(df_sorted.head(1).itertuples(), 1):\n",
    "    print(f\"\\nüì∞ Articolo {i}\")\n",
    "    print(f\"Titolo: {row.title}\")\n",
    "    print(f\"Testo completo:\\n{row.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c1406",
   "metadata": {},
   "source": [
    "## Text cleaning, lemmatization , vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete NLP Text Processing Pipeline\n",
    "- Text cleaning\n",
    "- Collocation extraction (POS-based and PMI-based)\n",
    "- Lemmatization with stopword removal\n",
    "- Term-Document Matrix creation\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# REQUIRED LIBRARIES\n",
    "# ============================================================================\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK stopwords\n",
    "try:\n",
    "    nltk_stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. TEXT CLEANING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text(x=None, \n",
    "               hashtag=True, \n",
    "               mention=True, \n",
    "               numbers=False, \n",
    "               punctuation=False,\n",
    "               lowercase=True):\n",
    "    \"\"\"Clean text by removing/replacing various elements\"\"\"\n",
    "    \n",
    "    if x is None or not isinstance(x, (str, list)):\n",
    "        raise ValueError(\"Invalid text input - must be string or list of strings\")\n",
    "    \n",
    "    if isinstance(x, str):\n",
    "        x = [x]\n",
    "        return_single = True\n",
    "    else:\n",
    "        return_single = False\n",
    "    \n",
    "    for arg in [hashtag, mention, numbers, punctuation, lowercase]:\n",
    "        if not isinstance(arg, bool):\n",
    "            raise ValueError(\"All flags must be boolean (True/False)\")\n",
    "    \n",
    "    html_symbols = [\n",
    "        \"&copy;\", \"&reg;\", \"&trade;\", \"&ldquo;\", \"&lsquo;\", \"&rsquo;\", \"&bull;\",\n",
    "        \"&middot;\", \"&sdot;\", \"&ndash;\", \"&mdash;\", \"&cent;\", \"&pound;\", \"&euro;\",\n",
    "        \"&ne;\", \"&frac12;\", \"&frac14;\", \"&frac34;\", \"&deg;\", \"&larr;\", \"&rarr;\",\n",
    "        \"&hellip;\", \"&nbsp;\", \"&lt;\", \"&gt;\", \"&amp;\", \"&quot;\"\n",
    "    ]\n",
    "    html_symbols_pattern = \"|\".join(re.escape(sym) for sym in html_symbols)\n",
    "    \n",
    "    char_map = {\n",
    "        \"\\034\": '\"',\n",
    "        \"\\035\": '\"',\n",
    "        \"\\036\": '\"',\n",
    "        \"\\030\": \"'\",\n",
    "        \"\\031\": \"'\"\n",
    "    }\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    \n",
    "    for text in x:\n",
    "        if not isinstance(text, str):\n",
    "            cleaned_texts.append(text)\n",
    "            continue\n",
    "        \n",
    "        xtxt = text\n",
    "        xtxt = re.sub(r\"(f|ht)(tp)(s?)(://)(.\\S+)[.|/](.\\S+)\", \" \", xtxt)\n",
    "        xtxt = re.sub(r\"(RT|via)((?:\\b\\W*@\\w+)+)\", \" \", xtxt)\n",
    "        xtxt = re.sub(r\"(rt|via)((?:\\b\\W*@\\w+)+)\", \" \", xtxt)\n",
    "        xtxt = re.sub(html_symbols_pattern, \" \", xtxt)\n",
    "        \n",
    "        if punctuation:\n",
    "            xtxt = re.sub(r\"([#@])|[^\\w\\s]\", r\" \\1\", xtxt)\n",
    "        \n",
    "        for old_char, new_char in char_map.items():\n",
    "            xtxt = xtxt.replace(old_char, new_char)\n",
    "        \n",
    "        xtxt = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', xtxt)\n",
    "        xtxt = re.sub(r'[^\\x20-\\x7E]', ' ', xtxt)\n",
    "        \n",
    "        if hashtag:\n",
    "            xtxt = re.sub(r\"#\\S+\", \" \", xtxt)\n",
    "        \n",
    "        if mention:\n",
    "            xtxt = re.sub(r\"@\\S+\", \" \", xtxt)\n",
    "        \n",
    "        if numbers:\n",
    "            xtxt = re.sub(r\"[0-9]\", \"\", xtxt)\n",
    "        \n",
    "        xtxt = re.sub(r\"[ \\t]{2,}\", \" \", xtxt)\n",
    "        xtxt = re.sub(r\"\\s+\", \" \", xtxt)\n",
    "        xtxt = xtxt.strip()\n",
    "        \n",
    "        if lowercase:\n",
    "            xtxt = xtxt.lower()\n",
    "        \n",
    "        cleaned_texts.append(xtxt)\n",
    "    \n",
    "    if return_single:\n",
    "        return cleaned_texts[0]\n",
    "    else:\n",
    "        return cleaned_texts\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. SPACY UTILITIES & COLLOCATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_spacy_model(model_name):\n",
    "    \"\"\"Load spaCy model\"\"\"\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "    except OSError:\n",
    "        raise FileNotFoundError(f\"Cannot load spaCy model: {model_name}. \"\n",
    "                                f\"Run: python -m spacy download {model_name}\")\n",
    "    return nlp\n",
    "\n",
    "\n",
    "def annotate_text(nlp, text):\n",
    "    \"\"\"Annotate single text with spaCy\"\"\"\n",
    "    doc = nlp(text or \"\")\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        tokens.append({\n",
    "            'form': token.text,\n",
    "            'lemma': token.lemma_,\n",
    "            'upos': token.pos_\n",
    "        })\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def annotate_texts(nlp, texts, show_progress=True):\n",
    "    \"\"\"Annotate list of texts\"\"\"\n",
    "    out = []\n",
    "    iterable = texts if not show_progress else tqdm(texts, desc=\"Annotating\")\n",
    "    for t in iterable:\n",
    "        out.append(annotate_text(nlp, str(t) if pd.notna(t) else \"\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "def my_collocations_POS(texts, nlp, pos_patterns=[('ADJ','NOUN'), ('NOUN','NOUN'), \n",
    "                                                    ('NOUN','PROPN'), ('PROPN','PROPN')], \n",
    "                        min_freq=2, xlsx_save=True, xlsx_name=\"colloc_POS.xlsx\", verbose=True):\n",
    "    \"\"\"Extract POS-based collocations\"\"\"\n",
    "    \n",
    "    if verbose: print(\"Annotating texts for POS collocations...\")\n",
    "    annotated = annotate_texts(nlp, texts, show_progress=verbose)\n",
    "    counts = Counter()\n",
    "    pattern_map = {}\n",
    "    \n",
    "    if verbose: print(\"Counting adjacent bigrams filtered by POS...\")\n",
    "    for doc in annotated:\n",
    "        for i in range(len(doc)-1):\n",
    "            t1, t2 = doc[i], doc[i+1]\n",
    "            p = (t1['upos'], t2['upos'])\n",
    "            if p in pos_patterns:\n",
    "                w1 = t1['lemma'] if t1['lemma'] and t1['lemma'] != '_' else t1['form']\n",
    "                w2 = t2['lemma'] if t2['lemma'] and t2['lemma'] != '_' else t2['form']\n",
    "                coll = f\"{w1.lower()} {w2.lower()}\"\n",
    "                counts[coll] += 1\n",
    "                pattern_map[coll] = f\"{p[0]} {p[1]}\"\n",
    "    \n",
    "    rows = [{'collocation': c, 'freq': f, 'pos_pattern': pattern_map.get(c, '')}\n",
    "            for c, f in counts.items() if f >= min_freq]\n",
    "    \n",
    "    if not rows:\n",
    "        print(f\"‚ö† Warning: No collocations found with min_freq={min_freq}\")\n",
    "        df = pd.DataFrame(columns=['collocation', 'freq', 'pos_pattern'])\n",
    "    else:\n",
    "        df = pd.DataFrame(rows).sort_values(['freq','collocation'], ascending=[False,True]).reset_index(drop=True)\n",
    "    \n",
    "    if xlsx_save:\n",
    "        df.to_excel(xlsx_name, index=False)\n",
    "        if verbose: print(f\"Saved {xlsx_name} ({len(df)} rows)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def my_collocations(texts, nlp, nn=200, sort_ord=\"pmi\", min_freq=2,\n",
    "                    xlsx_save=True, xlsx_name=\"colloc_PMI.xlsx\", verbose=True):\n",
    "    \"\"\"Extract generic collocations using PMI\"\"\"\n",
    "    \n",
    "    if verbose: print(\"Annotating texts for PMI collocations...\")\n",
    "    annotated = annotate_texts(nlp, texts, show_progress=verbose)\n",
    "    unigram = Counter()\n",
    "    bigram = Counter()\n",
    "    total_unigrams = 0\n",
    "    \n",
    "    for doc in annotated:\n",
    "        lemmas = []\n",
    "        for tok in doc:\n",
    "            if tok['upos'] == 'PUNCT':\n",
    "                continue\n",
    "            lemma = tok['lemma'] if tok['lemma'] and tok['lemma'] != '_' else tok['form']\n",
    "            w = lemma.lower()\n",
    "            lemmas.append(w)\n",
    "            unigram[w] += 1\n",
    "            total_unigrams += 1\n",
    "        \n",
    "        for i in range(len(lemmas)-1):\n",
    "            big = f\"{lemmas[i]} {lemmas[i+1]}\"\n",
    "            bigram[big] += 1\n",
    "    \n",
    "    N = total_unigrams if total_unigrams > 0 else 1\n",
    "    rows = []\n",
    "    \n",
    "    for big, freq in bigram.items():\n",
    "        if freq < min_freq: \n",
    "            continue\n",
    "        w1, w2 = big.split(\" \", 1)\n",
    "        p_w1 = unigram[w1] / N\n",
    "        p_w2 = unigram[w2] / N\n",
    "        p_w1w2 = freq / max(1, N-1)\n",
    "        \n",
    "        if p_w1 > 0 and p_w2 > 0 and p_w1w2 > 0:\n",
    "            pmi = math.log2(p_w1w2 / (p_w1 * p_w2))\n",
    "        else:\n",
    "            pmi = float('-inf')\n",
    "        \n",
    "        rows.append({'collocation': big, 'freq': freq, 'pmi': pmi})\n",
    "    \n",
    "    if not rows:\n",
    "        print(f\"‚ö† Warning: No collocations found with min_freq={min_freq}\")\n",
    "        df = pd.DataFrame(columns=['collocation', 'freq', 'pmi'])\n",
    "    else:\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        if sort_ord.lower() in ['pmi', 'pmi_desc']:\n",
    "            df = df.sort_values(['pmi', 'freq'], ascending=[False, False])\n",
    "        else:\n",
    "            df = df.sort_values(['freq', 'pmi'], ascending=[False, False])\n",
    "    \n",
    "    if nn is not None:\n",
    "        df = df.head(nn).reset_index(drop=True)\n",
    "    \n",
    "    if xlsx_save:\n",
    "        df.to_excel(xlsx_name, index=False)\n",
    "        if verbose: print(f\"Saved {xlsx_name} ({len(df)} rows)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def corMultWord_xlsx(texts, xlsx_file, verbose=True):\n",
    "    \"\"\"Replace multi-word collocations with underscores\"\"\"\n",
    "    \n",
    "    if not os.path.exists(xlsx_file):\n",
    "        if verbose:\n",
    "            print(f\"‚ö† Warning: File not found: {xlsx_file}. Skipping replacement.\")\n",
    "        return pd.Series(texts, index=None)\n",
    "    \n",
    "    df = pd.read_excel(xlsx_file, engine='openpyxl')\n",
    "    if 'collocation' not in df.columns:\n",
    "        raise ValueError(\"Excel file missing 'collocation' column\")\n",
    "    \n",
    "    colls = sorted(df['collocation'].dropna().astype(str).unique(), \n",
    "                   key=lambda s: len(s.split()), reverse=True)\n",
    "    \n",
    "    if not colls:\n",
    "        if verbose:\n",
    "            print(f\"‚ö† No collocations found in {xlsx_file}\")\n",
    "        return pd.Series(texts, index=None)\n",
    "    \n",
    "    patterns = []\n",
    "    for c in colls:\n",
    "        escaped = r'\\s+'.join(re.escape(part) for part in c.split())\n",
    "        pattern = re.compile(rf'\\b{escaped}\\b', flags=re.IGNORECASE)\n",
    "        replacement = \"_\".join(c.split())\n",
    "        patterns.append((pattern, replacement))\n",
    "    \n",
    "    out_texts = []\n",
    "    iterator = texts if not verbose else tqdm(texts, desc=f\"Applying {os.path.basename(xlsx_file)}\")\n",
    "    \n",
    "    for t in iterator:\n",
    "        if pd.isna(t):\n",
    "            out_texts.append(t)\n",
    "            continue\n",
    "        s = str(t)\n",
    "        for pattern, repl in patterns:\n",
    "            s = pattern.sub(repl, s)\n",
    "        out_texts.append(s)\n",
    "    \n",
    "    return pd.Series(out_texts, index=None)\n",
    "\n",
    "\n",
    "def apply_pipeline_on_df(df_export, text_col='text_cleaned', verbose=True):\n",
    "    \"\"\"Apply complete collocation pipeline\"\"\"\n",
    "    \n",
    "    if text_col not in df_export.columns:\n",
    "        raise KeyError(f\"Column {text_col} not found in df_export\")\n",
    "    \n",
    "    if verbose: print(\"Loading spaCy model...\")\n",
    "    nlp = load_spacy_model(\"en_core_web_sm\")\n",
    "    \n",
    "    if verbose: print(\"Step 1: POS-based collocations\")\n",
    "    out_pos = my_collocations_POS(df_export[text_col], nlp, min_freq=1,\n",
    "                                  xlsx_save=True, xlsx_name=\"colloc_POS.xlsx\", verbose=verbose)\n",
    "    \n",
    "    if verbose: print(\"Step 2: Applying POS substitutions\")\n",
    "    df_export[text_col] = corMultWord_xlsx(df_export[text_col], xlsx_file=\"colloc_POS.xlsx\", verbose=verbose).values\n",
    "    \n",
    "    if verbose: print(\"Step 3: PMI-based collocations\")\n",
    "    out_pmi = my_collocations(df_export[text_col], nlp, nn=200, sort_ord=\"pmi\", min_freq=2,\n",
    "                              xlsx_save=True, xlsx_name=\"colloc_PMI.xlsx\", verbose=verbose)\n",
    "    \n",
    "    if verbose: print(\"Step 4: Applying PMI substitutions\")\n",
    "    df_export[text_col] = corMultWord_xlsx(df_export[text_col], xlsx_file=\"colloc_PMI.xlsx\", verbose=verbose).values\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Pipeline complete.\")\n",
    "        print(f\"POS collocations saved: colloc_POS.xlsx ({len(out_pos)} rows)\")\n",
    "        print(f\"PMI collocations saved: colloc_PMI.xlsx ({len(out_pmi)} rows)\")\n",
    "    \n",
    "    return df_export, out_pos, out_pmi\n",
    "\n",
    "\n",
    "def check_collocation_files(xlsx_pos=\"colloc_POS.xlsx\", xlsx_pmi=\"colloc_PMI.xlsx\"):\n",
    "    \"\"\"Verify and display statistics of generated collocation files\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"COLLOCATION FILES VERIFICATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        df_pos = pd.read_excel(xlsx_pos)\n",
    "        print(f\"\\n‚úì POS-based file loaded: {xlsx_pos}\")\n",
    "        print(f\"  - Number of collocations: {len(df_pos)}\")\n",
    "        print(f\"  - Columns: {list(df_pos.columns)}\")\n",
    "        print(f\"  - Average frequency: {df_pos['freq'].mean():.2f}\")\n",
    "        print(f\"  - Max frequency: {df_pos['freq'].max()}\")\n",
    "        print(f\"  - Min frequency: {df_pos['freq'].min()}\")\n",
    "        print(\"\\n  Top 10 POS collocations:\")\n",
    "        print(df_pos.head(10)[['collocation', 'freq', 'pos_pattern']].to_string(index=False))\n",
    "        print(\"\\n  POS pattern distribution:\")\n",
    "        print(df_pos['pos_pattern'].value_counts())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n‚úó File not found: {xlsx_pos}\")\n",
    "        df_pos = None\n",
    "    \n",
    "    try:\n",
    "        df_pmi = pd.read_excel(xlsx_pmi)\n",
    "        print(f\"\\n‚úì PMI-based file loaded: {xlsx_pmi}\")\n",
    "        print(f\"  - Number of collocations: {len(df_pmi)}\")\n",
    "        print(f\"  - Columns: {list(df_pmi.columns)}\")\n",
    "        print(f\"  - Average PMI: {df_pmi['pmi'].mean():.2f}\")\n",
    "        print(f\"  - Max PMI: {df_pmi['pmi'].max():.2f}\")\n",
    "        print(f\"  - Min PMI: {df_pmi['pmi'].min():.2f}\")\n",
    "        print(\"\\n  Top 10 PMI collocations:\")\n",
    "        print(df_pmi.head(10)[['collocation', 'freq', 'pmi']].to_string(index=False))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n‚úó File not found: {xlsx_pmi}\")\n",
    "        df_pmi = None\n",
    "    \n",
    "    return df_pos, df_pmi\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. LEMMATIZATION WITH STOPWORDS\n",
    "# ============================================================================\n",
    "\n",
    "def lemmatize_spacy_en(x, model, stopwords_list=None, doc_id=None, verbose=False):\n",
    "    \"\"\"Lemmatize text preserving multi-word expressions\"\"\"\n",
    "    \n",
    "    if x is None:\n",
    "        raise ValueError(\"missing text (x is None)\")\n",
    "    if model is None:\n",
    "        raise ValueError(\"missing language model (model is None)\")\n",
    "    \n",
    "    if stopwords_list is None:\n",
    "        stopwords_list = list(nltk_stopwords.words('english'))\n",
    "    \n",
    "    stopwords_lower = [w.lower() for w in stopwords_list]\n",
    "    \n",
    "    if doc_id is None:\n",
    "        doc_id = [f\"docid{i}\" for i in range(len(x))]\n",
    "    \n",
    "    if len(doc_id) != len(x):\n",
    "        raise ValueError(\"doc_id length must match length(x)\")\n",
    "    \n",
    "    results = []\n",
    "    iterator = zip(x, doc_id)\n",
    "    if verbose:\n",
    "        iterator = tqdm(list(iterator), desc=\"Lemmatizing documents\")\n",
    "    \n",
    "    for doc_idx, (text, d_id) in enumerate(iterator):\n",
    "        doc = model(str(text) if pd.notna(text) else \"\")\n",
    "        \n",
    "        for token_idx, token in enumerate(doc):\n",
    "            if token.is_punct or token.is_space:\n",
    "                continue\n",
    "            \n",
    "            token_text = token.text\n",
    "            lemma_text = token.lemma_\n",
    "            pos_tag = token.pos_\n",
    "            dep = token.dep_\n",
    "            \n",
    "            is_stopword = (token_text.lower() in stopwords_lower or \n",
    "                          lemma_text.lower() in stopwords_lower)\n",
    "            \n",
    "            results.append({\n",
    "                'doc_id': d_id,\n",
    "                'token_id': token_idx + 1,\n",
    "                'token': token_text,\n",
    "                'lemma': lemma_text,\n",
    "                'upos': pos_tag,\n",
    "                'dep': dep,\n",
    "                'head_token_id': token.head.i + 1,\n",
    "                'STOP': is_stopword\n",
    "            })\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    if result_df.empty:\n",
    "        print(\"Warning: No tokens extracted!\")\n",
    "        return result_df\n",
    "    \n",
    "    print(f\"‚úì Lemmatization complete: {len(result_df)} tokens from {len(doc_id)} documents\")\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def apply_lemmatization_to_df(df_export, text_column='text_cleaned', verbose=True):\n",
    "    \"\"\"Apply lemmatization to df_export\"\"\"\n",
    "    \n",
    "    if text_column not in df_export.columns:\n",
    "        raise KeyError(f\"Column '{text_column}' not found\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Loading spaCy model...\")\n",
    "    \n",
    "    nlp = load_spacy_model(\"en_core_web_sm\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Loading NLTK stopwords...\")\n",
    "    stopwords_list = list(nltk_stopwords.words('english'))\n",
    "    \n",
    "    doc_ids = [f\"doc_{i}\" for i in range(len(df_export))]\n",
    "    texts = df_export[text_column].fillna(\"\").astype(str).tolist()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Lemmatizing {len(texts)} documents...\")\n",
    "    \n",
    "    lem_df = lemmatize_spacy_en(\n",
    "        x=texts,\n",
    "        model=nlp,\n",
    "        stopwords_list=stopwords_list,\n",
    "        doc_id=doc_ids,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    return lem_df\n",
    "\n",
    "# ============================================================================\n",
    "# 4. TERM-DOCUMENT MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "def create_term_document_matrix(texts, remove_punctuation=False, min_word_length=1, lowercase=True):\n",
    "    \"\"\"\n",
    "    Create Document-Term Matrix (equivalent to R's textmineR::CreateDtm)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : list\n",
    "        List of text documents\n",
    "    remove_punctuation : bool\n",
    "        If True, removes punctuation (default: False, matching R behavior)\n",
    "    min_word_length : int\n",
    "        Minimum word length to include (default: 1)\n",
    "    lowercase : bool\n",
    "        Convert tokens to lowercase (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (dtm_df: DataFrame with terms as rows, docs as columns), (vectorizer: fitted CountVectorizer)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CREATING DOCUMENT-TERM MATRIX (DTM)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Build token pattern based on punctuation handling\n",
    "    if remove_punctuation:\n",
    "        # Remove punctuation: keep only word characters and spaces\n",
    "        token_pattern = r'\\b\\w{' + str(min_word_length) + r',}\\b'\n",
    "    else:\n",
    "        # Keep punctuation as part of tokens (like R's textmineR)\n",
    "        token_pattern = r'(?u)\\b\\w+\\b|[^\\w\\s]'\n",
    "    \n",
    "    # Create vectorizer with options matching R's CreateDtm\n",
    "    vectorizer = CountVectorizer(\n",
    "        lowercase=lowercase,\n",
    "        token_pattern=token_pattern,\n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 1),  # Only unigrams\n",
    "        min_df=1,  # Include all terms\n",
    "        max_df=1.0\n",
    "    )\n",
    "    \n",
    "    # Fit and transform texts to get Document-Term Matrix\n",
    "    dtm_sparse = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Convert to DataFrame: rows=terms, columns=documents\n",
    "    # This matches R's textmineR output structure\n",
    "    dtm_df = pd.DataFrame(\n",
    "        dtm_sparse.toarray().T,\n",
    "        index=vectorizer.get_feature_names_out(),\n",
    "        columns=[f'doc_{i}' for i in range(dtm_sparse.shape[0])]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDTM Shape: {dtm_df.shape} (terms √ó documents)\")\n",
    "    print(f\"  ‚Ä¢ Total unique terms (vocabulary): {dtm_df.shape[0]}\")\n",
    "    print(f\"  ‚Ä¢ Total documents: {dtm_df.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ Sparsity: {(dtm_sparse.nnz / (dtm_sparse.shape[0] * dtm_sparse.shape[1]) * 100):.2f}%\")\n",
    "    \n",
    "    print(f\"\\nFirst 10 terms (rows) √ó all documents (columns):\")\n",
    "    print(dtm_df.head(10))\n",
    "    \n",
    "    print(f\"\\nTerm frequency statistics:\")\n",
    "    term_freq = dtm_df.sum(axis=1)\n",
    "    print(f\"  ‚Ä¢ Mean term frequency: {term_freq.mean():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Median term frequency: {term_freq.median():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Max term frequency: {term_freq.max():.0f}\")\n",
    "    print(f\"  ‚Ä¢ Min term frequency: {term_freq.min():.0f}\")\n",
    "    \n",
    "    return dtm_df, vectorizer\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"NLP TEXT PROCESSING PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_export = pd.read_csv('output.csv', sep=';') \n",
    "\n",
    "    print(f\"\\nLoaded {len(df_export)} documents from df_export['text']\")\n",
    "    \n",
    "    # ---- STEP 1: TEXT CLEANING ----\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 1: TEXT CLEANING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_export['text_cleaned'] = df_export['text'].apply(\n",
    "        lambda x: clean_text(x, hashtag=True, mention=True, numbers=False, \n",
    "                            punctuation=False, lowercase=True)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Cleaned {len(df_export)} documents\")\n",
    "    print(f\"Average text length before: {df_export['text'].str.len().mean():.0f} chars\")\n",
    "    print(f\"Average text length after: {df_export['text_cleaned'].str.len().mean():.0f} chars\")\n",
    "    \n",
    "    # Show first 3 examples\n",
    "    print(\"\\nFirst 3 Before/After Examples:\")\n",
    "    for i, row in df_export.head(3).iterrows():\n",
    "        print(f\"\\nüìÑ ROW {i} BEFORE:\\n{row['text'][:100]}...\")\n",
    "        print(f\"‚ú® AFTER:\\n{row['text_cleaned'][:100]}...\")\n",
    "    \n",
    "    # ---- STEP 2: COLLOCATIONS ----\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: EXTRACTING COLLOCATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_export, out_pos, out_pmi = apply_pipeline_on_df(df_export, text_col='text_cleaned')\n",
    "    \n",
    "    # Verify generated files\n",
    "    check_collocation_files(\"colloc_POS.xlsx\", \"colloc_PMI.xlsx\")\n",
    "    \n",
    "    # ---- STEP 3: LEMMATIZATION ----\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: LEMMATIZATION WITH STOPWORD REMOVAL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    lemmatized_tokens = apply_lemmatization_to_df(df_export, text_column='text_cleaned')\n",
    "    \n",
    "    print(f\"\\nTotal tokens extracted: {len(lemmatized_tokens)}\")\n",
    "    print(f\"Unique documents: {lemmatized_tokens['doc_id'].nunique()}\")\n",
    "    print(f\"Stopwords found: {lemmatized_tokens['STOP'].sum()}\")\n",
    "    print(f\"Content words: {(~lemmatized_tokens['STOP']).sum()}\")\n",
    "    \n",
    "    first_doc = lemmatized_tokens['doc_id'].iloc[0]\n",
    "    sample_tokens = lemmatized_tokens[lemmatized_tokens['doc_id'] == first_doc].head(15)\n",
    "    \n",
    "    print(f\"\\nSample tokens from {first_doc}:\")\n",
    "    print(sample_tokens[['token', 'lemma', 'upos', 'STOP']].to_string(index=False))\n",
    "    \n",
    "    print(\"\\nMost common content words:\")\n",
    "    print(lemmatized_tokens[~lemmatized_tokens['STOP']]['lemma'].value_counts().head(10))\n",
    "    \n",
    "    lemmatized_tokens.to_csv('lemmatized_tokens.csv', index=False, encoding='utf-8')\n",
    "    print(f\"\\nüíæ Lemmatized tokens saved to 'lemmatized_tokens.csv'\")\n",
    "    \n",
    "\n",
    "    # ---- STEP 3.5: CREATE text_nostop COLUMN (NO STOPWORDS) ----\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3.5: CREATING text_nostop COLUMN (STOPWORDS REMOVED)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    def reconstruct_text_without_stopwords(doc_id, lemmatized_df):\n",
    "        \"\"\"Ricostruisce il testo usando solo lemmi NON-stopword\"\"\"\n",
    "        content_lemmas = lemmatized_df[\n",
    "            (lemmatized_df['doc_id'] == doc_id) & \n",
    "            (lemmatized_df['STOP'] == False)\n",
    "        ]['lemma'].tolist()\n",
    "        return ' '.join(content_lemmas)\n",
    "\n",
    "    # Crea colonna text_nostop\n",
    "    print(\"Building text_nostop column...\")\n",
    "    df_export['text_nostop'] = [\n",
    "        reconstruct_text_without_stopwords(f'doc_{i}', lemmatized_tokens)\n",
    "        for i in range(len(df_export))\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n‚úì Created 'text_nostop' column in df_export\")\n",
    "\n",
    "    # Statistics\n",
    "    avg_len_with = df_export['text_cleaned'].str.split().str.len().mean()\n",
    "    avg_len_without = df_export['text_nostop'].str.split().str.len().mean()\n",
    "    reduction = ((avg_len_with - avg_len_without) / avg_len_with * 100)\n",
    "\n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"  Average tokens WITH stopwords (text_cleaned): {avg_len_with:.1f}\")\n",
    "    print(f\"  Average tokens WITHOUT stopwords (text_nostop): {avg_len_without:.1f}\")\n",
    "    print(f\"  Reduction: {reduction:.1f}%\")\n",
    "\n",
    "    # Show examples\n",
    "    print(\"\\nüìÑ First 3 Examples (text_cleaned vs text_nostop):\")\n",
    "    for i in range(min(3, len(df_export))):\n",
    "        print(f\"\\n--- Document {i} ---\")\n",
    "        print(f\"WITH stopwords:    {df_export.iloc[i]['text_cleaned'][:120]}...\")\n",
    "        print(f\"WITHOUT stopwords: {df_export.iloc[i]['text_nostop'][:120]}...\")\n",
    "\n",
    "\n",
    "    # ---- STEP 4: DOCUMENT-TERM MATRIX (FROM text_nostop) ----\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 4: DOCUMENT-TERM MATRIX (DTM) - WITHOUT STOPWORDS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Use text_nostop (no stopwords) instead of text_cleaned\n",
    "    texts = df_export['text_nostop'].tolist()\n",
    "\n",
    "    dtm_df, vectorizer = create_term_document_matrix(\n",
    "        texts, \n",
    "        remove_punctuation=False,\n",
    "        min_word_length=1,\n",
    "        lowercase=True\n",
    "    )\n",
    "    dtm_df.to_csv('document_term_matrix.csv')\n",
    "    print(f\"\\nüíæ Document-Term Matrix (NO STOPWORDS) saved to 'document_term_matrix.csv'\")\n",
    "    \n",
    "    # ---- FINAL SUMMARY ----\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PIPELINE COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüìä PROCESSING STATISTICS:\")\n",
    "    print(f\"  Total documents processed: {len(df_export)}\")\n",
    "    print(f\"  Total tokens extracted: {len(lemmatized_tokens)}\")\n",
    "    print(f\"  Average tokens per document: {len(lemmatized_tokens) / lemmatized_tokens['doc_id'].nunique():.1f}\")\n",
    "    print(f\"  Content words (non-stopwords): {(~lemmatized_tokens['STOP']).sum()}\")\n",
    "    print(f\"  Stopwords filtered: {lemmatized_tokens['STOP'].sum()}\")\n",
    "    print(f\"  POS-based collocations found: {len(out_pos)}\")\n",
    "    print(f\"  PMI-based collocations found: {len(out_pmi)}\")\n",
    "    #print(f\"  Unique terms in vocabulary: {tdm_df.shape[0]}\")\n",
    "    \n",
    "    print(f\"\\nüíæ GENERATED FILES:\")\n",
    "    print(f\"  ‚úì colloc_POS.xlsx - POS-based collocations ({len(out_pos)} rows)\")\n",
    "    print(f\"  ‚úì colloc_PMI.xlsx - PMI-based collocations ({len(out_pmi)} rows)\")\n",
    "    print(f\"  ‚úì lemmatized_tokens.csv - All lemmatized tokens with stopword flags\")\n",
    "    print(f\"  ‚úì document_term_matrix.csv - DTM Matrix ({dtm_df.shape[0]} terms √ó {dtm_df.shape[1]} docs)\")\n",
    "    \n",
    "    print(f\"\\nüìà TOP INSIGHTS:\")\n",
    "    \n",
    "    print(f\"\\n  Top 10 Most Frequent Content Words:\")\n",
    "    top_words = lemmatized_tokens[~lemmatized_tokens['STOP']]['lemma'].value_counts().head(10)\n",
    "    for word, freq in top_words.items():\n",
    "        print(f\"    ‚Ä¢ {word}: {freq}\")\n",
    "    \n",
    "    print(f\"\\n  Top 5 POS Collocations by Frequency:\")\n",
    "    if len(out_pos) > 0:\n",
    "        for _, row in out_pos.head(5).iterrows():\n",
    "            print(f\"    ‚Ä¢ {row['collocation']} ({row['pos_pattern']}): freq={row['freq']}\")\n",
    "    \n",
    "    print(f\"\\n  Top 5 PMI Collocations:\")\n",
    "    if len(out_pmi) > 0:\n",
    "        for _, row in out_pmi.head(5).iterrows():\n",
    "            pmi_val = f\"{row['pmi']:.2f}\" if row['pmi'] != float('-inf') else \"inf\"\n",
    "            print(f\"    ‚Ä¢ {row['collocation']}: PMI={pmi_val}, freq={row['freq']}\")\n",
    "    \n",
    "    print(f\"\\n  Document Statistics:\")\n",
    "    doc_token_counts = lemmatized_tokens.groupby('doc_id').size()\n",
    "    print(f\"    ‚Ä¢ Average tokens per document: {doc_token_counts.mean():.1f}\")\n",
    "    print(f\"    ‚Ä¢ Max tokens in a document: {doc_token_counts.max()}\")\n",
    "    print(f\"    ‚Ä¢ Min tokens in a document: {doc_token_counts.min()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_export['text_nostop'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d69b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea copia e pulisci in un passaggio\n",
    "df_clean = df_export.copy()\n",
    "\n",
    "# Pulisci tutte le colonne di tipo object/string\n",
    "df_clean = df_clean.apply(\n",
    "    lambda x: x.str.replace('\\t|\\r|\\n', ' ', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip() \n",
    "    if x.dtype == 'object' else x\n",
    ")\n",
    "\n",
    "# Esporta\n",
    "df_clean.to_csv(\n",
    "    'df_clean.csv', \n",
    "    sep=';', \n",
    "    index=False, \n",
    "    encoding='utf-8',\n",
    "    quoting=csv.QUOTE_NONNUMERIC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca309de",
   "metadata": {},
   "source": [
    "## Explorative data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b533568",
   "metadata": {},
   "source": [
    "#### Remove duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db63df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_clean.csv', sep=';')\n",
    "\n",
    "df['id'] = range(len(df))\n",
    "\n",
    "# Deletion of repeated texts\n",
    "dfControl = df.groupby('text_nostop').agg({'id': 'min'}).reset_index()\n",
    "dfControl['n'] = df.groupby('text_nostop').size().values\n",
    "dfControl = dfControl.sort_values('n', ascending=False)\n",
    "\n",
    "df = df[df['id'].isin(dfControl['id'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8fe0c",
   "metadata": {},
   "source": [
    "#### Remove useless frequent stopwords and comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_nostop'] = df['text_nostop'].str.replace(r\"(?i)\\b(also|say|could|make)s?\\b\", \"\", regex=True)\n",
    "df[\"text_nostop\"] = df[\"text_nostop\"].str.replace(\",\", \"\", regex=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91286d66",
   "metadata": {},
   "source": [
    "#### Which is the most frequent ticker ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendi i top 10 ticker dal tuo snippet\n",
    "top_tickers = df['tickers'].str.split(',').explode().value_counts().head(10).index\n",
    "\n",
    "# Espandi i tickers nel dataframe (una riga per ciascun ticker)\n",
    "df_exploded = df.copy()\n",
    "df_exploded = df_exploded.assign(ticker = df_exploded['tickers'].str.split(',')).explode('ticker')\n",
    "\n",
    "# Filtra solo i top 10\n",
    "df_top = df_exploded[df_exploded['ticker'].isin(top_tickers)]\n",
    "\n",
    "# Raggruppa per ticker e calcola numero di news, prima e ultima data\n",
    "ticker_stats = df_top.groupby('ticker').agg(\n",
    "    n_news=('ticker', 'count'),\n",
    "    first_date=('parsed_date', 'min'),\n",
    "    last_date=('parsed_date', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Ordina per numero di news decrescente\n",
    "ticker_stats = ticker_stats.sort_values(by='n_news', ascending=False)\n",
    "\n",
    "# Mostra il risultato\n",
    "print(ticker_stats)\n",
    "\n",
    "# Get top 30 tickers\n",
    "ticker_counts = df['tickers'].str.split(',').explode().value_counts().head(30)\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "ticker_counts.plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Ticker')\n",
    "plt.title('30 Most Frequent Tickers')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9920b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# Parametri\n",
    "# -------------------------\n",
    "tickers_of_interest = ['BTC-USD', 'NVDA', 'OPAI.PVT']\n",
    "start_date = \"2025-10-01\"\n",
    "end_date   = \"2025-10-20\"\n",
    "\n",
    "# -------------------------\n",
    "# 1. Assicura datetime\n",
    "# -------------------------\n",
    "df['parsed_date'] = pd.to_datetime(df['parsed_date'], errors='coerce')\n",
    "\n",
    "# -------------------------\n",
    "# 2. Filtra per range di date (solo giorno)\n",
    "# -------------------------\n",
    "start = pd.to_datetime(start_date).date()\n",
    "end   = pd.to_datetime(end_date).date()\n",
    "mask = df['parsed_date'].dt.date.between(start, end)\n",
    "df_period = df.loc[mask].copy()\n",
    "\n",
    "# -------------------------\n",
    "# 3. Espandi i ticker separati da virgola e normalizza\n",
    "# -------------------------\n",
    "# Se ci sono NaN, trasformali in stringa vuota per evitare errori\n",
    "df_period['tickers'] = df_period['tickers'].fillna('')\n",
    "\n",
    "# split + explode\n",
    "df_exploded = df_period.assign(ticker = df_period['tickers'].str.split(',')).explode('ticker')\n",
    "\n",
    "# strip spazi e rimuovi eventuali ticker vuoti\n",
    "df_exploded['ticker'] = df_exploded['ticker'].astype(str).str.strip()\n",
    "df_exploded = df_exploded[df_exploded['ticker'] != ''].copy()\n",
    "\n",
    "# -------------------------\n",
    "# 4. Raggruppa per ticker e giorno (date senza orario)\n",
    "# -------------------------\n",
    "# crea una colonna date_only come datetime (00:00:00) per poter reindexare con date_range\n",
    "df_exploded['date_only'] = pd.to_datetime(df_exploded['parsed_date'].dt.date)\n",
    "\n",
    "daily_counts = (\n",
    "    df_exploded\n",
    "    .groupby(['ticker', 'date_only'])\n",
    "    .size()\n",
    "    .reset_index(name='n_articles')\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5. Pivot e reindex robusto\n",
    "# -------------------------\n",
    "daily_counts_pivot = daily_counts.pivot(index='date_only', columns='ticker', values='n_articles').fillna(0)\n",
    "\n",
    "# crea indice giornaliero completo (DatetimeIndex) e reindexa per avere tutti i giorni\n",
    "full_index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "daily_counts_pivot = daily_counts_pivot.reindex(full_index, fill_value=0)\n",
    "daily_counts_pivot.index.name = 'date'\n",
    "\n",
    "# assicurati che le colonne contengano esattamente i ticker che ti interessano\n",
    "# se un ticker non √® presente viene creato con tutti zeri (evita KeyError)\n",
    "for t in tickers_of_interest:\n",
    "    if t not in daily_counts_pivot.columns:\n",
    "        daily_counts_pivot[t] = 0\n",
    "\n",
    "# ordina le colonne come nella lista richiesta\n",
    "daily_counts_pivot = daily_counts_pivot[tickers_of_interest]\n",
    "\n",
    "# -------------------------\n",
    "# 6. Plot\n",
    "# -------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import sys\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Palette professionale (toni di blu)\n",
    "colors = [\"#1f77b4\", \"#869fb8\", \"#0b3d91\"]  # tre tonalit√† di blu\n",
    "\n",
    "for i, ticker in enumerate(tickers_of_interest):\n",
    "    plt.plot(\n",
    "        daily_counts_pivot.index,\n",
    "        daily_counts_pivot[ticker],\n",
    "        color=colors[i % len(colors)],\n",
    "        linewidth=2,\n",
    "        label=ticker\n",
    "    )\n",
    "\n",
    "plt.title(f\"Daily number of articles for {', '.join(tickers_of_interest)}\", fontsize=14, weight='bold')\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Number of articles\", fontsize=12)\n",
    "\n",
    "# Formatta l'asse x: giorno + abbreviazione del mese, orizzontale\n",
    "ax = plt.gca()\n",
    "\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%#d %b'))  # Windows\n",
    "else:\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%-d %b'))  # macOS/Linux\n",
    "\n",
    "plt.xticks(rotation=0, fontsize=10)\n",
    "plt.legend(frameon=False, fontsize=11)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9675207d",
   "metadata": {},
   "source": [
    "#### Lexicometric measures (N, V ,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fff0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparazione testi: converti in stringhe e riempi NaN con stringhe vuote\n",
    "texts = df['text_nostop'].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# --- 1) Creazione della Document-Term Matrix (conteggi) ---\n",
    "# token_pattern con \\w{2,} per includere solo token di lunghezza >= 2 (equivalente a wordLengths = c(2, Inf))\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w{2,}\\b', lowercase=True)\n",
    "dtm = vectorizer.fit_transform(texts)   # matrice (n_docs x n_terms) sparse\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# --- 2) DataFrame dei termini con frequenze complessive\n",
    "term_freq = np.array(dtm.sum(axis=0)).flatten()   # somma per colonna -> frequenza totale di ciascun termine\n",
    "dfTerms2 = pd.DataFrame({'term': terms, 'Freq': term_freq}).sort_values('Freq', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# --- 3) Calcoli lessicometrici ---\n",
    "N = int(dfTerms2['Freq'].sum())            # corpus size: totale occorrenze\n",
    "V = int(dfTerms2.shape[0])                 # vocabulary size: numero termini distinti\n",
    "H = int((dfTerms2['Freq'] == 1).sum())     # hapax: termini che compaiono una sola volta\n",
    "\n",
    "TTR = V / N if N > 0 else np.nan           # Type-Token Ratio\n",
    "HTR = H / V if V > 0 else np.nan           # Hapax-Type Ratio\n",
    "TMF = N / V if V > 0 else np.nan           # Type Mean Frequency\n",
    "G = V / np.sqrt(N) if N > 0 else np.nan    # Guiraud Index\n",
    "\n",
    "# Stampa risultati simili a cbind in R\n",
    "metrics = pd.DataFrame({\n",
    "    'N': [N],\n",
    "    'V': [V],\n",
    "    'H': [H],\n",
    "    'TTR': [TTR],\n",
    "    'HTR': [HTR],\n",
    "    'TMF': [TMF],\n",
    "    'G': [G]\n",
    "})\n",
    "print(metrics.to_string(index=False))\n",
    "\n",
    "# --- 4) Numero di token/lemmi per documento (nlem2) ---\n",
    "# Qui interpretiamo \"lemmi per review\" come numero di token nella stringa gi√† pre-elaborata;\n",
    "# si pu√≤ usare la matrice dtm per contare parole per documento:\n",
    "nlem2 = np.array(dtm.sum(axis=1)).flatten()   # somma riga per documento\n",
    "\n",
    "# Statistiche descrittive (equivalente a summary(nlem2))\n",
    "desc = {\n",
    "    'Min': nlem2.min(),\n",
    "    '1st Qu.': np.percentile(nlem2, 25),\n",
    "    'Median': np.median(nlem2),\n",
    "    'Mean': nlem2.mean(),\n",
    "    '3rd Qu.': np.percentile(nlem2, 75),\n",
    "    'Max': nlem2.max()\n",
    "}\n",
    "print(\"\\nSummary of number of lemmas (tokens) per news:\")\n",
    "for k, v in desc.items():\n",
    "    # stampo valori con due decimali per Mean\n",
    "    if k == 'Mean':\n",
    "        print(f\"{k:8s}: {v:.2f}\")\n",
    "    else:\n",
    "        print(f\"{k:8s}: {int(v)}\")\n",
    "\n",
    "# --- 5) Aggiungo la colonna n.lemma a df (equivalente a mutate(n.lemma=nlem2)) ---\n",
    "df = df.copy()\n",
    "df['n_lemma'] = nlem2\n",
    "\n",
    "# --- 6) Istogramma della distribuzione del numero di lemmi per review ---\n",
    "max_n = int(nlem2.max())\n",
    "bins = np.arange(0, max_n + 10, 10)  # breakpoints every 10 (0,10,20,...)\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.hist(nlem2, bins=bins, edgecolor='gray', linewidth=0.5)\n",
    "plt.title(\"Distribution of the number of lemmas per news article\")\n",
    "plt.xlabel(\"Number of lemmas (tokens) per news\")\n",
    "plt.ylabel(\"Count of news\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4523b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tickers_of_interest = [\"BTC-USD\", \"NVDA\"]\n",
    "\n",
    "# Assicurati che 'tickers' non contenga NaN\n",
    "df_plot = df.copy()\n",
    "df_plot['tickers'] = df_plot['tickers'].fillna(\"\")\n",
    "\n",
    "# Crea colonna ticker_filtered: lista dei ticker presenti\n",
    "df_plot['ticker_filtered'] = df_plot['tickers'].apply(\n",
    "    lambda x: [t for t in tickers_of_interest if t in x]\n",
    ")\n",
    "df_plot = df_plot.explode('ticker_filtered')\n",
    "df_plot = df_plot.dropna(subset=['ticker_filtered'])\n",
    "\n",
    "# Imposta figura con 2 subplot affiancati\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5), sharey=True)\n",
    "\n",
    "# Colori professionali\n",
    "colors = [\"#1f77b4\", \"#869fb8\"]\n",
    "\n",
    "for i, ticker in enumerate(tickers_of_interest):\n",
    "    sns.histplot(\n",
    "        data=df_plot[df_plot['ticker_filtered'] == ticker],\n",
    "        x='n_lemma',\n",
    "        bins=30,\n",
    "        color=colors[i],\n",
    "        edgecolor='gray',\n",
    "        alpha=0.7,\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f\"Distribution of lemmas for {ticker}\", fontsize=12, weight='bold')\n",
    "    axes[i].set_xlabel(\"Number of lemmas per news\")\n",
    "    axes[i].set_ylabel(\"Count of news\")\n",
    "    axes[i].grid(axis='y', alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3175c63",
   "metadata": {},
   "source": [
    "#### Top 10 most frequent lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd509d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 terms by total frequency:\")\n",
    "print(dfTerms2.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7166f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Prendi le prime 20 righe\n",
    "top_terms = dfTerms2.head(20)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    data=top_terms,\n",
    "    y='term',          \n",
    "    x='Freq',          \n",
    "    color=\"#4c9dd6\"\n",
    ")\n",
    "\n",
    "# Titolo pi√π elegante\n",
    "plt.title(\"20 most frequent lemmas\", \n",
    "          fontsize=13,       \n",
    "          #weight='bold',      \n",
    "          pad=20,             \n",
    "          loc='left')       \n",
    "\n",
    "\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Freq\", fontsize=10, weight='bold')\n",
    "plt.grid(axis='x', alpha=0.2)  \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTerms2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638cf3f",
   "metadata": {},
   "source": [
    "### New Wordcloud for News Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5afafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# --- Creiamo una maschera circolare (nero=riempito, bianco=vuoto) ---\n",
    "size = 1200\n",
    "x, y = np.ogrid[:size, :size]\n",
    "radius = size // 2\n",
    "mask = (x - radius)**2 + (y - radius)**2 <= radius**2\n",
    "mask = 255 * mask.astype(int)   # 255 = nero\n",
    "mask = 255 - mask                # invertiamo: testo dentro il cerchio\n",
    "\n",
    "# --- Creiamo una funzione di colorazione personalizzata ---\n",
    "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    # Otteniamo la frequenza della parola\n",
    "    freq = word_freq_dict[word]\n",
    "    # Normalizziamo tra 0.3 e 1 (iniziamo da 0.3 per evitare colori troppo chiari)\n",
    "    normalized = 0.3 + 0.7 * ((freq - min_freq) / (max_freq - min_freq)) if max_freq != min_freq else 0.65\n",
    "    \n",
    "    # Usiamo la colormap Blues standard di matplotlib\n",
    "    blues = cm.get_cmap('Blues')\n",
    "    rgb = blues(normalized)[:3]  # Prendiamo solo RGB, non alpha\n",
    "    \n",
    "    return f\"rgb({int(rgb[0]*255)}, {int(rgb[1]*255)}, {int(rgb[2]*255)})\"\n",
    "\n",
    "# --- Prepariamo il dizionario delle frequenze ---\n",
    "word_freq_dict = dict(zip(dfTerms2['term'], dfTerms2['Freq']))\n",
    "max_freq = dfTerms2['Freq'].max()\n",
    "min_freq = dfTerms2['Freq'].min()\n",
    "\n",
    "# --- WordCloud ---\n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=150,\n",
    "    mask=mask,\n",
    "    contour_width=0,\n",
    "    width=size,\n",
    "    height=size\n",
    ")\n",
    "\n",
    "# --- Generiamo dai termini/frequenze ---\n",
    "wc.generate_from_frequencies(word_freq_dict)\n",
    "\n",
    "# --- Applichiamo la funzione di colorazione ---\n",
    "wc.recolor(color_func=color_func)\n",
    "\n",
    "# --- Visualizzazione ---\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('wordcloud_bitcoin.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddaf71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# 1Ô∏è‚É£ FILTRAGGIO E PREPARAZIONE\n",
    "# -----------------------------\n",
    "\n",
    "# Funzione per capire se un articolo parla di BTC o NVDA\n",
    "def contains_ticker(tickers_str, ticker):\n",
    "    if pd.isna(tickers_str):\n",
    "        return False\n",
    "    return ticker in [t.strip() for t in tickers_str.split(',')]\n",
    "\n",
    "# Filtriamo gli articoli che contengono almeno uno dei due tickers\n",
    "df = df[df['tickers'].apply(lambda x: contains_ticker(x, 'BTC-USD') or contains_ticker(x, 'NVDA'))].copy()\n",
    "\n",
    "# Creiamo una colonna \"group\" con il tipo di ticker dominante (BTC o NVDA)\n",
    "def assign_group(tickers_str):\n",
    "    tickers = [t.strip() for t in tickers_str.split(',')]\n",
    "    if 'BTC-USD' in tickers and 'NVDA' not in tickers:\n",
    "        return 'BTC-USD'\n",
    "    elif 'NVDA' in tickers and 'BTC-USD' not in tickers:\n",
    "        return 'NVDA'\n",
    "    elif 'BTC-USD' in tickers and 'NVDA' in tickers:\n",
    "        return 'BOTH'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['group'] = df['tickers'].apply(assign_group)\n",
    "df = df[df['group'].isin(['BTC-USD', 'NVDA'])]  # rimuoviamo quelli con BOTH per evitare ambiguit√†\n",
    "\n",
    "# Raggruppiamo e uniamo i testi per gruppo (come in R con summarise)\n",
    "texts = df.groupby('group')['text_nostop'].apply(lambda x: ' '.join(x)).to_dict()\n",
    "\n",
    "# -----------------------------\n",
    "# 2Ô∏è‚É£ CREAZIONE MATRICE TERMINI-DOCUMENTI\n",
    "# -----------------------------\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, token_pattern=r'\\b[a-zA-Z]{2,}\\b')  # parole con almeno 2 lettere\n",
    "X = vectorizer.fit_transform([texts['BTC-USD'], texts['NVDA']])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Matrice termine-documento\n",
    "tdm = pd.DataFrame(X.toarray().T, index=terms, columns=['BTC-USD', 'NVDA'])\n",
    "\n",
    "# -----------------------------\n",
    "# 3Ô∏è‚É£ CALCOLO FREQUENZE RELATIVE\n",
    "# -----------------------------\n",
    "\n",
    "tdm['rfBTC'] = tdm['BTC-USD'] / tdm['BTC-USD'].sum()\n",
    "tdm['rfNVDA'] = tdm['NVDA'] / tdm['NVDA'].sum()\n",
    "\n",
    "# -----------------------------\n",
    "# 4Ô∏è‚É£ SCATTER PLOT FREQUENZE RELATIVE (con etichette)\n",
    "# -----------------------------\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=tdm, x='rfBTC', y='rfNVDA', alpha=0.6, s=30)\n",
    "\n",
    "# Linea diagonale (parit√† di frequenze)\n",
    "max_val = max(tdm['rfBTC'].max(), tdm['rfNVDA'].max())\n",
    "plt.plot([0, max_val], [0, max_val], color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "# Etichette parole\n",
    "# (mostriamo solo le parole pi√π frequenti per evitare sovrapposizione)\n",
    "tdm_sorted = tdm.sort_values(by=['rfBTC', 'rfNVDA'], ascending=False)\n",
    "subset = tdm_sorted.head(100)  # mostra fino a 100 parole pi√π frequenti\n",
    "for i, row in subset.iterrows():\n",
    "    plt.text(row['rfBTC'], row['rfNVDA'], i, fontsize=8, alpha=0.7)\n",
    "\n",
    "# Scala log per leggibilit√†\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Frequenza relativa - BTC-USD')\n",
    "plt.ylabel('Frequenza relativa - NVDA')\n",
    "plt.title('Confronto delle distribuzioni delle parole (BTC vs NVDA)')\n",
    "plt.grid(True, which=\"both\", ls=\"--\", lw=0.5, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5Ô∏è‚É£ CALCOLO LOG ODDS RATIO\n",
    "# -----------------------------\n",
    "\n",
    "tdm['pBTC'] = (tdm['BTC-USD'] + 1) / (tdm['BTC-USD'].sum() + 1)\n",
    "tdm['pNVDA'] = (tdm['NVDA'] + 1) / (tdm['NVDA'].sum() + 1)\n",
    "tdm['log_ratio'] = np.log(tdm['pBTC'] / tdm['pNVDA'])\n",
    "\n",
    "# -----------------------------\n",
    "# 6Ô∏è‚É£ GRAFICO DEL LOG ODDS RATIO\n",
    "# -----------------------------\n",
    "\n",
    "# Selezioniamo le parole pi√π caratteristiche\n",
    "top_words = (\n",
    "    tdm.reindex(tdm['log_ratio'].abs().sort_values(ascending=False).index)\n",
    "    .head(30)\n",
    "    .assign(word=lambda d: d.index)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.barplot(\n",
    "    data=top_words,\n",
    "    y='word',\n",
    "    x='log_ratio',\n",
    "    hue=(top_words['log_ratio'] < 0),\n",
    "    dodge=False,\n",
    "    palette={False: 'steelblue', True: 'seagreen'}  # colori validi\n",
    ")\n",
    "plt.axvline(0, color='black', linewidth=1)\n",
    "plt.ylabel('Parole')\n",
    "plt.xlabel('Log Odds Ratio (BTC-USD / NVDA)')\n",
    "plt.title('Parole pi√π caratteristiche tra BTC e NVDA')\n",
    "plt.legend(title='', labels=['BTC-USD', 'NVDA'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 7Ô∏è‚É£ OPZIONALE: OUTPUT PER WORDCLOUD\n",
    "# -----------------------------\n",
    "\n",
    "# Puoi riusare questa variabile con il tuo script WordCloud\n",
    "dfTerms2 = (\n",
    "    tdm[['BTC-USD', 'NVDA', 'rfBTC', 'rfNVDA', 'log_ratio']]\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'term'})\n",
    ")\n",
    "\n",
    "# Ad esempio per creare una wordcloud solo per BTC:\n",
    "dfTerms2_btc = dfTerms2[['term', 'BTC-USD']].rename(columns={'BTC-USD': 'Freq'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# DATI: si assume che tu abbia gi√† df pronto\n",
    "# -----------------------------\n",
    "\n",
    "def contains_ticker(tickers_str, ticker):\n",
    "    if pd.isna(tickers_str):\n",
    "        return False\n",
    "    return ticker in [t.strip() for t in tickers_str.split(',')]\n",
    "\n",
    "df = df[df['tickers'].apply(lambda x: contains_ticker(x, 'BTC-USD') or contains_ticker(x, 'NVDA'))].copy()\n",
    "\n",
    "def assign_group(tickers_str):\n",
    "    tickers = [t.strip() for t in tickers_str.split(',')]\n",
    "    if 'BTC-USD' in tickers and 'NVDA' not in tickers:\n",
    "        return 'BTC-USD'\n",
    "    elif 'NVDA' in tickers and 'BTC-USD' not in tickers:\n",
    "        return 'NVDA'\n",
    "    elif 'BTC-USD' in tickers and 'NVDA' in tickers:\n",
    "        return 'BOTH'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['group'] = df['tickers'].apply(assign_group)\n",
    "df = df[df['group'].isin(['BTC-USD', 'NVDA'])]\n",
    "\n",
    "texts = df.groupby('group')['text_nostop'].apply(lambda x: ' '.join(x)).to_dict()\n",
    "\n",
    "# -----------------------------\n",
    "# MATRICE TERMINI-DOCUMENTI\n",
    "# -----------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "X = vectorizer.fit_transform([texts['BTC-USD'], texts['NVDA']])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "tdm = pd.DataFrame(X.toarray().T, index=terms, columns=['BTC-USD', 'NVDA'])\n",
    "tdm['rfBTC'] = tdm['BTC-USD'] / tdm['BTC-USD'].sum()\n",
    "tdm['rfNVDA'] = tdm['NVDA'] / tdm['NVDA'].sum()\n",
    "\n",
    "# -----------------------------\n",
    "# SCATTER PLOT (frequenze relative)\n",
    "# -----------------------------\n",
    "tdm['distance_from_diagonal'] = np.abs(np.log(tdm['rfBTC'] + 1e-10) - np.log(tdm['rfNVDA'] + 1e-10))\n",
    "tdm['total_freq'] = tdm['rfBTC'] + tdm['rfNVDA']\n",
    "\n",
    "btc_words = tdm[tdm['rfBTC'] > tdm['rfNVDA'] * 1.5].nlargest(15, 'rfBTC')\n",
    "nvda_words = tdm[tdm['rfNVDA'] > tdm['rfBTC'] * 1.5].nlargest(15, 'rfNVDA')\n",
    "common_words = tdm[\n",
    "    (tdm['rfBTC'] / (tdm['rfNVDA'] + 1e-10) > 0.7) & \n",
    "    (tdm['rfBTC'] / (tdm['rfNVDA'] + 1e-10) < 1.3)\n",
    "].nlargest(10, 'total_freq')\n",
    "\n",
    "words_to_label = pd.concat([btc_words, nvda_words, common_words])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tdm['rfBTC'], tdm['rfNVDA'], alpha=0.3, s=30, color='lightgray')\n",
    "plt.scatter(btc_words['rfBTC'], btc_words['rfNVDA'], color='black', s=50, alpha=0.8, edgecolors='black')\n",
    "plt.scatter(nvda_words['rfBTC'], nvda_words['rfNVDA'], color='black', s=50, alpha=0.8, edgecolors='black')\n",
    "plt.scatter(common_words['rfBTC'], common_words['rfNVDA'], color='black', s=50, alpha=0.8, edgecolors='black')\n",
    "\n",
    "max_val = max(tdm['rfBTC'].max(), tdm['rfNVDA'].max())\n",
    "plt.plot([0, max_val], [0, max_val], color='red', linestyle='-', linewidth=2, alpha=0.5)\n",
    "\n",
    "for word, row in words_to_label.iterrows():\n",
    "    plt.text(row['rfBTC'], row['rfNVDA'], word, fontsize=10, color='black', ha='left', va='bottom')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Frequenza relativa - BTC-USD (scala logaritmica)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Frequenza relativa - NVDA (scala logaritmica)', fontsize=13, fontweight='bold')\n",
    "plt.title('Confronto Distribuzioni Parole: BTC-USD vs NVDA', fontsize=15, fontweight='bold', pad=15)\n",
    "plt.grid(True, which=\"both\", ls=\"-\", lw=0.5, alpha=0.3, color='lightgray')\n",
    "plt.tight_layout()\n",
    "plt.savefig('scatter_plot_frequenze.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dec63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# -----------------------------\n",
    "# LOG ODDS RATIO\n",
    "# -----------------------------\n",
    "tdm['pBTC'] = (tdm['BTC-USD'] + 1) / (tdm['BTC-USD'].sum() + 1)\n",
    "tdm['pNVDA'] = (tdm['NVDA'] + 1) / (tdm['NVDA'].sum() + 1)\n",
    "tdm['log_ratio'] = np.log(tdm['pBTC'] / tdm['pNVDA'])\n",
    "\n",
    "top_words = (\n",
    "    tdm.reindex(tdm['log_ratio'].abs().sort_values(ascending=False).index)\n",
    "    .head(40)\n",
    "    .assign(word=lambda d: d.index)\n",
    "    .sort_values('log_ratio')\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# MINIMAL COLORS\n",
    "# -----------------------------\n",
    "color_btc = \"#1f77b4\"   # blue\n",
    "color_nvda = \"#869fb8\"  # light blue\n",
    "colors = [color_btc if x > 0 else color_nvda for x in top_words['log_ratio']]\n",
    "\n",
    "# -----------------------------\n",
    "# PLOT\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(10, 10))\n",
    "bars = plt.barh(\n",
    "    range(len(top_words)),\n",
    "    top_words['log_ratio'],\n",
    "    color=colors,\n",
    "    edgecolor='none'\n",
    ")\n",
    "\n",
    "plt.yticks(range(len(top_words)), top_words['word'], fontsize=11)\n",
    "plt.axvline(0, color='black', linewidth=1.2)\n",
    "\n",
    "# Labels inside the bars\n",
    "for i, (idx, row) in enumerate(top_words.iterrows()):\n",
    "    value = row['log_ratio']\n",
    "    # Space from the inner edge (slightly reduced for smaller font)\n",
    "    offset = 0.03 if value > 0 else -0.03\n",
    "    # Position text just inside the bar\n",
    "    x_pos = value - offset if value > 0 else value - offset\n",
    "    ha = 'right' if value > 0 else 'left'\n",
    "    plt.text(\n",
    "        x_pos, i, f'{value:.2f}',\n",
    "        va='center', ha=ha,\n",
    "        fontsize=8, \n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Log Odds Ratio (BTC-USD / NVDA)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Words', fontsize=13, fontweight='bold')\n",
    "plt.title('Characteristic Words: BTC-USD vs NVDA\\n(Positive = more BTC | Negative = more NVDA)',\n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=color_btc, label='More characteristic of BTC-USD'),\n",
    "    Patch(facecolor=color_nvda, label='More characteristic of NVDA')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='lower right', fontsize=11)\n",
    "\n",
    "plt.grid(axis='x', alpha=0.25, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.savefig('log_odds_ratio_inside_edge_labels.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d33c1",
   "metadata": {},
   "source": [
    "### Clustering Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w{2,}\\b')\n",
    "tdm_matrix = vectorizer.fit_transform(df['text_nostop'])\n",
    "tdm = tdm_matrix.T\n",
    "print(f\"Term Document Matrix: {tdm.shape[0]} terms, {tdm.shape[1]} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_docs = int(np.ceil(0.01 * tdm.shape[1]))\n",
    "term_counts = np.array((tdm > 0).sum(axis=1)).flatten()\n",
    "mask = term_counts >= min_docs\n",
    "tdm_rs = tdm[mask]\n",
    "terms = np.array(vectorizer.get_feature_names_out())[mask]\n",
    "\n",
    "print(f\"Reduced TDM: {tdm_rs.shape[0]} terms, {tdm_rs.shape[1]} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola distanze cosine\n",
    "tdm_rs_dense = tdm_rs.toarray()\n",
    "dst_cs_rs = pdist(tdm_rs_dense, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering con metodo Ward\n",
    "h_cl_cs_rs = linkage(dst_cs_rs, method='ward')\n",
    "\n",
    "# Plot dendrogram SENZA colori (tutto nero come in R)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Hierarchical Cluster\\ncosine - Ward\", fontsize=12)\n",
    "dendrogram(h_cl_cs_rs, \n",
    "           labels=terms, \n",
    "           leaf_font_size=2,\n",
    "           color_threshold=0,  \n",
    "           above_threshold_color=\"#111111\")  \n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(fontsize=6)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola Silhouette scores per diversi numeri di cluster\n",
    "lim_clu = range(2, 21)\n",
    "vSilIn = []\n",
    "\n",
    "dst_cs_rs_matrix = squareform(dst_cs_rs)\n",
    "\n",
    "for k in lim_clu:\n",
    "    clusters = fcluster(h_cl_cs_rs, k, criterion='maxclust')\n",
    "    sil_score = silhouette_score(dst_cs_rs_matrix, clusters, metric='precomputed')\n",
    "    vSilIn.append(sil_score)\n",
    "\n",
    "# Plot Silhouette scores\n",
    "df_sil = pd.DataFrame({'n_clust': list(lim_clu), 'silhouette': vSilIn})\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_sil['n_clust'], df_sil['silhouette'], color='blue', linewidth=2, marker='o', markersize=6)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette score')\n",
    "plt.xticks(lim_clu)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd002750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot dendrogram con rettangoli per k=6 cluster\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Crea il dendrogramma con TUTTI i rami neri\n",
    "dend = dendrogram(h_cl_cs_rs, \n",
    "                  labels=terms, \n",
    "                  leaf_font_size=6,\n",
    "                  color_threshold=0,  # Tutto nero\n",
    "                  above_threshold_color='black',  # Colore nero per tutti i rami\n",
    "                  ax=ax)\n",
    "\n",
    "# Ottieni i cluster per k=6\n",
    "clu6 = fcluster(h_cl_cs_rs, 6, criterion='maxclust')\n",
    "\n",
    "# Calcola l'altezza di taglio per k=6 cluster\n",
    "# Questo √® il punto dove il dendrogramma viene tagliato per ottenere k cluster\n",
    "n = len(h_cl_cs_rs) + 1\n",
    "# L'altezza di taglio √® tra la (n-k)esima e la (n-k+1)esima fusione\n",
    "cut_height = (h_cl_cs_rs[-(6-1), 2] + h_cl_cs_rs[-(6), 2]) / 2\n",
    "\n",
    "# Funzione per disegnare i rettangoli esattamente come rect.hclust in R\n",
    "def rect_hclust_r_style(linkage_matrix, k, dend_dict, cut_height, ax, color='red', linewidth=1.2):\n",
    "    \"\"\"\n",
    "    Replica ESATTAMENTE rect.hclust() di R\n",
    "    \"\"\"\n",
    "    clusters = fcluster(linkage_matrix, k, criterion='maxclust')\n",
    "    \n",
    "    # Ottieni le posizioni x delle foglie\n",
    "    leaves = np.array(dend_dict['leaves'])\n",
    "    leaf_x = {leaf: (i * 10 + 5) for i, leaf in enumerate(leaves)}\n",
    "    \n",
    "    # Per ogni cluster, disegna un rettangolo\n",
    "    for cluster_id in range(1, k + 1):\n",
    "        # Trova le foglie del cluster\n",
    "        cluster_indices = np.where(clusters == cluster_id)[0]\n",
    "        \n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Trova le posizioni x delle foglie nel dendrogramma\n",
    "        x_positions = []\n",
    "        for idx in cluster_indices:\n",
    "            if idx in leaf_x:\n",
    "                x_positions.append(leaf_x[idx])\n",
    "        \n",
    "        if len(x_positions) == 0:\n",
    "            continue\n",
    "        \n",
    "        x_min = min(x_positions)\n",
    "        x_max = max(x_positions)\n",
    "        \n",
    "        # Usa l'altezza di taglio come altezza del rettangolo\n",
    "        # Questo √® il comportamento di rect.hclust in R\n",
    "        y_max = cut_height\n",
    "        \n",
    "        # Disegna il rettangolo\n",
    "        rect_width = x_max - x_min + 10\n",
    "        rect = plt.Rectangle((x_min - 5, 0), rect_width, y_max,\n",
    "                            fill=False, \n",
    "                            edgecolor=color, \n",
    "                            linewidth=linewidth,\n",
    "                            zorder=10)  # zorder alto per essere sopra il dendrogramma\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "# Aggiungi i rettangoli rossi con linewidth=1.2\n",
    "rect_hclust_r_style(h_cl_cs_rs, 6, dend, cut_height, ax, color='red', linewidth=1.2)\n",
    "\n",
    "plt.title(\"Hierarchical Cluster\\ncosine - Ward\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(rotation=90, fontsize=5)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crea dataframe con cluster assignment per k=6\n",
    "dfclu6 = pd.DataFrame({'term': terms, 'clu6': clu6})\n",
    "dfclu6_summary = dfclu6.groupby('clu6').agg(\n",
    "    n_terms=('term', 'count'),\n",
    "    terms=('term', lambda x: '; '.join(x))\n",
    ").reset_index()\n",
    "\n",
    "print(dfclu6_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0694cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster, to_tree\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "def plot_dendrogram_2d_layout(linkage_matrix, labels, k=6, figsize=(16, 12),\n",
    "                              max_labels=80, label_fontsize=8):\n",
    "    \"\"\"\n",
    "    Crea un dendrogramma con layout 2D a coordinate cartesiane\n",
    "    simile all'immagine fornita\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determina i cluster\n",
    "    clusters = fcluster(linkage_matrix, k, criterion='maxclust')\n",
    "    \n",
    "    # Converti linkage in albero\n",
    "    tree = to_tree(linkage_matrix, rd=False)\n",
    "    \n",
    "    # Calcola le posizioni usando il dendrogramma standard\n",
    "    dend = dendrogram(linkage_matrix, no_plot=True)\n",
    "    \n",
    "    # Estrai le posizioni x dalle foglie del dendrogramma\n",
    "    leaves_positions = {}\n",
    "    for i, (icoord, dcoord) in enumerate(zip(dend['icoord'], dend['dcoord'])):\n",
    "        for j in range(len(icoord)):\n",
    "            if dcoord[j] == 0:  # √à una foglia\n",
    "                leaf_idx = int(icoord[j] / 10)\n",
    "                if leaf_idx not in leaves_positions:\n",
    "                    leaves_positions[leaf_idx] = icoord[j]\n",
    "    \n",
    "    # Crea mappatura posizione foglia -> indice originale\n",
    "    leaf_order = dend['leaves']\n",
    "    \n",
    "    # Funzione ricorsiva per calcolare le posizioni dei nodi\n",
    "    def get_node_positions(node, x_positions, y_positions, node_id=None):\n",
    "        if node_id is None:\n",
    "            node_id = id(node)\n",
    "            \n",
    "        if node.is_leaf():\n",
    "            # Trova la posizione x di questa foglia\n",
    "            leaf_idx = node.id\n",
    "            position_idx = leaf_order.index(leaf_idx)\n",
    "            x = position_idx * 10 + 5\n",
    "            y = 0\n",
    "            x_positions[node_id] = x\n",
    "            y_positions[node_id] = y\n",
    "            return x, y\n",
    "        else:\n",
    "            # Calcola ricorsivamente per i figli\n",
    "            left_x, left_y = get_node_positions(node.left, x_positions, y_positions, id(node.left))\n",
    "            right_x, right_y = get_node_positions(node.right, x_positions, y_positions, id(node.right))\n",
    "            \n",
    "            # La posizione x del nodo √® la media dei figli\n",
    "            x = (left_x + right_x) / 2\n",
    "            y = node.dist\n",
    "            \n",
    "            x_positions[node_id] = x\n",
    "            y_positions[node_id] = y\n",
    "            \n",
    "            return x, y\n",
    "    \n",
    "    # Calcola tutte le posizioni\n",
    "    x_positions = {}\n",
    "    y_positions = {}\n",
    "    get_node_positions(tree, x_positions, y_positions)\n",
    "    \n",
    "    # Filtra le etichette da mostrare\n",
    "    if len(labels) > max_labels:\n",
    "        labels_per_cluster = max_labels // k\n",
    "        indices_to_show = set()\n",
    "        \n",
    "        for cluster_id in range(1, k+1):\n",
    "            cluster_indices = np.where(clusters == cluster_id)[0]\n",
    "            selected = cluster_indices[:min(labels_per_cluster, len(cluster_indices))]\n",
    "            indices_to_show.update(selected)\n",
    "    else:\n",
    "        indices_to_show = set(range(len(labels)))\n",
    "    \n",
    "    # Colori per i cluster\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, k))\n",
    "    \n",
    "    # Crea il plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Disegna i collegamenti dell'albero\n",
    "    def draw_tree_edges(node, parent_pos=None):\n",
    "        node_id = id(node)\n",
    "        x = x_positions[node_id]\n",
    "        y = y_positions[node_id]\n",
    "        \n",
    "        if parent_pos is not None:\n",
    "            # Disegna linea dal parent al nodo corrente\n",
    "            ax.plot([parent_pos[0], x], [parent_pos[1], y], \n",
    "                   'k-', linewidth=0.5, alpha=0.3)\n",
    "        \n",
    "        if not node.is_leaf():\n",
    "            draw_tree_edges(node.left, (x, y))\n",
    "            draw_tree_edges(node.right, (x, y))\n",
    "    \n",
    "    draw_tree_edges(tree)\n",
    "    \n",
    "    # Disegna i nodi foglia con etichette\n",
    "    for i, label in enumerate(labels):\n",
    "        if i in leaf_order:\n",
    "            position_idx = leaf_order.index(i)\n",
    "            x = position_idx * 10 + 5\n",
    "            y = 0\n",
    "            \n",
    "            cluster_id = clusters[i]\n",
    "            color = colors[cluster_id - 1]\n",
    "            \n",
    "            # Disegna il punto\n",
    "            ax.plot(x, y, 'o', color=color, markersize=4, alpha=0.8)\n",
    "            \n",
    "            # Aggiungi etichetta se selezionata\n",
    "            if i in indices_to_show:\n",
    "                ax.text(x, y, label, fontsize=label_fontsize, \n",
    "                       color=color, rotation=90, ha='right', va='bottom',\n",
    "                       fontweight='bold')\n",
    "    \n",
    "    # Imposta limiti e aspetto\n",
    "    ax.set_xlabel('Terms', fontsize=12)\n",
    "    ax.set_ylabel('Distance', fontsize=12)\n",
    "    ax.set_title(f'Hierarchical Clustering Dendrogram - {k} Clusters', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Legenda\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=colors[i], label=f'Cluster {i+1}') \n",
    "                      for i in range(k)]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', \n",
    "             fontsize=10, title='Clusters', title_fontsize=12, framealpha=0.95)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_dendrogram_network_style(linkage_matrix, labels, k=6, figsize=(18, 12),\n",
    "                                  max_labels=70, label_fontsize=9, seed=42):\n",
    "    \"\"\"\n",
    "    Crea un dendrogramma con layout a grafo come nell'immagine\n",
    "    I nodi sono posizionati in uno spazio 2D usando un algoritmo di layout\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determina i cluster\n",
    "    clusters = fcluster(linkage_matrix, k, criterion='maxclust')\n",
    "    \n",
    "    # Converti in albero\n",
    "    tree = to_tree(linkage_matrix, rd=False)\n",
    "    \n",
    "    # Crea un grafo NetworkX dall'albero\n",
    "    import networkx as nx\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    node_labels_map = {}\n",
    "    node_clusters = {}\n",
    "    \n",
    "    def add_edges(node, parent=None):\n",
    "        node_id = id(node)\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            leaf_label = labels[node.id]\n",
    "            node_labels_map[node_id] = leaf_label\n",
    "            node_clusters[node_id] = clusters[node.id]\n",
    "            G.add_node(node_id, is_leaf=True, label=leaf_label, \n",
    "                      cluster=clusters[node.id])\n",
    "        else:\n",
    "            node_labels_map[node_id] = \"\"\n",
    "            G.add_node(node_id, is_leaf=False, label=\"\")\n",
    "            add_edges(node.left, node_id)\n",
    "            add_edges(node.right, node_id)\n",
    "        \n",
    "        if parent is not None:\n",
    "            G.add_edge(parent, node_id)\n",
    "    \n",
    "    add_edges(tree)\n",
    "    \n",
    "    # Usa un layout gerarchico personalizzato\n",
    "    # Prova diversi layout per ottenere quello desiderato\n",
    "    pos = nx.spring_layout(G, k=3, iterations=100, seed=seed, scale=30000)\n",
    "    \n",
    "    # Colori per i cluster\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, k))\n",
    "    \n",
    "    # Filtra le etichette\n",
    "    leaf_nodes = [n for n in G.nodes() if G.nodes[n].get('is_leaf', False)]\n",
    "    \n",
    "    if len(leaf_nodes) > max_labels:\n",
    "        labels_per_cluster = max_labels // k\n",
    "        labels_to_show = set()\n",
    "        \n",
    "        for cluster_id in range(1, k+1):\n",
    "            cluster_nodes = [n for n in leaf_nodes \n",
    "                           if node_clusters.get(n) == cluster_id]\n",
    "            selected = cluster_nodes[:labels_per_cluster]\n",
    "            labels_to_show.update(selected)\n",
    "    else:\n",
    "        labels_to_show = set(leaf_nodes)\n",
    "    \n",
    "    # Crea il plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Disegna gli archi\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.15, width=0.5, edge_color='black', ax=ax)\n",
    "    \n",
    "    # Disegna i nodi per cluster\n",
    "    for cluster_id in range(1, k+1):\n",
    "        cluster_nodes = [n for n in leaf_nodes if node_clusters.get(n) == cluster_id]\n",
    "        \n",
    "        # Nodi con etichetta\n",
    "        labeled = [n for n in cluster_nodes if n in labels_to_show]\n",
    "        unlabeled = [n for n in cluster_nodes if n not in labels_to_show]\n",
    "        \n",
    "        if labeled:\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=labeled,\n",
    "                                  node_color=[colors[cluster_id-1]],\n",
    "                                  node_size=50, alpha=0.9, ax=ax)\n",
    "        \n",
    "        if unlabeled:\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=unlabeled,\n",
    "                                  node_color=[colors[cluster_id-1]],\n",
    "                                  node_size=20, alpha=0.5, ax=ax)\n",
    "    \n",
    "    # Disegna nodi interni\n",
    "    internal = [n for n in G.nodes() if not G.nodes[n].get('is_leaf', False)]\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=internal,\n",
    "                          node_color='lightgray', node_size=10, alpha=0.3, ax=ax)\n",
    "    \n",
    "    # Aggiungi etichette\n",
    "    for node in labels_to_show:\n",
    "        x, y = pos[node]\n",
    "        label = node_labels_map[node]\n",
    "        cluster_id = node_clusters[node]\n",
    "        \n",
    "        ax.text(x, y, label, fontsize=label_fontsize,\n",
    "               color=colors[cluster_id-1], ha='center', va='center',\n",
    "               fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.3', facecolor='white',\n",
    "                        edgecolor='none', alpha=0.7))\n",
    "    \n",
    "    ax.set_title(f'Hierarchical Clustering Dendrogram - {k} Clusters',\n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Legenda\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=colors[i], label=f'Cluster {i+1}')\n",
    "                      for i in range(k)]\n",
    "    ax.legend(handles=legend_elements, loc='upper right',\n",
    "             fontsize=10, title='Clusters', title_fontsize=12, framealpha=0.95)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Dendrogramma\n",
    "# ============================================\n",
    "print(\"Creazione dendrogramma con layout a grafo...\")\n",
    "fig, ax = plot_dendrogram_network_style(\n",
    "    h_cl_cs_rs,\n",
    "    terms,\n",
    "    k=6,\n",
    "    figsize=(20, 14),\n",
    "    max_labels=70,\n",
    "    label_fontsize=8,\n",
    "    seed=42\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888fa6ea",
   "metadata": {},
   "source": [
    "### Clustering documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e387ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea il Document Term Matrix (DTM - documenti come righe)\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w{2,}\\b')\n",
    "dtm_matrix = vectorizer.fit_transform(df['text_nostop'])\n",
    "\n",
    "print(f\"Document Term Matrix: {dtm_matrix.shape[0]} documents, {dtm_matrix.shape[1]} terms\")\n",
    "\n",
    "# Rimuovi termini sparsi (equivalente a removeSparseTerms con sparse=0.99)\n",
    "min_docs = int(np.ceil(0.01 * dtm_matrix.shape[0]))\n",
    "term_counts = np.array((dtm_matrix > 0).sum(axis=0)).flatten()\n",
    "mask = term_counts >= min_docs\n",
    "dtm_rs = dtm_matrix[:, mask]\n",
    "terms = np.array(vectorizer.get_feature_names_out())[mask]\n",
    "\n",
    "print(f\"Reduced DTM: {dtm_rs.shape[0]} documents, {dtm_rs.shape[1]} terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f1869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola distanze cosine sui DOCUMENTI\n",
    "dtm_rs_dense = dtm_rs.toarray()\n",
    "dst_cs_rs_d = pdist(dtm_rs_dense, metric='cosine')\n",
    "\n",
    "# Hierarchical Clustering con metodo Ward\n",
    "h_cl_cs_rs_d = linkage(dst_cs_rs_d, method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24613124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram (documenti)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Hierarchical Cluster\\ndocuments\\ncosine - Ward\", fontsize=12)\n",
    "dendrogram(h_cl_cs_rs_d, \n",
    "           leaf_font_size=5,\n",
    "           color_threshold=0,\n",
    "           above_threshold_color='black')\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(fontsize=6)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola Silhouette scores per diversi numeri di cluster\n",
    "lim_clu = range(2, 21)\n",
    "vSilIn = []\n",
    "\n",
    "dst_cs_rs_d_matrix = squareform(dst_cs_rs_d)\n",
    "\n",
    "for k in lim_clu:\n",
    "    clusters = fcluster(h_cl_cs_rs_d, k, criterion='maxclust')\n",
    "    sil_score = silhouette_score(dst_cs_rs_d_matrix, clusters, metric='precomputed')\n",
    "    vSilIn.append(sil_score)\n",
    "\n",
    "# Plot Silhouette scores\n",
    "df_sil = pd.DataFrame({'n_clust': list(lim_clu), 'silhouette': vSilIn})\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_sil['n_clust'], df_sil['silhouette'], \n",
    "         color='blue', linewidth=2, marker='o', markersize=6)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette score')\n",
    "plt.xticks(lim_clu)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba75675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Wedge\n",
    "\n",
    "def comparison_cloud_sectorial(dfclu, max_words=200, words_per_cluster=None,\n",
    "                               figsize=(14, 14), title=\"comparison cloud terms by cluster\"):\n",
    "    \"\"\"\n",
    "    Crea una comparison cloud SETTORIALE come comparison.cloud() in R\n",
    "    Ogni cluster occupa un settore circolare\n",
    "    Le parole NON vengono ripetute tra i cluster (ogni parola appare solo nel cluster dominante)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_clusters = len(dfclu.columns)\n",
    "    colors_map = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    # Se non specificato, distribuisci equamente le parole\n",
    "    if words_per_cluster is None:\n",
    "        words_per_cluster = max_words // n_clusters\n",
    "    \n",
    "    # FASE 1: Assegna ogni parola al cluster dove ha il valore massimo\n",
    "    cluster_words = {}\n",
    "    used_words = set()\n",
    "    \n",
    "    for idx, clust in enumerate(dfclu.columns):\n",
    "        cluster_words[clust] = {}\n",
    "    \n",
    "    # Per ogni parola, trova il cluster dove ha il valore massimo\n",
    "    for word in dfclu.index:\n",
    "        max_cluster = dfclu.loc[word].idxmax()\n",
    "        max_value = dfclu.loc[word, max_cluster]\n",
    "        \n",
    "        if max_value > 0:\n",
    "            cluster_words[max_cluster][word] = max_value\n",
    "    \n",
    "    # FASE 2: Seleziona top N parole per ogni cluster\n",
    "    for clust in dfclu.columns:\n",
    "        sorted_words = sorted(cluster_words[clust].items(), key=lambda x: x[1], reverse=True)\n",
    "        cluster_words[clust] = dict(sorted_words[:words_per_cluster])\n",
    "    \n",
    "    # Crea figura\n",
    "    fig, ax = plt.subplots(figsize=figsize, subplot_kw=dict(aspect=\"equal\"))\n",
    "    \n",
    "    # Dimensioni della word cloud\n",
    "    size = 1200\n",
    "    center_x, center_y = size // 2, size // 2\n",
    "    radius = 550\n",
    "    \n",
    "    # Crea un'immagine vuota\n",
    "    full_image = np.ones((size, size, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Angolo per ogni settore\n",
    "    angle_step = 360 / n_clusters\n",
    "    \n",
    "    for idx, clust in enumerate(dfclu.columns):\n",
    "        # Angolo iniziale e finale per questo cluster\n",
    "        start_angle = idx * angle_step\n",
    "        end_angle = (idx + 1) * angle_step\n",
    "        \n",
    "        # Usa le parole uniche assegnate a questo cluster\n",
    "        frequencies = cluster_words[clust]\n",
    "        \n",
    "        if not frequencies:\n",
    "            continue\n",
    "        \n",
    "        # Crea maschera settoriale (wedge/spicchio)\n",
    "        x, y = np.ogrid[:size, :size]\n",
    "        \n",
    "        # Converti coordinate cartesiane in polari\n",
    "        dx = x - center_x\n",
    "        dy = y - center_y\n",
    "        distance = np.sqrt(dx**2 + dy**2)\n",
    "        angle = np.degrees(np.arctan2(dy, dx)) % 360\n",
    "        \n",
    "        # Crea maschera: True fuori dal settore\n",
    "        mask = np.ones((size, size), dtype=bool)\n",
    "        \n",
    "        # Dentro il cerchio\n",
    "        in_circle = distance <= radius\n",
    "        \n",
    "        # Dentro il settore angolare\n",
    "        if start_angle < end_angle:\n",
    "            in_sector = (angle >= start_angle) & (angle < end_angle)\n",
    "        else:  # Caso che attraversa lo 0\n",
    "            in_sector = (angle >= start_angle) | (angle < end_angle)\n",
    "        \n",
    "        # La maschera finale: False dove vogliamo le parole\n",
    "        mask = ~(in_circle & in_sector)\n",
    "        mask = (mask * 255).astype(np.uint8)\n",
    "        \n",
    "        # Colore per questo cluster\n",
    "        color = colors_map[idx]\n",
    "        \n",
    "        def color_func_single(word, **kwargs):\n",
    "            return tuple(int(c * 255) for c in color[:3])\n",
    "        \n",
    "        # Crea word cloud per questo settore\n",
    "        wc = WordCloud(\n",
    "            width=size,\n",
    "            height=size,\n",
    "            background_color='white',\n",
    "            max_words=words_per_cluster,\n",
    "            relative_scaling=0.4,\n",
    "            min_font_size=6,\n",
    "            max_font_size=60,\n",
    "            prefer_horizontal=0.7,\n",
    "            margin=5,\n",
    "            mask=mask,\n",
    "            color_func=color_func_single,\n",
    "            mode='RGBA'\n",
    "        ).generate_from_frequencies(frequencies)\n",
    "        \n",
    "        # Sovrapponi questa word cloud sull'immagine completa\n",
    "        wc_array = np.array(wc)\n",
    "        # Copia solo le parti non bianche\n",
    "        non_white = (wc_array[:,:,0] < 255) | (wc_array[:,:,1] < 255) | (wc_array[:,:,2] < 255)\n",
    "        full_image[non_white] = wc_array[non_white, :3]\n",
    "    \n",
    "    # Mostra l'immagine finale\n",
    "    ax.imshow(full_image)\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold', pad=15)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Aggiungi legenda\n",
    "    legend_elements = []\n",
    "    for idx, clust in enumerate(dfclu.columns):\n",
    "        color = colors_map[idx]\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                         markerfacecolor=color, markersize=10, \n",
    "                                         label=clust))\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=10, \n",
    "             framealpha=0.9, title='Clusters', title_fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def comparison_cloud_sectorial_balanced(dfclu, max_words=200, \n",
    "                                       figsize=(14, 14), \n",
    "                                       title=\"comparison cloud terms by cluster\"):\n",
    "    \"\"\"\n",
    "    Versione bilanciata: distribuisce equamente le parole tra i cluster\n",
    "    Le parole non vengono ripetute\n",
    "    \"\"\"\n",
    "    n_clusters = len(dfclu.columns)\n",
    "    words_per_cluster = max_words // n_clusters\n",
    "    \n",
    "    return comparison_cloud_sectorial(dfclu, max_words=max_words, \n",
    "                                     words_per_cluster=words_per_cluster,\n",
    "                                     figsize=figsize, title=title)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Crea le comparison clouds SETTORIALI\n",
    "# ============================================\n",
    "\n",
    "# Opzione 1: Comparison cloud settoriale bilanciata\n",
    "print(\"Creazione comparison cloud settoriale bilanciata...\")\n",
    "fig, ax = comparison_cloud_sectorial_balanced(dfclu, max_words=200, figsize=(14, 14))\n",
    "plt.show()\n",
    "\n",
    "# Opzione 2: Comparison cloud settoriale con TF-IDF\n",
    "print(\"Creazione comparison cloud settoriale con TF-IDF...\")\n",
    "fig, ax = comparison_cloud_sectorial_balanced(dfclu_tfidf, max_words=200, \n",
    "                                             figsize=(14, 14),\n",
    "                                             title=\"comparison cloud terms by cluster (TF-IDF)\")\n",
    "plt.show()\n",
    "\n",
    "# Opzione 3: Settoriale con numero personalizzato di parole per cluster\n",
    "print(\"Creazione comparison cloud settoriale (30 parole/cluster)...\")\n",
    "fig, ax = comparison_cloud_sectorial(dfclu, max_words=240, words_per_cluster=30,\n",
    "                                    figsize=(14, 14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c94171",
   "metadata": {},
   "source": [
    "## Tickers Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc569824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "articles_df = pd.read_csv(\"output.csv\", sep=';')\n",
    "\n",
    "tickers_df = articles_df['tickers'].str.get_dummies(sep=',')\n",
    "tickers_df.head()\n",
    "\n",
    "# Estrai i nomi dei ticker dalle colonne del DataFrame\n",
    "ticker_names = list(tickers_df.columns)\n",
    "\n",
    "print(f\"‚úÖ Estratti {len(ticker_names)} ticker unici\")\n",
    "print(f\"üìã Primi 10 ticker: {ticker_names[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2490e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moltiplichiamo la matrice binaria per la sua trasposta\n",
    "co_occurrence_matrix = tickers_df.T.dot(tickers_df)\n",
    "\n",
    "# Rimuovere la diagonale (opzionale)\n",
    "import numpy as np\n",
    "np.fill_diagonal(co_occurrence_matrix.values, 0)\n",
    "\n",
    "co_occurrence_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti i DataFrame in numpy array\n",
    "tickers_df = tickers_df.values \n",
    "co_occurrence_matrix = co_occurrence_matrix.values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f528e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# ============================================\n",
    "# 1Ô∏è‚É£ ANALISI MATRICE ONE-HOT (tickers_adj_m)\n",
    "# ============================================\n",
    "\n",
    "def analyze_onehot(tickers_df, ticker_names):\n",
    "    \"\"\"Analisi della matrice binaria ticker-articoli\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä ANALISI MATRICE ONE-HOT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Top ticker per frequenza\n",
    "    freq = tickers_df.sum(axis=0)\n",
    "    top_idx = np.argsort(freq)[::-1][:15]\n",
    "    \n",
    "    df_top = pd.DataFrame({\n",
    "        'Ticker': [ticker_names[i] for i in top_idx],\n",
    "        'Menzioni': freq[top_idx],\n",
    "        '% Articoli': (freq[top_idx] / len(tickers_df) * 100).round(1)\n",
    "    })\n",
    "    \n",
    "    print(\"\\nüîù TOP 15 TICKER PER FREQUENZA:\")\n",
    "    print(df_top.to_string(index=False))\n",
    "    \n",
    "    # Distribuzione ticker per articolo\n",
    "    tickers_per_article = tickers_df.sum(axis=1)\n",
    "    \n",
    "    print(f\"\\nüìà DISTRIBUZIONE TICKER PER ARTICOLO:\")\n",
    "    print(f\"  Media: {tickers_per_article.mean():.2f}\")\n",
    "    print(f\"  Mediana: {np.median(tickers_per_article):.0f}\")\n",
    "    print(f\"  Max: {tickers_per_article.max():.0f}\")\n",
    "    print(f\"  Articoli con 1 solo ticker: {(tickers_per_article == 1).sum()} ({(tickers_per_article == 1).sum()/len(tickers_df)*100:.1f}%)\")\n",
    "    print(f\"  Articoli con 3+ ticker: {(tickers_per_article >= 3).sum()} ({(tickers_per_article >= 3).sum()/len(tickers_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Sparsit√†\n",
    "    sparsity = 100 * (1 - np.count_nonzero(tickers_df) / tickers_df.size)\n",
    "    print(f\"\\nüí≠ SPARSIT√Ä MATRICE: {sparsity:.1f}%\")\n",
    "    \n",
    "    return df_top, tickers_per_article\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2Ô∏è‚É£ ANALISI CO-OCCURRENCE MATRIX\n",
    "# ============================================\n",
    "\n",
    "def analyze_cooccurrence(co_matrix, ticker_names, threshold=5):\n",
    "    \"\"\"Analisi della matrice di co-occorrenza\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üï∏Ô∏è  ANALISI NETWORK CO-OCCURRENCE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Rimuovi diagonale (auto-occorrenze)\n",
    "    co_clean = co_matrix.copy()\n",
    "    np.fill_diagonal(co_clean, 0)\n",
    "    \n",
    "    # Top co-occorrenze\n",
    "    triu_idx = np.triu_indices_from(co_clean, k=1)\n",
    "    edges = [(ticker_names[i], ticker_names[j], co_clean[i, j]) \n",
    "             for i, j in zip(*triu_idx) if co_clean[i, j] >= threshold]\n",
    "    edges_sorted = sorted(edges, key=lambda x: x[2], reverse=True)[:20]\n",
    "    \n",
    "    print(f\"\\nüîó TOP 20 CO-OCCORRENZE (min {threshold} articoli):\")\n",
    "    for t1, t2, count in edges_sorted[:10]:\n",
    "        print(f\"  {t1:6s} ‚Üî {t2:6s}: {count:3.0f} articoli\")\n",
    "    \n",
    "    # Metriche di rete\n",
    "    G = nx.Graph()\n",
    "    for t1, t2, w in edges:\n",
    "        G.add_edge(t1, t2, weight=w)\n",
    "    \n",
    "    if len(G.nodes()) > 0:\n",
    "        print(f\"\\nüåê METRICHE NETWORK:\")\n",
    "        print(f\"  Nodi (ticker connessi): {G.number_of_nodes()}\")\n",
    "        print(f\"  Archi (relazioni): {G.number_of_edges()}\")\n",
    "        print(f\"  Densit√†: {nx.density(G):.3f}\")\n",
    "        \n",
    "        # Centralit√† (top 10)\n",
    "        if G.number_of_nodes() > 0:\n",
    "            centrality = nx.degree_centrality(G)\n",
    "            top_central = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            print(f\"\\n‚≠ê TOP 10 TICKER PER CENTRALIT√Ä (pi√π connessi):\")\n",
    "            for ticker, cent in top_central:\n",
    "                print(f\"  {ticker:6s}: {cent:.3f}\")\n",
    "    \n",
    "    return G, edges_sorted\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3Ô∏è‚É£ CLUSTERING & COMMUNITIES\n",
    "# ============================================\n",
    "\n",
    "def find_communities(co_matrix, ticker_names, min_cooccur=3):\n",
    "    \"\"\"Trova comunit√† di ticker correlati\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ COMMUNITY DETECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filtra ticker con almeno min_cooccur co-occorrenze\n",
    "    mask = (co_matrix.sum(axis=0) - np.diag(co_matrix)) >= min_cooccur\n",
    "    co_filtered = co_matrix[mask][:, mask]\n",
    "    tickers_filtered = [ticker_names[i] for i in range(len(ticker_names)) if mask[i]]\n",
    "    \n",
    "    if len(tickers_filtered) < 3:\n",
    "        print(\"‚ö†Ô∏è  Troppi pochi ticker per clustering\")\n",
    "        return None\n",
    "    \n",
    "    # Similarity matrix (Cosine)\n",
    "    sim_matrix = cosine_similarity(co_filtered)\n",
    "    np.fill_diagonal(sim_matrix, 0)\n",
    "    \n",
    "    # Hierarchical clustering\n",
    "    condensed_dist = 1 - sim_matrix[np.triu_indices_from(sim_matrix, k=1)]\n",
    "    linkage_matrix = linkage(condensed_dist, method='ward')\n",
    "    \n",
    "    # Identifica cluster (threshold empirico)\n",
    "    from scipy.cluster.hierarchy import fcluster\n",
    "    clusters = fcluster(linkage_matrix, t=0.7, criterion='distance')\n",
    "    \n",
    "    # Raggruppa ticker per cluster\n",
    "    cluster_dict = {}\n",
    "    for ticker, cluster_id in zip(tickers_filtered, clusters):\n",
    "        cluster_dict.setdefault(cluster_id, []).append(ticker)\n",
    "    \n",
    "    print(f\"\\nüîç Trovati {len(cluster_dict)} cluster:\")\n",
    "    for cid, members in sorted(cluster_dict.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "        if len(members) >= 2:\n",
    "            print(f\"\\n  Cluster {cid} ({len(members)} ticker):\")\n",
    "            print(f\"    {', '.join(members)}\")\n",
    "    \n",
    "    return cluster_dict, linkage_matrix, tickers_filtered\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4Ô∏è‚É£ VISUALIZZAZIONI COMPATTE\n",
    "# ============================================\n",
    "\n",
    "def plot_summary(tickers_adj_m, co_matrix, ticker_names, df_top):\n",
    "    \"\"\"4 plot compatti in una figura\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('üìä Ticker Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Top ticker\n",
    "    ax = axes[0, 0]\n",
    "    ax.barh(df_top['Ticker'][:10], df_top['Menzioni'][:10], color='steelblue')\n",
    "    ax.set_xlabel('Numero di Menzioni')\n",
    "    ax.set_title('üîù Top 10 Ticker per Frequenza')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 2. Distribuzione ticker per articolo\n",
    "    ax = axes[0, 1]\n",
    "    tickers_per_article = tickers_adj_m.sum(axis=1)\n",
    "    ax.hist(tickers_per_article, bins=range(0, int(tickers_per_article.max())+2), \n",
    "            color='coral', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Numero di Ticker per Articolo')\n",
    "    ax.set_ylabel('Frequenza')\n",
    "    ax.set_title('üìà Distribuzione Ticker/Articolo')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Heatmap co-occorrenze (top ticker)\n",
    "    ax = axes[1, 0]\n",
    "    top_idx = np.argsort(tickers_adj_m.sum(axis=0))[::-1][:15]\n",
    "    co_sub = co_matrix[top_idx][:, top_idx]\n",
    "    sns.heatmap(co_sub, xticklabels=[ticker_names[i] for i in top_idx],\n",
    "                yticklabels=[ticker_names[i] for i in top_idx],\n",
    "                cmap='YlOrRd', annot=False, fmt='g', ax=ax, cbar_kws={'label': 'Co-occorrenze'})\n",
    "    ax.set_title('üî• Heatmap Co-occurrence (Top 15)')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 4. Network visualization (top edges)\n",
    "    ax = axes[1, 1]\n",
    "    co_clean = co_matrix.copy()\n",
    "    np.fill_diagonal(co_clean, 0)\n",
    "    triu_idx = np.triu_indices_from(co_clean, k=1)\n",
    "    edges = [(ticker_names[i], ticker_names[j], co_clean[i, j]) \n",
    "             for i, j in zip(*triu_idx) if co_clean[i, j] >= 8]\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for t1, t2, w in edges:\n",
    "        G.add_edge(t1, t2, weight=w)\n",
    "    \n",
    "    if len(G.nodes()) > 0:\n",
    "        pos = nx.spring_layout(G, k=0.5, seed=42)\n",
    "        weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "        nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=500, ax=ax)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8, ax=ax)\n",
    "        nx.draw_networkx_edges(G, pos, width=[w/3 for w in weights], alpha=0.5, ax=ax)\n",
    "        ax.set_title('üï∏Ô∏è  Network Co-occurrence (‚â•8 articoli)')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Nessuna co-occorrenza forte', ha='center', va='center')\n",
    "        ax.set_title('üï∏Ô∏è  Network Co-occurrence')\n",
    "    \n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# üöÄ ESECUZIONE COMPLETA\n",
    "# ============================================\n",
    "\n",
    "# Assumendo che hai gi√†:\n",
    "# - tickers_adj_m: matrice (num_articoli √ó num_ticker)\n",
    "# - co_occurrence_matrix: matrice (num_ticker √ó num_ticker)\n",
    "# - ticker_names: lista dei nomi dei ticker\n",
    "\n",
    "# Esempio di utilizzo:\n",
    "\n",
    "# Esegui analisi\n",
    "df_top, tickers_per_art = analyze_onehot(tickers_df, ticker_names)\n",
    "G, edges = analyze_cooccurrence(co_occurrence_matrix, ticker_names, threshold=5)\n",
    "clusters, linkage_m, tickers_filt = find_communities(co_occurrence_matrix, ticker_names, min_cooccur=3)\n",
    "plot_summary(tickers_df, co_occurrence_matrix, ticker_names, df_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bc61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Polygon\n",
    "import networkx as nx\n",
    "from scipy.spatial import ConvexHull\n",
    "from itertools import cycle\n",
    "\n",
    "# ============================================\n",
    "# üï∏Ô∏è NETWORK CON COMMUNITY DETECTION\n",
    "# ============================================\n",
    "\n",
    "def create_network_with_communities(co_matrix, ticker_names, threshold=5):\n",
    "    \"\"\"\n",
    "    Crea network e rileva community usando Louvain algorithm\n",
    "    \"\"\"\n",
    "    # Costruisci grafo\n",
    "    co_clean = co_matrix.copy()\n",
    "    np.fill_diagonal(co_clean, 0)\n",
    "    \n",
    "    triu_idx = np.triu_indices_from(co_clean, k=1)\n",
    "    edges = [(ticker_names[i], ticker_names[j], co_clean[i, j]) \n",
    "             for i, j in zip(*triu_idx) if co_clean[i, j] >= threshold]\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for t1, t2, w in edges:\n",
    "        G.add_edge(t1, t2, weight=w)\n",
    "    \n",
    "    if len(G.nodes()) == 0:\n",
    "        print(\"‚ö†Ô∏è  Nessun nodo nel grafo. Abbassa la threshold.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Community detection (Louvain)\n",
    "    communities = nx.community.louvain_communities(G, seed=42)\n",
    "    \n",
    "    # Crea mapping nodo -> community\n",
    "    node_to_comm = {}\n",
    "    for idx, comm in enumerate(communities):\n",
    "        for node in comm:\n",
    "            node_to_comm[node] = idx\n",
    "    \n",
    "    print(f\"\\nüéØ COMMUNITY DETECTION:\")\n",
    "    print(f\"  Comunit√† trovate: {len(communities)}\")\n",
    "    for idx, comm in enumerate(sorted(communities, key=len, reverse=True)[:10]):\n",
    "        print(f\"  Community {idx+1}: {len(comm)} nodi - {list(comm)[:5]}{'...' if len(comm) > 5 else ''}\")\n",
    "    \n",
    "    return G, communities\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# üé® VISUALIZZAZIONE CON ELLISSI/HULLS (FIXED)\n",
    "# ============================================\n",
    "\n",
    "def plot_network_with_communities(G, communities, highlight_top=30, figsize=(18, 14)):\n",
    "    \"\"\"\n",
    "    Visualizza network con community colorate e convex hulls\n",
    "    ‚ö†Ô∏è FIXED: Gestione corretta dei nodi mancanti nelle posizioni\n",
    "    \"\"\"\n",
    "    if G is None or len(G.nodes()) == 0:\n",
    "        print(\"‚ö†Ô∏è  Grafo vuoto, impossibile visualizzare\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Layout spring (migliore per community)\n",
    "    pos = nx.spring_layout(G, k=0.8, iterations=50, seed=42)\n",
    "    \n",
    "    # Verifica: stampa nodi nel grafo vs nodi con posizioni\n",
    "    print(f\"\\nüîç DEBUG INFO:\")\n",
    "    print(f\"   Nodi nel grafo G: {G.number_of_nodes()}\")\n",
    "    print(f\"   Nodi con posizioni: {len(pos)}\")\n",
    "    \n",
    "    # Colori per community\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(communities)))\n",
    "    node_colors = []\n",
    "    node_to_comm = {}\n",
    "    \n",
    "    for idx, comm in enumerate(communities):\n",
    "        for node in comm:\n",
    "            node_to_comm[node] = idx\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        comm_idx = node_to_comm.get(node, 0)\n",
    "        node_colors.append(colors[comm_idx])\n",
    "    \n",
    "    # Disegna convex hulls per ogni community (solo se >= 3 nodi)\n",
    "    for idx, comm in enumerate(communities):\n",
    "        if len(comm) < 3:\n",
    "            continue\n",
    "        \n",
    "        # ‚úÖ FIX: Verifica che i nodi siano in pos\n",
    "        points = np.array([pos[node] for node in comm if node in pos])\n",
    "        if len(points) < 3:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            hull = ConvexHull(points)\n",
    "            hull_points = points[hull.vertices]\n",
    "            \n",
    "            # Espandi leggermente l'hull per estetica\n",
    "            center = points.mean(axis=0)\n",
    "            expanded = center + 1.2 * (hull_points - center)\n",
    "            \n",
    "            polygon = Polygon(expanded, alpha=0.15, color=colors[idx], \n",
    "                            ec=colors[idx], linewidth=2, linestyle='--')\n",
    "            ax.add_patch(polygon)\n",
    "        except:\n",
    "            continue  # Skip se punti collineari\n",
    "    \n",
    "    # Node sizes basati su degree centrality\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    node_sizes = [500 + 3000 * centrality.get(node, 0) for node in G.nodes()]\n",
    "    \n",
    "    # ‚úÖ FIX: Evidenzia top nodi CHE SONO NEL GRAFO E HANNO POSIZIONI\n",
    "    top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:highlight_top]\n",
    "    top_node_names = [n[0] for n in top_nodes if n[0] in pos]  # ‚Üê FIX PRINCIPALE\n",
    "    \n",
    "    print(f\"   Top nodes selezionati: {len(top_node_names)}\")\n",
    "    \n",
    "    # Disegna edges\n",
    "    weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    max_weight = max(weights) if weights else 1\n",
    "    edge_widths = [1 + 3 * (w / max_weight) for w in weights]\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3, width=edge_widths, \n",
    "                           edge_color='gray', ax=ax)\n",
    "    \n",
    "    # Disegna nodi\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                          node_size=node_sizes, alpha=0.8, \n",
    "                          edgecolors='black', linewidths=1.5, ax=ax)\n",
    "    \n",
    "    # ‚úÖ FIX: Labels solo per nodi che esistono in pos\n",
    "    labels_dict = {node: node for node in top_node_names if node in pos}\n",
    "    if labels_dict:\n",
    "        nx.draw_networkx_labels(G, pos, labels=labels_dict, \n",
    "                               font_size=8, font_weight='bold', ax=ax)\n",
    "    \n",
    "    # Legend per community (top 5)\n",
    "    legend_elements = []\n",
    "    for idx, comm in enumerate(sorted(communities, key=len, reverse=True)[:5]):\n",
    "        sample = list(comm)[:3]\n",
    "        label = f\"Comm {idx+1} ({len(comm)} nodi): {', '.join(sample)}...\"\n",
    "        legend_elements.append(mpatches.Patch(color=colors[idx], label=label))\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=9)\n",
    "    \n",
    "    ax.set_title(f'Network Analysis: {len(communities)} Communities Detection\\n(Hulls = aree di community, dimensione nodo = centralit√†)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pos\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# üìä LAYOUT ALTERNATIVI (FIXED)\n",
    "# ============================================\n",
    "\n",
    "def plot_circular_by_community(G, communities, figsize=(14, 14)):\n",
    "    \"\"\"\n",
    "    Layout circolare: ogni community in un anello separato\n",
    "    ‚ö†Ô∏è FIXED: Gestione corretta dei nodi\n",
    "    \"\"\"\n",
    "    if G is None or len(G.nodes()) == 0:\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Ordina communities per dimensione\n",
    "    sorted_comms = sorted(communities, key=len, reverse=True)\n",
    "    \n",
    "    # Posiziona nodi in cerchi concentrici per community\n",
    "    pos = {}\n",
    "    radius_step = 2\n",
    "    \n",
    "    for comm_idx, comm in enumerate(sorted_comms):\n",
    "        radius = 3 + comm_idx * radius_step\n",
    "        n_nodes = len(comm)\n",
    "        \n",
    "        for node_idx, node in enumerate(comm):\n",
    "            angle = 2 * np.pi * node_idx / n_nodes\n",
    "            pos[node] = (radius * np.cos(angle), radius * np.sin(angle))\n",
    "    \n",
    "    # Colori\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(communities)))\n",
    "    node_colors = []\n",
    "    node_to_comm = {node: idx for idx, comm in enumerate(sorted_comms) for node in comm}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        comm_idx = node_to_comm.get(node, 0)\n",
    "        node_colors.append(colors[comm_idx])\n",
    "    \n",
    "    # Draw\n",
    "    centrality = nx.degree_centrality(G)\n",
    "    node_sizes = [300 + 1500 * centrality.get(node, 0) for node in G.nodes()]\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5, edge_color='gray', ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, \n",
    "                          alpha=0.8, edgecolors='black', linewidths=1, ax=ax)\n",
    "    \n",
    "    # ‚úÖ FIX: Labels selettivi - verifica nodi in pos\n",
    "    top_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "    labels_dict = {n[0]: n[0] for n in top_nodes if n[0] in pos and n[0] in G.nodes()}\n",
    "    \n",
    "    if labels_dict:\n",
    "        nx.draw_networkx_labels(G, pos, labels=labels_dict, font_size=8, ax=ax)\n",
    "    \n",
    "    ax.set_title('Network Layout Circolare (per Community)', fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# üìà COMMUNITY STATISTICS\n",
    "# ============================================\n",
    "\n",
    "def analyze_communities(G, communities):\n",
    "    \"\"\"\n",
    "    Statistiche dettagliate sulle community\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä STATISTICHE COMMUNITY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    comm_stats = []\n",
    "    \n",
    "    for idx, comm in enumerate(communities):\n",
    "        subgraph = G.subgraph(comm)\n",
    "        \n",
    "        # Metriche\n",
    "        density = nx.density(subgraph) if len(comm) > 1 else 0\n",
    "        avg_degree = np.mean([d for n, d in subgraph.degree()]) if len(comm) > 0 else 0\n",
    "        \n",
    "        # Nodi pi√π centrali nella community\n",
    "        if len(comm) > 0:\n",
    "            sub_centrality = nx.degree_centrality(subgraph)\n",
    "            top_node = max(sub_centrality.items(), key=lambda x: x[1])[0] if sub_centrality else \"N/A\"\n",
    "        else:\n",
    "            top_node = \"N/A\"\n",
    "        \n",
    "        comm_stats.append({\n",
    "            'Community': idx + 1,\n",
    "            'Nodi': len(comm),\n",
    "            'Archi': subgraph.number_of_edges(),\n",
    "            'Densit√†': round(density, 3),\n",
    "            'Grado Medio': round(avg_degree, 2),\n",
    "            'Nodo Centrale': top_node,\n",
    "            'Membri': ', '.join(list(comm)[:5]) + ('...' if len(comm) > 5 else '')\n",
    "        })\n",
    "    \n",
    "    df_stats = pd.DataFrame(comm_stats).sort_values('Nodi', ascending=False)\n",
    "    \n",
    "    print(\"\\nüîù TOP COMMUNITIES:\")\n",
    "    print(df_stats.head(10).to_string(index=False))\n",
    "    \n",
    "    # Modularity (qualit√† del clustering)\n",
    "    modularity = nx.community.modularity(G, communities)\n",
    "    print(f\"\\nüìê MODULARITY SCORE: {modularity:.3f}\")\n",
    "    print(\"   (>0.3 = buona separazione delle community)\")\n",
    "    \n",
    "    return df_stats\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# üöÄ ESECUZIONE COMPLETA\n",
    "# ============================================\n",
    "\n",
    "def run_advanced_network_analysis(co_occurrence_matrix, ticker_names, threshold=5):\n",
    "    \"\"\"\n",
    "    Pipeline completa di analisi network avanzata\n",
    "    ‚ö†Ô∏è FIXED: Gestione robusta degli errori\n",
    "    \"\"\"\n",
    "    print(\"üöÄ AVVIO ANALISI NETWORK AVANZATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Crea network e rileva community\n",
    "    G, communities = create_network_with_communities(\n",
    "        co_occurrence_matrix, ticker_names, threshold=threshold\n",
    "    )\n",
    "    \n",
    "    if G is None:\n",
    "        print(\"\\n‚ö†Ô∏è  Analisi interrotta: grafo vuoto\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # 2. Statistiche community\n",
    "    df_stats = analyze_communities(G, communities)\n",
    "    \n",
    "    # 3. Visualizzazione con hulls\n",
    "    print(\"\\nüìä Generazione visualizzazioni...\")\n",
    "    try:\n",
    "        plot_network_with_communities(G, communities, highlight_top=50)  # ‚Üê Pi√π nomi!\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Errore nella visualizzazione con hulls: {e}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ ANALISI COMPLETATA\")\n",
    "    \n",
    "    return G, communities, df_stats\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# üí° ESEMPIO DI UTILIZZO\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assumendo che hai gi√†:\n",
    "    # - co_occurrence_matrix (numpy array)\n",
    "    # - ticker_names (lista di nomi ticker)\n",
    "    \n",
    "    # Esegui l'analisi completa\n",
    "    G, communities, df_stats = run_advanced_network_analysis(\n",
    "        co_occurrence_matrix, \n",
    "        ticker_names, \n",
    "        threshold=5  # Abbassa per pi√π nodi, alza per network pi√π pulito\n",
    "    )\n",
    "    \n",
    "    # Salva risultati\n",
    "    if df_stats is not None:\n",
    "        df_stats.to_csv('community_statistics.csv', index=False)\n",
    "        print(\"\\nüíæ Statistiche salvate in: community_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518377b",
   "metadata": {},
   "source": [
    "# Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c80099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community as nx_comm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import community as community_louvain\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df8b9e",
   "metadata": {},
   "source": [
    "## Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d79b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "\n",
    "def scrape_tweets_for_date(since_date, until_date, target_count=2500):\n",
    "    \"\"\"\n",
    "    Scrape tweets per una specifica data range\n",
    "    \n",
    "    Args:\n",
    "        since_date: data inizio (formato YYYY-MM-DD)\n",
    "        until_date: data fine (formato YYYY-MM-DD)\n",
    "        target_count: numero target di tweet da raccogliere\n",
    "    \n",
    "    Returns:\n",
    "        Lista di dizionari contenenti i dati dei tweet\n",
    "    \"\"\"\n",
    "    # Configurazione opzioni Chrome\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument('--headless')  \n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    # Inizializza il driver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # URL della pagina con le date parametrizzate\n",
    "    url = f\"https://nitter.net/search?f=tweets&q=bitcoin+lang%3Aen&e-nativeretweets=on&since={since_date}&until={until_date}&near=\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Inizio scraping per periodo: {since_date} ‚Üí {until_date}\")\n",
    "    print(f\"Target: {target_count} tweet\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Apri la pagina\n",
    "    driver.get(url)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    tweets_data = []\n",
    "    page_count = 0\n",
    "\n",
    "    while len(tweets_data) < target_count:\n",
    "        print(f\"\\n--- Pagina {page_count + 1} (Raccolti: {len(tweets_data)}/{target_count}) ---\")\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        all_items = driver.find_elements(By.CLASS_NAME, \"timeline-item\")\n",
    "        tweets = [item for item in all_items if \"show-more\" not in item.get_attribute(\"class\")]\n",
    "        print(f\"Trovati {len(tweets)} elementi timeline-item\")\n",
    "        \n",
    "        for tweet_elem in tweets:\n",
    "            # Controlla se abbiamo gi√† raggiunto il target\n",
    "            if len(tweets_data) >= target_count:\n",
    "                print(f\"\\n‚úì Target raggiunto: {len(tweets_data)} tweet\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Extract username\n",
    "                username_elem = tweet_elem.find_element(By.CLASS_NAME, \"username\")\n",
    "                username = username_elem.text.strip().replace('@', '')\n",
    "                \n",
    "                # Extract tweet text\n",
    "                tweet_content = tweet_elem.find_element(By.CLASS_NAME, \"tweet-content\")\n",
    "                tweet_text = tweet_content.text.strip()\n",
    "                \n",
    "                # Extract reply-to username (se presente)\n",
    "                reply = None\n",
    "                try:\n",
    "                    reply_elem = tweet_elem.find_element(By.CLASS_NAME, \"replying-to\")\n",
    "                    reply = reply_elem.text.strip().replace('Replying to @', '').replace('@', '')\n",
    "                except NoSuchElementException:\n",
    "                    pass  \n",
    "\n",
    "                # Skip retweets\n",
    "                if tweet_text.startswith('RT @'):\n",
    "                    continue\n",
    "                \n",
    "                # Extract engagement metrics\n",
    "                stats_container = tweet_elem.find_element(By.CLASS_NAME, \"tweet-stats\")\n",
    "                icon_containers = stats_container.find_elements(By.CLASS_NAME, \"icon-container\")\n",
    "\n",
    "                comments = retweets = likes = quotes = 0\n",
    "\n",
    "                for container in icon_containers:\n",
    "                    container_text = container.text.strip()\n",
    "                    number_parts = [part for part in container_text.split() if part.replace(',', '').isdigit()]\n",
    "                    num = int(number_parts[0].replace(',', '')) if number_parts else 0\n",
    "        \n",
    "                    if container.find_elements(By.CLASS_NAME, \"icon-comment\"):\n",
    "                        comments = num\n",
    "                    elif container.find_elements(By.CLASS_NAME, \"icon-retweet\"):\n",
    "                        retweets = num\n",
    "                    elif container.find_elements(By.CLASS_NAME, \"icon-quote\"):\n",
    "                        quotes = num\n",
    "                    elif container.find_elements(By.CLASS_NAME, \"icon-heart\"):\n",
    "                        likes = num\n",
    "                \n",
    "                # Extract mentions\n",
    "                mentions = re.findall(r'@(\\w+)', tweet_text)\n",
    "                mentions = [m for m in mentions if m != username]\n",
    "\n",
    "                # Extract tags\n",
    "                hashtags = re.findall(r'#(\\w+)', tweet_text)\n",
    "                \n",
    "                # Trova parole precedute da $ ma non seguite da un numero\n",
    "                cashtags = re.findall(r'\\$([A-Za-z][A-Za-z0-9_]*)', tweet_text)\n",
    "\n",
    "                # Unisci hashtag e cashtag, evitando duplicati\n",
    "                hashtags = list(set(hashtags + cashtags))\n",
    "                \n",
    "                # Aggiungi reply alle mentions (se presente e non gi√† incluso)\n",
    "                if reply and reply not in mentions:\n",
    "                    mentions.insert(0, reply)\n",
    "                \n",
    "                # Extract date\n",
    "                date_elem = tweet_elem.find_element(By.CLASS_NAME, \"tweet-date\")\n",
    "                date_link = date_elem.find_element(By.TAG_NAME, \"a\")  \n",
    "                date_str = date_link.get_attribute('title')  \n",
    "                tweet_date = datetime.strptime(date_str, '%b %d, %Y ¬∑ %I:%M %p %Z')\n",
    "                \n",
    "                # Avoid duplicates\n",
    "                tweet_id = f\"{username}_{tweet_text[:10]}\"\n",
    "                if not any(t['text_id'] == tweet_id for t in tweets_data):\n",
    "                    tweets_data.append({\n",
    "                        'thread_title': None,\n",
    "                        'thread_author': None,\n",
    "                        'thread_score': None,\n",
    "                        'thread_num_comments': None,\n",
    "                        'text_id': tweet_id,\n",
    "                        'comment_parent_id': None,\n",
    "                        'text_author': username,\n",
    "                        'text': tweet_text,\n",
    "                        'likes': likes,\n",
    "                        'text_date': tweet_date,\n",
    "                        'text_num_replies': comments,\n",
    "                        'retweets': retweets,\n",
    "                        'comment_parent_author': None,\n",
    "                        'text_mentions': ' '.join(mentions) if mentions else '',\n",
    "                        'text_hashtags': ' '.join(hashtags) if hashtags else '',\n",
    "                        'argument': 'Bitcoin',\n",
    "                        'site':'Nitter'\n",
    "                    })\n",
    "                    print(f\"‚úì @{username}: {tweet_date} (Totale: {len(tweets_data)})\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Errore nell'estrazione del tweet: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Controlla se abbiamo raggiunto il target dopo il loop dei tweet\n",
    "        if len(tweets_data) >= target_count:\n",
    "            print(f\"\\n‚úì Target raggiunto: {len(tweets_data)} tweet\")\n",
    "            break\n",
    "        \n",
    "        # Bottone \"Load more\" \n",
    "        try:\n",
    "            # Trova il div.show-more \n",
    "            load_more_div = driver.find_element(By.CSS_SELECTOR, \"div.show-more:not(.timeline-item)\")\n",
    "            load_more_link = load_more_div.find_element(By.TAG_NAME, \"a\")\n",
    "            \n",
    "            link_text = load_more_link.text.strip()\n",
    "            if \"Load more\" in link_text:\n",
    "                print(f\"\\n‚Üí Cliccando '{link_text}'...\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_link)\n",
    "                time.sleep(1)\n",
    "                load_more_link.click()\n",
    "                page_count += 1\n",
    "            else:\n",
    "                print(f\"\\n‚úì Fine scraping - trovato '{link_text}' invece di 'Load more'\")\n",
    "                break\n",
    "                \n",
    "        except NoSuchElementException:\n",
    "            print(f\"\\n‚ö† Fine scraping - nessun bottone 'Load more' trovato\")\n",
    "            print(f\"‚ö† Raccolti solo {len(tweets_data)}/{target_count} tweet per questo periodo\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö† Fine scraping - errore: {e}\")\n",
    "            print(f\"‚ö† Raccolti solo {len(tweets_data)}/{target_count} tweet per questo periodo\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì SCRAPING COMPLETATO per {since_date} ‚Üí {until_date}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Totale tweet raccolti: {len(tweets_data)}\")\n",
    "    print(f\"Pagine caricate: {page_count + 1}\")\n",
    "    \n",
    "    # Chiudi il driver\n",
    "    driver.quit()\n",
    "    \n",
    "    return tweets_data\n",
    "\n",
    "\n",
    "# MAIN SCRIPT\n",
    "if __name__ == \"__main__\":\n",
    "    # Definisci i periodi da scrapare\n",
    "    date_ranges = [\n",
    "        (\"2025-10-17\", \"2025-10-18\"),\n",
    "        (\"2025-10-18\", \"2025-10-19\")\n",
    "    ]\n",
    "    \n",
    "    # Lista per raccogliere tutti i tweet\n",
    "    all_tweets = []\n",
    "    \n",
    "    # Loop attraverso tutti i periodi\n",
    "    for since, until in date_ranges:\n",
    "        tweets = scrape_tweets_for_date(since, until, target_count=2500)\n",
    "        all_tweets.extend(tweets)\n",
    "        print(f\"\\n‚Üí Tweet totali raccolti finora: {len(all_tweets)}\")\n",
    "        \n",
    "        # Pausa tra una sessione e l'altra\n",
    "        if (since, until) != date_ranges[-1]:  # Non aspettare dopo l'ultima iterazione\n",
    "            print(\"\\n‚è≥ Pausa di 10 secondi prima del prossimo periodo...\")\n",
    "            time.sleep(10)\n",
    "    \n",
    "    # Risultato finale\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"‚úì‚úì‚úì SCRAPING TOTALE COMPLETATO ‚úì‚úì‚úì\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    print(f\"Tweet raccolti per periodo:\")\n",
    "    \n",
    "    for since, until in date_ranges:\n",
    "        count = sum(1 for t in all_tweets if since in str(t['text_date']))\n",
    "        print(f\"  ‚Ä¢ {since} ‚Üí {until}: ~{count} tweet\")\n",
    "    \n",
    "    print(f\"\\nTotale complessivo: {len(all_tweets)} tweet\")\n",
    "    \n",
    "    # Qui puoi salvare i dati come preferisci\n",
    "    # es. in un CSV o database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1290fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_Bitcoin = pd.DataFrame(all_tweets)\n",
    "tweets_Bitcoin.to_excel('tweets_bitcoin.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15870ce",
   "metadata": {},
   "source": [
    "## Nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f36d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "\n",
    "def scrape_tweets_for_date(since_date, until_date, target_count=2500):\n",
    "    \"\"\"\n",
    "    Scrape tweets per una specifica data range\n",
    "    \n",
    "    Args:\n",
    "        since_date: data inizio (formato YYYY-MM-DD)\n",
    "        until_date: data fine (formato YYYY-MM-DD)\n",
    "        target_count: numero target di tweet da raccogliere\n",
    "    \n",
    "    Returns:\n",
    "        Lista di dizionari contenenti i dati dei tweet\n",
    "    \"\"\"\n",
    "    # Configurazione opzioni Chrome\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument('--headless')  \n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    # Inizializza il driver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # URL della pagina con le date parametrizzate\n",
    "    url = f\"https://nitter.net/search?f=tweets&q=nvidia+lang%3Aen&e-nativeretweets=on&since={since_date}&until={until_date}&near=\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Inizio scraping per periodo: {since_date} ‚Üí {until_date}\")\n",
    "    print(f\"Target: {target_count} tweet\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Apri la pagina\n",
    "    driver.get(url)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    tweets_data = []\n",
    "    page_count = 0\n",
    "\n",
    "    while len(tweets_data) < target_count:\n",
    "        print(f\"\\n--- Pagina {page_count + 1} (Raccolti: {len(tweets_data)}/{target_count}) ---\")\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        all_items = driver.find_elements(By.CLASS_NAME, \"timeline-item\")\n",
    "        tweets = [item for item in all_items if \"show-more\" not in item.get_attribute(\"class\")]\n",
    "        print(f\"Trovati {len(tweets)} elementi timeline-item\")\n",
    "        \n",
    "        for tweet_elem in tweets:\n",
    "            # Controlla se abbiamo gi√† raggiunto il target\n",
    "            if len(tweets_data) >= target_count:\n",
    "                print(f\"\\n‚úì Target raggiunto: {len(tweets_data)} tweet\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Extract username\n",
    "                username_elem = tweet_elem.find_element(By.CLASS_NAME, \"username\")\n",
    "                username = username_elem.text.strip().replace('@', '')\n",
    "                \n",
    "                # Extract tweet text\n",
    "                tweet_content = tweet_elem.find_element(By.CLASS_NAME, \"tweet-content\")\n",
    "                tweet_text = tweet_content.text.strip()\n",
    "                \n",
    "                # Extract reply-to username (se presente)\n",
    "                reply = None\n",
    "                try:\n",
    "                    reply_elem = tweet_elem.find_element(By.CLASS_NAME, \"replying-to\")\n",
    "                    reply = reply_elem.text.strip().replace('Replying to @', '').replace('@', '')\n",
    "                except NoSuchElementException:\n",
    "                    pass  \n",
    "\n",
    "                # Skip retweets\n",
    "                if tweet_text.startswith('RT @'):\n",
    "                    continue\n",
    "                \n",
    "                # Extract engagement metrics\n",
    "                stats_container = tweet_elem.find_element(By.CLASS_NAME, \"tweet-stats\")\n",
    "                icon_containers = stats_container.find_elements(By.CLASS_NAME, \"icon-container\")\n",
    "\n",
    "                comments = retweets = likes = quotes = 0\n",
    "\n",
    "                for container in icon_containers:\n",
    "                    container_text = container.text.strip()\n",
    "                    number_parts = [part for part in container_text.split() if part.replace(',', '').isdigit()]\n",
    "                    num = int(number_parts[0].replace(',', '')) if number_parts else 0\n",
    "        \n",
    "                    if container.find_elements(By.CLASS_NAME, \"icon-comment\"):\n",
    "                        comments = num\n",
    "                    elif container.find_elements(By.CLASS_NAME, \"icon-retweet\"):\n",
    "                        retweets = num\n",
    "                    elif container.find_elements(By.CLASS_NAME, \"icon-quote\"):\n",
    "                        quotes = num\n",
    "                    elif container.find_elements(By.CLASS_NAME, \"icon-heart\"):\n",
    "                        likes = num\n",
    "                \n",
    "                # Extract mentions\n",
    "                mentions = re.findall(r'@(\\w+)', tweet_text)\n",
    "                mentions = [m for m in mentions if m != username]\n",
    "\n",
    "                # Extract tags\n",
    "                hashtags = re.findall(r'#(\\w+)', tweet_text)\n",
    "                \n",
    "                # Trova parole precedute da $ ma non seguite da un numero\n",
    "                cashtags = re.findall(r'\\$([A-Za-z][A-Za-z0-9_]*)', tweet_text)\n",
    "\n",
    "                # Unisci hashtag e cashtag, evitando duplicati\n",
    "                hashtags = list(set(hashtags + cashtags))\n",
    "                \n",
    "                # Aggiungi reply alle mentions (se presente e non gi√† incluso)\n",
    "                if reply and reply not in mentions:\n",
    "                    mentions.insert(0, reply)\n",
    "                \n",
    "                # Extract date\n",
    "                date_elem = tweet_elem.find_element(By.CLASS_NAME, \"tweet-date\")\n",
    "                date_link = date_elem.find_element(By.TAG_NAME, \"a\")  \n",
    "                date_str = date_link.get_attribute('title')  \n",
    "                tweet_date = datetime.strptime(date_str, '%b %d, %Y ¬∑ %I:%M %p %Z')\n",
    "                \n",
    "                # Avoid duplicates\n",
    "                tweet_id = f\"{username}_{tweet_text[:10]}\"\n",
    "                if not any(t['text_id'] == tweet_id for t in tweets_data):\n",
    "                    tweets_data.append({\n",
    "                        'thread_title': None,\n",
    "                        'thread_author': None,\n",
    "                        'thread_score': None,\n",
    "                        'thread_num_comments': None,\n",
    "                        'text_id': tweet_id,\n",
    "                        'comment_parent_id': None,\n",
    "                        'text_author': username,\n",
    "                        'text': tweet_text,\n",
    "                        'likes': likes,\n",
    "                        'text_date': tweet_date,\n",
    "                        'text_num_replies': comments,\n",
    "                        'retweets': retweets,\n",
    "                        'comment_parent_author': None,\n",
    "                        'text_mentions': ' '.join(mentions) if mentions else '',\n",
    "                        'text_hashtags': ' '.join(hashtags) if hashtags else '',\n",
    "                        'argument': 'Nvidia',\n",
    "                        'site':'Nitter'\n",
    "                    })\n",
    "                    print(f\"‚úì @{username}: {tweet_date} (Totale: {len(tweets_data)})\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Errore nell'estrazione del tweet: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Controlla se abbiamo raggiunto il target dopo il loop dei tweet\n",
    "        if len(tweets_data) >= target_count:\n",
    "            print(f\"\\n‚úì Target raggiunto: {len(tweets_data)} tweet\")\n",
    "            break\n",
    "        \n",
    "        # Bottone \"Load more\" \n",
    "        try:\n",
    "            # Trova il div.show-more \n",
    "            load_more_div = driver.find_element(By.CSS_SELECTOR, \"div.show-more:not(.timeline-item)\")\n",
    "            load_more_link = load_more_div.find_element(By.TAG_NAME, \"a\")\n",
    "            \n",
    "            link_text = load_more_link.text.strip()\n",
    "            if \"Load more\" in link_text:\n",
    "                print(f\"\\n‚Üí Cliccando '{link_text}'...\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_link)\n",
    "                time.sleep(1)\n",
    "                load_more_link.click()\n",
    "                page_count += 1\n",
    "            else:\n",
    "                print(f\"\\n‚úì Fine scraping - trovato '{link_text}' invece di 'Load more'\")\n",
    "                break\n",
    "                \n",
    "        except NoSuchElementException:\n",
    "            print(f\"\\n‚ö† Fine scraping - nessun bottone 'Load more' trovato\")\n",
    "            print(f\"‚ö† Raccolti solo {len(tweets_data)}/{target_count} tweet per questo periodo\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö† Fine scraping - errore: {e}\")\n",
    "            print(f\"‚ö† Raccolti solo {len(tweets_data)}/{target_count} tweet per questo periodo\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úì SCRAPING COMPLETATO per {since_date} ‚Üí {until_date}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Totale tweet raccolti: {len(tweets_data)}\")\n",
    "    print(f\"Pagine caricate: {page_count + 1}\")\n",
    "    \n",
    "    # Chiudi il driver\n",
    "    driver.quit()\n",
    "    \n",
    "    return tweets_data\n",
    "\n",
    "\n",
    "# MAIN SCRIPT\n",
    "if __name__ == \"__main__\":\n",
    "    # Definisci i periodi da scrapare\n",
    "    date_ranges = [\n",
    "        (\"2025-10-17\", \"2025-10-18\"),\n",
    "        (\"2025-10-18\", \"2025-10-19\")\n",
    "    ]\n",
    "    \n",
    "    # Lista per raccogliere tutti i tweet\n",
    "    all_tweets = []\n",
    "    \n",
    "    # Loop attraverso tutti i periodi\n",
    "    for since, until in date_ranges:\n",
    "        tweets = scrape_tweets_for_date(since, until, target_count=2500)\n",
    "        all_tweets.extend(tweets)\n",
    "        print(f\"\\n‚Üí Tweet totali raccolti finora: {len(all_tweets)}\")\n",
    "        \n",
    "        # Pausa tra una sessione e l'altra\n",
    "        if (since, until) != date_ranges[-1]:  # Non aspettare dopo l'ultima iterazione\n",
    "            print(\"\\n‚è≥ Pausa di 10 secondi prima del prossimo periodo...\")\n",
    "            time.sleep(10)\n",
    "    \n",
    "    # Risultato finale\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"‚úì‚úì‚úì SCRAPING TOTALE COMPLETATO ‚úì‚úì‚úì\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    print(f\"Tweet raccolti per periodo:\")\n",
    "    \n",
    "    for since, until in date_ranges:\n",
    "        count = sum(1 for t in all_tweets if since in str(t['text_date']))\n",
    "        print(f\"  ‚Ä¢ {since} ‚Üí {until}: ~{count} tweet\")\n",
    "    \n",
    "    print(f\"\\nTotale complessivo: {len(all_tweets)} tweet\")\n",
    "    \n",
    "    # Qui puoi salvare i dati come preferisci\n",
    "    # es. in un CSV o database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_Nvidia = pd.DataFrame(all_tweets)\n",
    "tweets_Nvidia.to_excel('tweets_nvidia.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364909ad",
   "metadata": {},
   "source": [
    "## Tweets df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_Bitcoin = pd.read_excel('tweets_bitcoin.xlsx')\n",
    "tweets_Nvidia = pd.read_excel('tweets_nvidia.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.concat([tweets_Bitcoin, tweets_Nvidia], ignore_index=True)\n",
    "tweets_df.to_excel('tweets_df.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d568e",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20036e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set color palette moderno (ORIGINALE)\n",
    "COLORS = {\n",
    "    'primary': '#FF6B6B',    # Rosso/corallo\n",
    "    'secondary': '#4ECDC4',  # Turchese\n",
    "    'accent': '#95E1D3',     # Verde acqua\n",
    "    'dark': '#34495e',       # Grigio scuro\n",
    "    'light': '#ECF0F1'       # Grigio chiaro\n",
    "}\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "# ==========================================\n",
    "# 1. NETWORK CONSTRUCTION\n",
    "# ==========================================\n",
    "\n",
    "def build_directed_weighted_network(df, argument_name):\n",
    "    \"\"\"Network principale: DIRECTED & WEIGHTED\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        user = row['text_author']\n",
    "        for mentioned in row['mentions_clean']:\n",
    "            if user != mentioned:\n",
    "                if G.has_edge(user, mentioned):\n",
    "                    G[user][mentioned]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(user, mentioned, weight=1)\n",
    "    \n",
    "    print(f\"\\nüîµ DIRECTED WEIGHTED NETWORK - {argument_name}\")\n",
    "    print(f\"   Nodi: {G.number_of_nodes()}, Archi: {G.number_of_edges()}\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "def build_reciprocal_network(G_directed, argument_name):\n",
    "    \"\"\"Network reciproco: UNDIRECTED & WEIGHTED\"\"\"\n",
    "    G_reciprocal = nx.Graph()\n",
    "    \n",
    "    for u, v, data in G_directed.edges(data=True):\n",
    "        if G_directed.has_edge(v, u):\n",
    "            weight = data['weight'] + G_directed[v][u]['weight']\n",
    "            if not G_reciprocal.has_edge(u, v):\n",
    "                G_reciprocal.add_edge(u, v, weight=weight)\n",
    "    \n",
    "    print(f\"\\nüü¢ RECIPROCAL UNDIRECTED NETWORK - {argument_name}\")\n",
    "    print(f\"   Nodi: {G_reciprocal.number_of_nodes()}, Archi: {G_reciprocal.number_of_edges()}\")\n",
    "    \n",
    "    return G_reciprocal\n",
    "\n",
    "def build_backbone_network(G_directed, argument_name, min_weight=3):\n",
    "    \"\"\"Network backbone: DIRECTED & UNWEIGHTED\"\"\"\n",
    "    G_backbone = nx.DiGraph()\n",
    "    \n",
    "    for u, v, data in G_directed.edges(data=True):\n",
    "        if data['weight'] >= min_weight:\n",
    "            G_backbone.add_edge(u, v)\n",
    "    \n",
    "    print(f\"\\nüî¥ BACKBONE NETWORK - {argument_name} (min weight={min_weight})\")\n",
    "    print(f\"   Nodi: {G_backbone.number_of_nodes()}, Archi: {G_backbone.number_of_edges()}\")\n",
    "    \n",
    "    return G_backbone\n",
    "\n",
    "# ==========================================\n",
    "# 2. NETWORK METRICS & STATISTICS\n",
    "# ==========================================\n",
    "\n",
    "def analyze_directed_network(G, argument_name):\n",
    "    \"\"\"Analisi del network diretto\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìà DIRECTED NETWORK ANALYSIS - {argument_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    in_degree_weighted = dict(G.in_degree(weight='weight'))\n",
    "    top_mentioned = sorted(in_degree_weighted.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nüéØ TOP 10 MOST MENTIONED:\")\n",
    "    for i, (user, mentions) in enumerate(top_mentioned, 1):\n",
    "        print(f\"   {i}. @{user}: {mentions} menzioni\")\n",
    "    \n",
    "    out_degree = dict(G.out_degree())\n",
    "    top_active = sorted(out_degree.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nüí¨ TOP 10 MOST ACTIVE:\")\n",
    "    for i, (user, mentions) in enumerate(top_active, 1):\n",
    "        print(f\"   {i}. @{user}: menziona {mentions} utenti\")\n",
    "    \n",
    "    pagerank = nx.pagerank(G, weight='weight')\n",
    "    top_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\n‚≠ê TOP 10 PAGERANK:\")\n",
    "    for i, (user, score) in enumerate(top_pagerank, 1):\n",
    "        print(f\"   {i}. @{user}: {score:.5f}\")\n",
    "    \n",
    "    try:\n",
    "        hits = nx.hits(G, max_iter=100)\n",
    "        authorities = sorted(hits[0].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        hubs = sorted(hits[1].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        \n",
    "        print(\"\\nüèÜ TOP 5 AUTHORITIES:\")\n",
    "        for i, (user, score) in enumerate(authorities, 1):\n",
    "            print(f\"   {i}. @{user}: {score:.5f}\")\n",
    "        \n",
    "        print(\"\\nüîó TOP 5 HUBS:\")\n",
    "        for i, (user, score) in enumerate(hubs, 1):\n",
    "            print(f\"   {i}. @{user}: {score:.5f}\")\n",
    "    except:\n",
    "        print(\"\\n‚ö†Ô∏è HITS algorithm non convergente\")\n",
    "    \n",
    "    return {\n",
    "        'in_degree_weighted': in_degree_weighted,\n",
    "        'pagerank': pagerank,\n",
    "        'out_degree': out_degree\n",
    "    }\n",
    "\n",
    "def analyze_reciprocal_network(G, argument_name):\n",
    "    \"\"\"Analisi del network reciproco\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìà RECIPROCAL NETWORK ANALYSIS - {argument_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if G.number_of_edges() == 0:\n",
    "        print(\"‚ö†Ô∏è Nessuna interazione reciproca trovata!\")\n",
    "        return {}\n",
    "    \n",
    "    betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    top_betweenness = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nüåâ TOP 10 BRIDGES:\")\n",
    "    for i, (user, score) in enumerate(top_betweenness, 1):\n",
    "        print(f\"   {i}. @{user}: {score:.5f}\")\n",
    "    \n",
    "    clustering = nx.clustering(G, weight='weight')\n",
    "    avg_clustering = sum(clustering.values()) / len(clustering)\n",
    "    \n",
    "    print(f\"\\nüîó CLUSTERING COEFFICIENT: {avg_clustering:.4f}\")\n",
    "    \n",
    "    try:\n",
    "        import community as community_louvain\n",
    "        communities = community_louvain.best_partition(G, weight='weight')\n",
    "        n_communities = len(set(communities.values()))\n",
    "        \n",
    "        print(f\"\\nüë• COMMUNITY DETECTION: {n_communities} community\")\n",
    "        \n",
    "        comm_sizes = Counter(communities.values())\n",
    "        for comm_id, size in comm_sizes.most_common(5):\n",
    "            members = [u for u, c in communities.items() if c == comm_id][:5]\n",
    "            print(f\"   Community {comm_id}: {size} membri (es: {', '.join(members)})\")\n",
    "        \n",
    "        return {\n",
    "            'betweenness': betweenness,\n",
    "            'clustering': clustering,\n",
    "            'communities': communities\n",
    "        }\n",
    "    except ImportError:\n",
    "        return {\n",
    "            'betweenness': betweenness,\n",
    "            'clustering': clustering\n",
    "        }\n",
    "\n",
    "def analyze_backbone_network(G, argument_name):\n",
    "    \"\"\"Analisi del network backbone\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìà BACKBONE NETWORK ANALYSIS - {argument_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if G.number_of_edges() == 0:\n",
    "        print(\"‚ö†Ô∏è Backbone vuoto - riduci min_weight\")\n",
    "        return {}\n",
    "    \n",
    "    scc = list(nx.strongly_connected_components(G))\n",
    "    print(f\"\\nüîÑ STRONGLY CONNECTED COMPONENTS: {len(scc)}\")\n",
    "    largest_scc = max(scc, key=len)\n",
    "    print(f\"   Componente pi√π grande: {len(largest_scc)} nodi\")\n",
    "    \n",
    "    G_undirected = G.to_undirected()\n",
    "    G_undirected.remove_edges_from(nx.selfloop_edges(G_undirected))\n",
    "    \n",
    "    core_numbers = nx.core_number(G_undirected)\n",
    "    max_core = max(core_numbers.values())\n",
    "    \n",
    "    print(f\"\\nüíé K-CORE DECOMPOSITION:\")\n",
    "    print(f\"   Max core number: {max_core}\")\n",
    "    k_core = [u for u, k in core_numbers.items() if k == max_core]\n",
    "    print(f\"   {max_core}-core: {len(k_core)} nodi\")\n",
    "    if len(k_core) <= 10:\n",
    "        print(f\"   Membri: {', '.join(k_core)}\")\n",
    "    \n",
    "    try:\n",
    "        import community as community_louvain\n",
    "        G_und_simple = nx.Graph()\n",
    "        for u, v in G.edges():\n",
    "            if u != v:\n",
    "                G_und_simple.add_edge(u, v)\n",
    "        \n",
    "        communities = community_louvain.best_partition(G_und_simple)\n",
    "        n_communities = len(set(communities.values()))\n",
    "        \n",
    "        print(f\"\\nüö∂ COMMUNITY DETECTION: {n_communities} community\")\n",
    "        \n",
    "        return {\n",
    "            'scc': scc,\n",
    "            'core_numbers': core_numbers,\n",
    "            'communities': communities\n",
    "        }\n",
    "    except ImportError:\n",
    "        return {\n",
    "            'scc': scc,\n",
    "            'core_numbers': core_numbers\n",
    "        }\n",
    "\n",
    "# ==========================================\n",
    "# 3. VISUALIZATIONS\n",
    "# ==========================================\n",
    "\n",
    "def visualize_directed_network(G, metrics, argument_name, top_n=50):\n",
    "    \"\"\"Visualizza il network diretto\"\"\"\n",
    "    top_nodes = sorted(metrics['pagerank'].items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_nodes = [n[0] for n in top_nodes]\n",
    "    G_sub = G.subgraph(top_nodes).copy()\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    pos = nx.spring_layout(G_sub, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    node_sizes = [metrics['pagerank'].get(n, 0) * 50000 for n in G_sub.nodes()]\n",
    "    edges = G_sub.edges()\n",
    "    weights = [G_sub[u][v]['weight'] for u, v in edges]\n",
    "    \n",
    "    # ‚úÖ Frecce meno arrotondate\n",
    "    nx.draw_networkx_edges(G_sub, pos, edge_color=COLORS['light'], \n",
    "                          width=[w*0.5 for w in weights], alpha=0.6,\n",
    "                          arrows=True, arrowsize=10, \n",
    "                          arrowstyle='-|>', connectionstyle='arc3,rad=0.1')  # ‚Üê Frecce pi√π dritte\n",
    "    \n",
    "    nx.draw_networkx_nodes(G_sub, pos, node_size=node_sizes, \n",
    "                          node_color=COLORS['secondary'], alpha=0.8, \n",
    "                          edgecolors=COLORS['dark'], linewidths=2)\n",
    "    \n",
    "    nx.draw_networkx_labels(G_sub, pos, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title(f\"{argument_name} - DIRECTED NETWORK\\nTop {top_n} utenti per PageRank (Node's dimension = influence)\", \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{argument_name.lower()}_directed_network.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(f\"\\nüíæ Salvato: {argument_name.lower()}_directed_network.png\")\n",
    "\n",
    "def visualize_reciprocal_network(G, metrics, argument_name, top_n=40):\n",
    "    \"\"\"Visualizza il network reciproco\"\"\"\n",
    "    if G.number_of_edges() == 0:\n",
    "        print(\"‚ö†Ô∏è Nessuna interazione reciproca da visualizzare\")\n",
    "        return\n",
    "    \n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    G_sub = G.subgraph(largest_cc).copy()\n",
    "    \n",
    "    if 'betweenness' in metrics:\n",
    "        top_nodes = sorted(metrics['betweenness'].items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        top_nodes = [n[0] for n in top_nodes if n[0] in G_sub.nodes()]\n",
    "        G_sub = G_sub.subgraph(top_nodes).copy()\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    pos = nx.spring_layout(G_sub, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    if 'communities' in metrics:\n",
    "        communities = metrics['communities']\n",
    "        node_colors = [communities.get(n, 0) for n in G_sub.nodes()]\n",
    "        cmap = plt.cm.Set3\n",
    "    else:\n",
    "        node_colors = COLORS['accent']\n",
    "        cmap = None\n",
    "    \n",
    "    weights = [G_sub[u][v]['weight'] for u, v in G_sub.edges()]\n",
    "    \n",
    "    nx.draw_networkx_edges(G_sub, pos, width=[w*0.3 for w in weights], \n",
    "                          alpha=0.5, edge_color=COLORS['light'])\n",
    "    \n",
    "    nx.draw_networkx_nodes(G_sub, pos, node_size=300, \n",
    "                          node_color=node_colors, cmap=cmap, \n",
    "                          alpha=0.9, edgecolors=COLORS['dark'], linewidths=1.5)\n",
    "    \n",
    "    nx.draw_networkx_labels(G_sub, pos, font_size=7, font_weight='bold')\n",
    "    \n",
    "    plt.title(f'{argument_name} - RECIPROCAL NETWORK\\nBidirectional conversations (Colors = Community)', \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{argument_name.lower()}_reciprocal_network.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(f\"\\nüíæ Salvato: {argument_name.lower()}_reciprocal_network.png\")\n",
    "\n",
    "def visualize_statistics(G_dir, metrics_dir, argument_name):\n",
    "    \"\"\"Visualizzazioni statistiche compatte\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # 1. Degree distribution\n",
    "    in_degrees = [d for n, d in G_dir.in_degree()]\n",
    "    out_degrees = [d for n, d in G_dir.out_degree()]\n",
    "    \n",
    "    axes[0, 0].hist(in_degrees, bins=30, alpha=0.7, label='In-degree', \n",
    "                   color=COLORS['primary'], edgecolor=COLORS['dark'])\n",
    "    axes[0, 0].hist(out_degrees, bins=30, alpha=0.7, label='Out-degree', \n",
    "                   color=COLORS['secondary'], edgecolor=COLORS['dark'])\n",
    "    axes[0, 0].set_xlabel('Degree', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Frequenza', fontweight='bold')\n",
    "    axes[0, 0].set_title('Distribuzione Degree', fontweight='bold', fontsize=12)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Weight distribution\n",
    "    weights = [d['weight'] for u, v, d in G_dir.edges(data=True)]\n",
    "    axes[0, 1].hist(weights, bins=30, color=COLORS['accent'], alpha=0.8, \n",
    "                   edgecolor=COLORS['dark'])\n",
    "    axes[0, 1].set_xlabel('Peso (# menzioni)', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Frequenza', fontweight='bold')\n",
    "    axes[0, 1].set_title('Distribuzione Pesi Archi', fontweight='bold', fontsize=12)\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Top users bar chart\n",
    "    top_pr = sorted(metrics_dir['pagerank'].items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "    users = [u for u, _ in top_pr]\n",
    "    scores = [s for _, s in top_pr]\n",
    "    \n",
    "    colors_gradient = [COLORS['primary'] if i < 5 else COLORS['secondary'] if i < 10 \n",
    "                      else COLORS['accent'] for i in range(len(users))]\n",
    "    \n",
    "    axes[1, 0].barh(users, scores, color=colors_gradient, alpha=0.8, edgecolor=COLORS['dark'])\n",
    "    axes[1, 0].set_xlabel('PageRank Score', fontweight='bold')\n",
    "    axes[1, 0].set_title('Top 15 Utenti per PageRank', fontweight='bold', fontsize=12)\n",
    "    axes[1, 0].invert_yaxis()\n",
    "    axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 4. Adjacency heatmap\n",
    "    top_20 = [u for u, _ in top_pr[:20]]\n",
    "    G_sub = G_dir.subgraph(top_20)\n",
    "    adj_matrix = nx.to_numpy_array(G_sub, nodelist=top_20, weight='weight')\n",
    "    \n",
    "    sns.heatmap(adj_matrix, xticklabels=top_20, yticklabels=top_20, \n",
    "                cmap='RdYlGn', ax=axes[1, 1], cbar_kws={'label': 'Menzioni'},\n",
    "                linewidths=0.5, linecolor=COLORS['light'])\n",
    "    axes[1, 1].set_title('Heatmap Menzioni (Top 20 utenti)', fontweight='bold', fontsize=12)\n",
    "    axes[1, 1].set_xlabel('Menzionato', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Autore', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'{argument_name} - Network Statistics', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{argument_name.lower()}_statistics.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(f\"\\nüíæ Salvato: {argument_name.lower()}_statistics.png\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION PER SINGOLO ARGUMENT\n",
    "# ==========================================\n",
    "\n",
    "def analyze_single_argument(df, argument_name, min_weight_backbone=3, top_n_viz=50):\n",
    "    \"\"\"\n",
    "    Pipeline completa di analisi per UN SINGOLO argument\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame gi√† filtrato per argument\n",
    "    argument_name : 'Bitcoin' o 'Nvidia'\n",
    "    min_weight_backbone : soglia per backbone network\n",
    "    top_n_viz : numero nodi da visualizzare\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üöÄ ANALISI NETWORK: {argument_name.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Dataset: {len(df)} tweets\")\n",
    "    \n",
    "    # Build networks\n",
    "    G_directed = build_directed_weighted_network(df, argument_name)\n",
    "    G_reciprocal = build_reciprocal_network(G_directed, argument_name)\n",
    "    G_backbone = build_backbone_network(G_directed, argument_name, min_weight=min_weight_backbone)\n",
    "    \n",
    "    # Analyze\n",
    "    metrics_dir = analyze_directed_network(G_directed, argument_name)\n",
    "    metrics_recip = analyze_reciprocal_network(G_reciprocal, argument_name)\n",
    "    metrics_backbone = analyze_backbone_network(G_backbone, argument_name)\n",
    "    \n",
    "    # Visualize\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üé® GENERATING VISUALIZATIONS - {argument_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    visualize_directed_network(G_directed, metrics_dir, argument_name, top_n=top_n_viz)\n",
    "    visualize_reciprocal_network(G_reciprocal, metrics_recip, argument_name, top_n=top_n_viz)\n",
    "    visualize_statistics(G_directed, metrics_dir, argument_name)\n",
    "    \n",
    "    print(f\"\\n‚úÖ ANALISI COMPLETATA: {argument_name}\")\n",
    "    \n",
    "    return {\n",
    "        'networks': {\n",
    "            'directed': G_directed,\n",
    "            'reciprocal': G_reciprocal,\n",
    "            'backbone': G_backbone\n",
    "        },\n",
    "        'metrics': {\n",
    "            'directed': metrics_dir,\n",
    "            'reciprocal': metrics_recip,\n",
    "            'backbone': metrics_backbone\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# ESECUZIONE COMPLETA\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\\n\" + \"üåü\"*35)\n",
    "    print(\"  TWITTER NETWORK ANALYSIS BY ARGUMENT\")\n",
    "    print(\"üåü\"*35)\n",
    "    \n",
    "    # Carica DataFrame\n",
    "    df = pd.read_excel(\"tweets_df.xlsx\")\n",
    "    \n",
    "    # Prepara mentions\n",
    "    df['mentions_clean'] = df['text_mentions'].apply(lambda x: str(x).split() if pd.notna(x) else [])\n",
    "    df = df[df['mentions_clean'].apply(len) > 0]\n",
    "    \n",
    "    print(f\"\\nüìä Dataset totale: {len(df)} tweets con mentions\")\n",
    "    \n",
    "    # Verifica colonna argument\n",
    "    if 'argument' not in df.columns:\n",
    "        print(\"\\n‚ö†Ô∏è  Colonna 'argument' non trovata nel DataFrame!\")\n",
    "        print(\"    Assicurati che il file contenga la colonna 'argument' con valori 'Bitcoin' e 'Nvidia'\")\n",
    "        exit()\n",
    "    \n",
    "    # Mostra distribution\n",
    "    arg_counts = df['argument'].value_counts()\n",
    "    print(f\"\\nüìà Distribuzione argument:\")\n",
    "    for arg, count in arg_counts.items():\n",
    "        print(f\"   {arg}: {count} tweets\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # ANALISI BITCOIN\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\\n\" + \"üü†\"*35)\n",
    "    print(\"  BITCOIN ANALYSIS\")\n",
    "    print(\"üü†\"*35)\n",
    "    \n",
    "    df_bitcoin = df[df['argument'] == 'Bitcoin'].copy()\n",
    "    \n",
    "    if len(df_bitcoin) == 0:\n",
    "        print(\"‚ö†Ô∏è  Nessun tweet trovato per Bitcoin!\")\n",
    "    else:\n",
    "        results_bitcoin = analyze_single_argument(\n",
    "            df=df_bitcoin,\n",
    "            argument_name='Bitcoin',\n",
    "            min_weight_backbone=2,\n",
    "            top_n_viz=5000\n",
    "        )\n",
    "    \n",
    "    # ==========================================\n",
    "    # ANALISI NVIDIA\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\\n\" + \"üü¢\"*35)\n",
    "    print(\"  NVIDIA ANALYSIS\")\n",
    "    print(\"üü¢\"*35)\n",
    "    \n",
    "    df_nvidia = df[df['argument'] == 'Nvidia'].copy()\n",
    "    \n",
    "    if len(df_nvidia) == 0:\n",
    "        print(\"‚ö†Ô∏è  Nessun tweet trovato per Nvidia!\")\n",
    "    else:\n",
    "        results_nvidia = analyze_single_argument(\n",
    "            df=df_nvidia,\n",
    "            argument_name='Nvidia',\n",
    "            min_weight_backbone=2,\n",
    "            top_n_viz=5000\n",
    "        )\n",
    "    \n",
    "    # ==========================================\n",
    "    # SUMMARY FINALE\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"üéâ ANALISI COMPLETA TERMINATA!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìÅ FILE GENERATI:\")\n",
    "    print(f\"\\n   üü† Bitcoin:\")\n",
    "    print(f\"      - bitcoin_directed_network.png\")\n",
    "    print(f\"      - bitcoin_reciprocal_network.png\")\n",
    "    print(f\"      - bitcoin_statistics.png\")\n",
    "    \n",
    "    print(f\"\\n   üü¢ Nvidia:\")\n",
    "    print(f\"      - nvidia_directed_network.png\")\n",
    "    print(f\"      - nvidia_reciprocal_network.png\")\n",
    "    print(f\"      - nvidia_statistics.png\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ú® Tutte le analisi completate con successo! ‚ú®\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Set color palette moderno (ORIGINALE)\n",
    "COLORS = {\n",
    "    'primary': '#FF6B6B',    # Rosso/corallo\n",
    "    'secondary': '#4ECDC4',  # Turchese\n",
    "    'accent': '#95E1D3',     # Verde acqua\n",
    "    'dark': '#34495e',       # Grigio scuro\n",
    "    'light': '#ECF0F1'       # Grigio chiaro\n",
    "}\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "# ==========================================\n",
    "# ANALISI COMPARATIVA PER ARGUMENT\n",
    "# ==========================================\n",
    "\n",
    "def assign_user_argument(df):\n",
    "    \"\"\"\n",
    "    Assegna a ogni utente l'argument prevalente nei suoi tweet\n",
    "    \"\"\"\n",
    "    user_arguments = df.groupby('text_author')['argument'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0])\n",
    "    return user_arguments.to_dict()\n",
    "\n",
    "def build_cross_argument_network(df):\n",
    "    \"\"\"\n",
    "    Network con informazione sull'argument di ogni utente\n",
    "    Ritorna: G (network), user_arg_map (dict user->argument)\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    user_arg_map = assign_user_argument(df)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        user = row['text_author']\n",
    "        argument = row['argument']\n",
    "        \n",
    "        for mentioned in row['mentions_clean']:\n",
    "            if user != mentioned:\n",
    "                # Determina argument del menzionato (se presente nel dataset)\n",
    "                mentioned_arg = user_arg_map.get(mentioned, 'Unknown')\n",
    "                \n",
    "                if G.has_edge(user, mentioned):\n",
    "                    G[user][mentioned]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(user, mentioned, weight=1, \n",
    "                             source_arg=argument, \n",
    "                             target_arg=mentioned_arg)\n",
    "    \n",
    "    # Aggiungi attributi ai nodi\n",
    "    nx.set_node_attributes(G, user_arg_map, 'argument')\n",
    "    \n",
    "    print(f\"\\nüé® CROSS-ARGUMENT NETWORK\")\n",
    "    print(f\"   Nodi: {G.number_of_nodes()}, Archi: {G.number_of_edges()}\")\n",
    "    \n",
    "    # Statistiche intra vs inter-argument\n",
    "    intra_arg = sum(1 for u, v, d in G.edges(data=True) \n",
    "                    if d.get('source_arg') == d.get('target_arg'))\n",
    "    inter_arg = G.number_of_edges() - intra_arg\n",
    "    \n",
    "    print(f\"   Menzioni intra-argument: {intra_arg} ({intra_arg/G.number_of_edges()*100:.1f}%)\")\n",
    "    print(f\"   Menzioni inter-argument: {inter_arg} ({inter_arg/G.number_of_edges()*100:.1f}%)\")\n",
    "    \n",
    "    return G, user_arg_map\n",
    "\n",
    "def analyze_argument_communities(G, user_arg_map):\n",
    "    \"\"\"\n",
    "    Analizza le caratteristiche di ogni argument community\n",
    "    \"\"\"\n",
    "    arguments = set(user_arg_map.values()) - {'Unknown'}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä ARGUMENT COMMUNITIES ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    for arg in arguments:\n",
    "        users_in_arg = [u for u, a in user_arg_map.items() if a == arg and u in G.nodes()]\n",
    "        G_sub = G.subgraph(users_in_arg).copy()\n",
    "        \n",
    "        # Metriche base\n",
    "        n_nodes = G_sub.number_of_nodes()\n",
    "        n_edges = G_sub.number_of_edges()\n",
    "        density = nx.density(G_sub) if n_nodes > 1 else 0\n",
    "        \n",
    "        # Top influencers (PageRank locale)\n",
    "        if n_edges > 0:\n",
    "            pagerank = nx.pagerank(G_sub, weight='weight')\n",
    "            top_users = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        else:\n",
    "            top_users = []\n",
    "        \n",
    "        stats[arg] = {\n",
    "            'n_users': n_nodes,\n",
    "            'n_interactions': n_edges,\n",
    "            'density': density,\n",
    "            'top_users': top_users\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüè∑Ô∏è  ARGUMENT: {arg}\")\n",
    "        print(f\"   Utenti: {n_nodes}\")\n",
    "        print(f\"   Interazioni: {n_edges}\")\n",
    "        print(f\"   Densit√†: {density:.4f}\")\n",
    "        if top_users:\n",
    "            print(f\"   Top influencers:\")\n",
    "            for i, (user, score) in enumerate(top_users, 1):\n",
    "                print(f\"      {i}. @{user} (PR: {score:.5f})\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def find_bridge_users(G, user_arg_map):\n",
    "    \"\"\"\n",
    "    Identifica utenti che fanno da ponte tra argument\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üåâ BRIDGE USERS (Inter-Argument Connectors)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    bridge_scores = {}\n",
    "    \n",
    "    for user in G.nodes():\n",
    "        if user not in user_arg_map or user_arg_map[user] == 'Unknown':\n",
    "            continue\n",
    "        \n",
    "        user_arg = user_arg_map[user]\n",
    "        \n",
    "        # Conta menzioni verso altri argument\n",
    "        out_neighbors = list(G.successors(user))\n",
    "        cross_mentions = sum(1 for n in out_neighbors \n",
    "                           if user_arg_map.get(n, 'Unknown') != user_arg \n",
    "                           and user_arg_map.get(n, 'Unknown') != 'Unknown')\n",
    "        \n",
    "        # Conta menzioni ricevute da altri argument\n",
    "        in_neighbors = list(G.predecessors(user))\n",
    "        cross_received = sum(1 for n in in_neighbors \n",
    "                           if user_arg_map.get(n, 'Unknown') != user_arg \n",
    "                           and user_arg_map.get(n, 'Unknown') != 'Unknown')\n",
    "        \n",
    "        total_cross = cross_mentions + cross_received\n",
    "        \n",
    "        if total_cross > 0:\n",
    "            bridge_scores[user] = {\n",
    "                'total': total_cross,\n",
    "                'out': cross_mentions,\n",
    "                'in': cross_received,\n",
    "                'argument': user_arg\n",
    "            }\n",
    "    \n",
    "    # Top bridges\n",
    "    top_bridges = sorted(bridge_scores.items(), \n",
    "                        key=lambda x: x[1]['total'], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nüîù TOP 10 BRIDGE USERS:\")\n",
    "    for i, (user, data) in enumerate(top_bridges, 1):\n",
    "        print(f\"   {i}. @{user} ({data['argument']}): \"\n",
    "              f\"{data['total']} interazioni cross-argument \"\n",
    "              f\"({data['out']} out, {data['in']} in)\")\n",
    "    \n",
    "    return bridge_scores\n",
    "\n",
    "# ==========================================\n",
    "# VISUALIZZAZIONI COMPARATIVE\n",
    "# ==========================================\n",
    "\n",
    "def visualize_argument_network(G, user_arg_map, top_n=60):\n",
    "    \"\"\"\n",
    "    Network colorato per argument con evidenza dei collegamenti cross-argument\n",
    "    \"\"\"\n",
    "    # Prendi top utenti per PageRank\n",
    "    pagerank = nx.pagerank(G, weight='weight')\n",
    "    top_users = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_users = [u for u, _ in top_users]\n",
    "    \n",
    "    G_sub = G.subgraph(top_users).copy()\n",
    "    \n",
    "    # Setup colori: usa COLORS originali per i due argument\n",
    "    arguments = list(set(user_arg_map.values()) - {'Unknown'})\n",
    "    \n",
    "    # Usa i colori originali per Bitcoin e Nvidia\n",
    "    if 'Bitcoin' in arguments and 'Nvidia' in arguments:\n",
    "        color_map = {\n",
    "            'Bitcoin': COLORS['primary'],      # Rosso/corallo\n",
    "            'Nvidia': COLORS['secondary'],     # Turchese\n",
    "            'Unknown': COLORS['light']         # Grigio chiaro\n",
    "        }\n",
    "    else:\n",
    "        # Fallback se nomi diversi\n",
    "        color_map = {arg: [COLORS['primary'], COLORS['secondary']][i] \n",
    "                    for i, arg in enumerate(arguments)}\n",
    "        color_map['Unknown'] = COLORS['light']\n",
    "    \n",
    "    node_colors = [color_map.get(user_arg_map.get(n, 'Unknown'), COLORS['light']) \n",
    "                   for n in G_sub.nodes()]\n",
    "    \n",
    "    # Separa edge intra vs inter-argument\n",
    "    intra_edges = []\n",
    "    inter_edges = []\n",
    "    \n",
    "    for u, v, d in G_sub.edges(data=True):\n",
    "        if user_arg_map.get(u, 'Unknown') == user_arg_map.get(v, 'Unknown'):\n",
    "            intra_edges.append((u, v, d['weight']))\n",
    "        else:\n",
    "            inter_edges.append((u, v, d['weight']))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(18, 14))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    pos = nx.spring_layout(G_sub, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    # Draw intra-argument edges (grigio chiaro)\n",
    "    if intra_edges:\n",
    "        nx.draw_networkx_edges(G_sub, pos, \n",
    "                              edgelist=[(u, v) for u, v, w in intra_edges],\n",
    "                              width=[w*0.3 for u, v, w in intra_edges],\n",
    "                              edge_color=COLORS['light'], alpha=0.4,\n",
    "                              arrows=True, arrowsize=8,\n",
    "                              arrowstyle='-|>', connectionstyle='arc3,rad=0.1')  # ‚Üê Frecce dritte\n",
    "    \n",
    "    # Draw inter-argument edges (rosso/evidenziato)\n",
    "    if inter_edges:\n",
    "        nx.draw_networkx_edges(G_sub, pos,\n",
    "                              edgelist=[(u, v) for u, v, w in inter_edges],\n",
    "                              width=[w*0.5 for u, v, w in inter_edges],\n",
    "                              edge_color=COLORS['primary'], alpha=0.6,\n",
    "                              arrows=True, arrowsize=10, style='dashed',\n",
    "                              arrowstyle='-|>', connectionstyle='arc3,rad=0.1')  # ‚Üê Frecce dritte\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_sizes = [pagerank.get(n, 0) * 50000 for n in G_sub.nodes()]\n",
    "    nx.draw_networkx_nodes(G_sub, pos, node_size=node_sizes,\n",
    "                          node_color=node_colors, alpha=0.9,\n",
    "                          edgecolors=COLORS['dark'], linewidths=1.5)\n",
    "    \n",
    "    nx.draw_networkx_labels(G_sub, pos, font_size=7, font_weight='bold')\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = []\n",
    "    for arg in arguments:\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                         markerfacecolor=color_map[arg], markersize=12,\n",
    "                                         label=arg, markeredgecolor=COLORS['dark']))\n",
    "    legend_elements.append(plt.Line2D([0], [0], color=COLORS['light'], linewidth=2, \n",
    "                                     label='Intra-argument'))\n",
    "    legend_elements.append(plt.Line2D([0], [0], color=COLORS['primary'], linewidth=2, \n",
    "                                     linestyle='--', label='Inter-argument'))\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "    \n",
    "    plt.title(f'NETWORK COMPARISON BY ARGUMENT\\n'\n",
    "              f'Top {top_n} users - Node size = PageRank\\n'\n",
    "              f'Solid edges = same argument, Dashed = cross-argument',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('network_by_argument.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(\"\\nüíæ Salvato: network_by_argument.png\")\n",
    "\n",
    "def visualize_argument_comparison(G, user_arg_map, stats):\n",
    "    \"\"\"\n",
    "    Dashboard comparativa con 4 subplot\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    arguments = list(set(user_arg_map.values()) - {'Unknown'})\n",
    "    \n",
    "    # Usa i colori originali\n",
    "    if 'Bitcoin' in arguments and 'Nvidia' in arguments:\n",
    "        color_dict = {\n",
    "            'Bitcoin': COLORS['primary'],\n",
    "            'Nvidia': COLORS['secondary']\n",
    "        }\n",
    "    else:\n",
    "        color_dict = {arg: [COLORS['primary'], COLORS['secondary']][i] \n",
    "                     for i, arg in enumerate(arguments)}\n",
    "    \n",
    "    # 1. Confronto metriche base\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    metrics = ['n_users', 'n_interactions']\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    for i, arg in enumerate(arguments):\n",
    "        values = [stats[arg]['n_users'], stats[arg]['n_interactions']]\n",
    "        ax1.bar(x + i*width, values, width, label=arg, \n",
    "               color=color_dict[arg], alpha=0.8, edgecolor=COLORS['dark'])\n",
    "    \n",
    "    ax1.set_ylabel('Count', fontweight='bold')\n",
    "    ax1.set_title('Comparison: Users & Interactions', fontweight='bold', fontsize=12)\n",
    "    ax1.set_xticks(x + width/2)\n",
    "    ax1.set_xticklabels(['Users', 'Interactions'])\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Network density comparison\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    densities = [stats[arg]['density'] for arg in arguments]\n",
    "    bars = ax2.bar(arguments, densities, \n",
    "                   color=[color_dict[arg] for arg in arguments], \n",
    "                   alpha=0.8, edgecolor=COLORS['dark'])\n",
    "    ax2.set_ylabel('Network Density', fontweight='bold')\n",
    "    ax2.set_title('Network Density by Argument', fontweight='bold', fontsize=12)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Interaction matrix (inter vs intra-argument)\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    interaction_matrix = np.zeros((len(arguments), len(arguments)))\n",
    "    \n",
    "    for u, v, d in G.edges(data=True):\n",
    "        u_arg = user_arg_map.get(u, 'Unknown')\n",
    "        v_arg = user_arg_map.get(v, 'Unknown')\n",
    "        \n",
    "        if u_arg in arguments and v_arg in arguments:\n",
    "            i = arguments.index(u_arg)\n",
    "            j = arguments.index(v_arg)\n",
    "            interaction_matrix[i, j] += d['weight']\n",
    "    \n",
    "    sns.heatmap(interaction_matrix, annot=True, fmt='.0f', \n",
    "                xticklabels=arguments, yticklabels=arguments,\n",
    "                cmap='RdYlGn', ax=ax3, cbar_kws={'label': 'Total Mentions'},\n",
    "                linewidths=0.5, linecolor=COLORS['light'])\n",
    "    ax3.set_title('Cross-Argument Interaction Matrix', fontweight='bold', fontsize=12)\n",
    "    ax3.set_xlabel('Mentioned User Argument', fontweight='bold')\n",
    "    ax3.set_ylabel('Mentioning User Argument', fontweight='bold')\n",
    "    \n",
    "    # 4. PageRank distribution per argument\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    pagerank = nx.pagerank(G, weight='weight')\n",
    "    \n",
    "    pr_by_arg = {arg: [] for arg in arguments}\n",
    "    for user, pr_score in pagerank.items():\n",
    "        user_arg = user_arg_map.get(user, 'Unknown')\n",
    "        if user_arg in arguments:\n",
    "            pr_by_arg[user_arg].append(pr_score)\n",
    "    \n",
    "    for arg in arguments:\n",
    "        if pr_by_arg[arg]:\n",
    "            ax4.hist(pr_by_arg[arg], bins=30, alpha=0.6, \n",
    "                    label=arg, color=color_dict[arg], edgecolor=COLORS['dark'])\n",
    "    \n",
    "    ax4.set_xlabel('PageRank Score', fontweight='bold')\n",
    "    ax4.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax4.set_title('PageRank Distribution by Argument', fontweight='bold', fontsize=12)\n",
    "    ax4.legend()\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Argument Comparison Dashboard', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.savefig('argument_comparison_dashboard.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(\"\\nüíæ Salvato: argument_comparison_dashboard.png\")\n",
    "\n",
    "def visualize_side_by_side_networks(G, user_arg_map, top_n=40):\n",
    "    \"\"\"\n",
    "    Due subnetwork affiancati (uno per argument)\n",
    "    \"\"\"\n",
    "    arguments = list(set(user_arg_map.values()) - {'Unknown'})\n",
    "    \n",
    "    if len(arguments) != 2:\n",
    "        print(f\"‚ö†Ô∏è  Questa visualizzazione funziona meglio con 2 argument (trovati: {len(arguments)})\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Usa i colori originali\n",
    "    if 'Bitcoin' in arguments and 'Nvidia' in arguments:\n",
    "        color_map = {\n",
    "            'Bitcoin': COLORS['primary'],\n",
    "            'Nvidia': COLORS['secondary']\n",
    "        }\n",
    "    else:\n",
    "        color_map = {arg: [COLORS['primary'], COLORS['secondary']][i] \n",
    "                    for i, arg in enumerate(arguments)}\n",
    "    \n",
    "    for idx, arg in enumerate(arguments[:2]):\n",
    "        # Subgraph per argument\n",
    "        users_in_arg = [u for u, a in user_arg_map.items() if a == arg and u in G.nodes()]\n",
    "        G_sub = G.subgraph(users_in_arg).copy()\n",
    "        \n",
    "        # Top utenti\n",
    "        if G_sub.number_of_edges() > 0:\n",
    "            pagerank = nx.pagerank(G_sub, weight='weight')\n",
    "            top_users = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "            top_users = [u for u, _ in top_users if u in G_sub.nodes()]\n",
    "            G_viz = G_sub.subgraph(top_users).copy()\n",
    "        else:\n",
    "            G_viz = G_sub\n",
    "        \n",
    "        if G_viz.number_of_nodes() == 0:\n",
    "            axes[idx].text(0.5, 0.5, f'No data for {arg}', \n",
    "                          ha='center', va='center', fontsize=14)\n",
    "            axes[idx].axis('off')\n",
    "            continue\n",
    "        \n",
    "        # Layout e visualizzazione\n",
    "        pos = nx.spring_layout(G_viz, k=2, iterations=50, seed=42)\n",
    "        \n",
    "        # Edges\n",
    "        weights = [G_viz[u][v]['weight'] for u, v in G_viz.edges()]\n",
    "        nx.draw_networkx_edges(G_viz, pos, width=[w*0.4 for w in weights],\n",
    "                              edge_color=COLORS['light'], alpha=0.5,\n",
    "                              arrows=True, arrowsize=8, ax=axes[idx],\n",
    "                              arrowstyle='-|>', connectionstyle='arc3,rad=0.1')  # ‚Üê Frecce dritte\n",
    "        \n",
    "        # Nodes\n",
    "        node_sizes = [pagerank.get(n, 0.001) * 30000 for n in G_viz.nodes()] if G_viz.number_of_edges() > 0 else [300]*G_viz.number_of_nodes()\n",
    "        \n",
    "        nx.draw_networkx_nodes(G_viz, pos, node_size=node_sizes,\n",
    "                              node_color=color_map[arg],\n",
    "                              alpha=0.8, edgecolors=COLORS['dark'], linewidths=1.5,\n",
    "                              ax=axes[idx])\n",
    "        \n",
    "        nx.draw_networkx_labels(G_viz, pos, font_size=7, \n",
    "                               font_weight='bold', ax=axes[idx])\n",
    "        \n",
    "        axes[idx].set_title(f'{arg}\\n{G_viz.number_of_nodes()} users, '\n",
    "                          f'{G_viz.number_of_edges()} interactions',\n",
    "                          fontsize=14, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('SIDE-BY-SIDE NETWORK COMPARISON', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('side_by_side_networks.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(\"\\nüíæ Salvato: side_by_side_networks.png\")\n",
    "\n",
    "# ==========================================\n",
    "# MAIN FUNCTION PER ANALISI ARGUMENT\n",
    "# ==========================================\n",
    "\n",
    "def analyze_by_argument(df, top_n_viz=60):\n",
    "    \"\"\"\n",
    "    Pipeline completa per analisi comparativa argument\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame con 'text_author', 'mentions_clean', 'argument'\n",
    "    top_n_viz : numero nodi da visualizzare\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé® ARGUMENT-BASED NETWORK ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Build cross-argument network\n",
    "    G, user_arg_map = build_cross_argument_network(df)\n",
    "    \n",
    "    # Analyze communities\n",
    "    stats = analyze_argument_communities(G, user_arg_map)\n",
    "    \n",
    "    # Find bridges\n",
    "    bridges = find_bridge_users(G, user_arg_map)\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä GENERATING COMPARATIVE VISUALIZATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    visualize_argument_network(G, user_arg_map, top_n=top_n_viz)\n",
    "    visualize_argument_comparison(G, user_arg_map, stats)\n",
    "    visualize_side_by_side_networks(G, user_arg_map, top_n=40)\n",
    "    \n",
    "    print(\"\\n‚úÖ ANALISI ARGUMENT COMPLETATA!\")\n",
    "    \n",
    "    return {\n",
    "        'network': G,\n",
    "        'user_arg_map': user_arg_map,\n",
    "        'stats': stats,\n",
    "        'bridges': bridges\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# ESEMPIO DI UTILIZZO\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\\n\" + \"üåü\"*35)\n",
    "    print(\"  ARGUMENT COMPARISON - NETWORK ANALYSIS\")\n",
    "    print(\"üåü\"*35)\n",
    "    \n",
    "    # Carica dati\n",
    "    df = pd.read_excel(\"tweets_df.xlsx\")\n",
    "    \n",
    "    # Prepara mentions\n",
    "    df['mentions_clean'] = df['text_mentions'].apply(\n",
    "        lambda x: str(x).split() if pd.notna(x) else []\n",
    "    )\n",
    "    \n",
    "    # Filtra tweet con mentions\n",
    "    df = df[df['mentions_clean'].apply(len) > 0]\n",
    "    \n",
    "    print(f\"\\nüìä Dataset: {len(df)} tweets con mentions\")\n",
    "    \n",
    "    # Verifica colonna argument\n",
    "    if 'argument' not in df.columns:\n",
    "        print(\"\\n‚ö†Ô∏è  Colonna 'argument' non trovata!\")\n",
    "        exit()\n",
    "    \n",
    "    # Mostra distribuzione\n",
    "    arg_counts = df['argument'].value_counts()\n",
    "    print(f\"\\nüìà Distribuzione argument:\")\n",
    "    for arg, count in arg_counts.items():\n",
    "        print(f\"   {arg}: {count} tweets\")\n",
    "    \n",
    "    # Esegui analisi comparativa\n",
    "    results = analyze_by_argument(df, top_n_viz=60)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ ANALISI COMPLETA TERMINATA!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìÅ FILE GENERATI:\")\n",
    "    print(f\"   - network_by_argument.png\")\n",
    "    print(f\"   - argument_comparison_dashboard.png\")\n",
    "    print(f\"   - side_by_side_networks.png\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ú® Analisi completata con successo! ‚ú®\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff291e0",
   "metadata": {},
   "source": [
    "# Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb930a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import re\n",
    "import csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2da4a1",
   "metadata": {},
   "source": [
    "## Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INSERISCI LE TUE CREDENZIALI QUI =====\n",
    "CLIENT_ID = \"eDLBDOYp5Dg7AZFLdCoG1Q\"\n",
    "CLIENT_SECRET = \"EhcSQzvvKqkdQ2OTTLaRn-fUyndc0w\"\n",
    "USER_AGENT = \"python:bitcoin_scraper:v1.0 (by /u/ActKey2978)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559cab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# Intervallo date desiderato (UTC) - DAL 17 AL 20 OTTOBRE\n",
    "start_date = datetime(2025, 10, 17, 0, 0, 0, tzinfo=timezone.utc)\n",
    "end_date   = datetime(2025, 10, 21, 0, 0, 0, tzinfo=timezone.utc)\n",
    "\n",
    "# ========== AUTENTICAZIONE ==========\n",
    "print(\"üîê Autenticazione in corso...\")\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")\n",
    "print(\"‚úì Autenticazione riuscita! (solo lettura)\\n\")\n",
    "\n",
    "# ========== PARAMETRI ==========\n",
    "subreddits = \"CryptoCurrency+Bitcoin+btc+CryptoMarkets+investing+wallstreetbets\"\n",
    "keywords_filter = [\"bitcoin\", \"btc\"]\n",
    "output_filename = \"reddit_comments_bitcoin_oct17_20_2025.xlsx\"\n",
    "comments_data = []\n",
    "processed_threads = set()\n",
    "TARGET_COMMENTS = 15000  # Obiettivo commenti\n",
    "\n",
    "# Statistiche filtro lingua\n",
    "stats = {\n",
    "    \"threads_checked\": 0,\n",
    "    \"threads_non_english\": 0,\n",
    "    \"comments_checked\": 0,\n",
    "    \"comments_non_english\": 0\n",
    "}\n",
    "\n",
    "# ========== FUNZIONI DI UTILIT√Ä ==========\n",
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Verifica se il testo √® in inglese.\n",
    "    Ritorna True se inglese, False altrimenti.\n",
    "    \"\"\"\n",
    "    # Ignora testi troppo corti (meno di 10 caratteri)\n",
    "    if len(text.strip()) < 10:\n",
    "        return True  # Accetta comunque (troppo corto per rilevare)\n",
    "    \n",
    "    try:\n",
    "        # Rimuovi URL e menzioni per migliorare il rilevamento\n",
    "        clean_text = re.sub(r'https?://\\S+', '', text)\n",
    "        clean_text = re.sub(r'u/\\w+', '', clean_text)\n",
    "        clean_text = re.sub(r'@\\w+', '', clean_text)\n",
    "        \n",
    "        if len(clean_text.strip()) < 10:\n",
    "            return True\n",
    "        \n",
    "        detected_lang = detect(clean_text)\n",
    "        return detected_lang == 'en'\n",
    "    except LangDetectException:\n",
    "        # In caso di errore, accetta il testo\n",
    "        return True\n",
    "\n",
    "def extract_mentions(text):\n",
    "    \"\"\"Estrae le mention (@username o u/username) dal testo\"\"\"\n",
    "    mentions = []\n",
    "    mentions.extend(re.findall(r'u/(\\w+)', text))\n",
    "    mentions.extend(re.findall(r'@(\\w+)', text))\n",
    "    return list(set(mentions))\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    \"\"\"Estrae gli hashtag dal testo\"\"\"\n",
    "    return re.findall(r\"#\\w+\", text)\n",
    "\n",
    "def get_parent_author(comment):\n",
    "    \"\"\"Ottiene l'autore del commento parent (per network analysis)\"\"\"\n",
    "    try:\n",
    "        if comment.parent_id.startswith(\"t1_\"):\n",
    "            parent = reddit.comment(comment.parent_id.split(\"_\")[1])\n",
    "            return parent.author.name if parent.author else \"[deleted]\"\n",
    "        elif comment.parent_id.startswith(\"t3_\"):\n",
    "            parent = reddit.submission(comment.parent_id.split(\"_\")[1])\n",
    "            return parent.author.name if parent.author else \"[deleted]\"\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# ========== RICERCA THREAD ==========\n",
    "print(\"üîé Cerco thread su Bitcoin tra 17 e 20 ottobre 2025...\")\n",
    "print(f\"üìÖ Range: {start_date} - {end_date}\")\n",
    "print(f\"üéØ Subreddit: {subreddits}\")\n",
    "print(f\"üîç Keywords filter: {keywords_filter}\")\n",
    "print(f\"üåç Filtro lingua: SOLO INGLESE\")\n",
    "print(f\"üéØ Obiettivo: {TARGET_COMMENTS} commenti\\n\")\n",
    "\n",
    "# Strategia 1: Cerca post con keywords (pi√π efficiente)\n",
    "print(\"üîÑ Strategia 1: Ricerca per keyword...\")\n",
    "for submission in reddit.subreddit(subreddits).search(\n",
    "    \"bitcoin OR btc\",\n",
    "    sort=\"new\",\n",
    "    time_filter=\"month\",\n",
    "    limit=None\n",
    "):\n",
    "    # Check obiettivo\n",
    "    if len(comments_data) >= TARGET_COMMENTS:\n",
    "        print(f\"\\n‚úÖ Obiettivo raggiunto! {len(comments_data)} commenti raccolti\")\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        created_time = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # Filtra solo post tra 17 e 20 ottobre\n",
    "    if not (start_date <= created_time < end_date):\n",
    "        continue\n",
    "\n",
    "    # Filtra solo thread che menzionano Bitcoin/BTC\n",
    "    title_and_text = (submission.title + \" \" + (submission.selftext or \"\")).lower()\n",
    "    if not any(keyword in title_and_text for keyword in keywords_filter):\n",
    "        continue\n",
    "\n",
    "    # EVITA DUPLICATI\n",
    "    if submission.id in processed_threads:\n",
    "        continue\n",
    "    \n",
    "    processed_threads.add(submission.id)\n",
    "\n",
    "    # Salta thread senza commenti\n",
    "    if submission.num_comments == 0:\n",
    "        continue\n",
    "\n",
    "    # ========== FILTRO LINGUA THREAD ==========\n",
    "    stats[\"threads_checked\"] += 1\n",
    "    thread_text_to_check = submission.title + \" \" + (submission.selftext or \"\")\n",
    "    \n",
    "    if not is_english(thread_text_to_check):\n",
    "        stats[\"threads_non_english\"] += 1\n",
    "        print(f\"‚è≠Ô∏è  Thread NON inglese saltato: {submission.title[:50]}...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üß© Thread: {submission.title[:60]}... ({submission.subreddit.display_name}) - {submission.num_comments} commenti\")\n",
    "\n",
    "    # ========== SCARICA I COMMENTI ==========\n",
    "    try:\n",
    "        submission.comments.replace_more(limit=15)\n",
    "        \n",
    "        comments_count = 0\n",
    "        for comment in submission.comments.list():\n",
    "            if not comment.body or comment.body in [\"[deleted]\", \"[removed]\"]:\n",
    "                continue\n",
    "            \n",
    "            # FILTRA I COMMENTI PER DATA\n",
    "            try:\n",
    "                comment_created = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # SALTA COMMENTI FUORI DAL RANGE TEMPORALE\n",
    "            if not (start_date <= comment_created < end_date):\n",
    "                continue\n",
    "            \n",
    "            # ========== FILTRO LINGUA COMMENTO ==========\n",
    "            stats[\"comments_checked\"] += 1\n",
    "            if not is_english(comment.body):\n",
    "                stats[\"comments_non_english\"] += 1\n",
    "                continue\n",
    "            \n",
    "            if not comment.author:\n",
    "                author_name = \"[deleted]\"\n",
    "            else:\n",
    "                author_name = comment.author.name\n",
    "\n",
    "            # Estrai hashtags e mentions\n",
    "            hashtags = extract_hashtags(comment.body)\n",
    "            mentions = extract_mentions(comment.body)\n",
    "            \n",
    "            # Ottieni l'autore del parent (per network analysis)\n",
    "            parent_author = get_parent_author(comment)\n",
    "            \n",
    "            # Conta le interazioni\n",
    "            num_replies = len(comment.replies) if comment.replies else 0\n",
    "\n",
    "            comment_info = {\n",
    "                # Info thread\n",
    "                \"thread_title\": submission.title,\n",
    "                \"thread_author\": submission.author.name if submission.author else \"[deleted]\",\n",
    "                \"thread_score\": submission.score,\n",
    "                \"thread_num_comments\": submission.num_comments,\n",
    "                \n",
    "                # Info commento\n",
    "                \"text_id\": comment.id,\n",
    "                \"comment_parent_id\": comment.parent_id,\n",
    "                \"text_author\": author_name,\n",
    "                \"text\": comment.body,\n",
    "                \"likes\": comment.score,\n",
    "                \"text_date\": comment_created,\n",
    "                \"text_num_replies\": num_replies,\n",
    "                \"retweets\": None,\n",
    "                \n",
    "                # DATI PER NETWORK ANALYSIS\n",
    "                \"comment_parent_author\": parent_author,\n",
    "                \"text_mentions\": \", \".join(mentions),\n",
    "                \"text_hashtags\": \", \".join(hashtags),\n",
    "\n",
    "                # ARGUMENT:\n",
    "                \"argument\": \"Bitcoin\",\n",
    "                \"site\": \"Reddit\",\n",
    "            }\n",
    "\n",
    "            comments_data.append(comment_info)\n",
    "            comments_count += 1\n",
    "            \n",
    "            # Check obiettivo\n",
    "            if len(comments_data) >= TARGET_COMMENTS:\n",
    "                print(f\"  üéØ Obiettivo raggiunto!\")\n",
    "                break\n",
    "\n",
    "        print(f\"  üí¨ Commenti inglesi raccolti: {comments_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Errore nel thread: {e}\")\n",
    "    \n",
    "    time.sleep(1.2)\n",
    "\n",
    "# Strategia 2: Se non abbiamo raggiunto l'obiettivo, scansiona i post recenti\n",
    "if len(comments_data) < TARGET_COMMENTS:\n",
    "    print(f\"\\nüîÑ Strategia 2: Scansione post recenti... (raccolti: {len(comments_data)}/{TARGET_COMMENTS})\")\n",
    "    \n",
    "    for submission in reddit.subreddit(subreddits).new(limit=None):\n",
    "        # Check obiettivo\n",
    "        if len(comments_data) >= TARGET_COMMENTS:\n",
    "            print(f\"\\n‚úÖ Obiettivo raggiunto! {len(comments_data)} commenti raccolti\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            created_time = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Se siamo andati troppo indietro nel tempo, fermati\n",
    "        if created_time < start_date:\n",
    "            break\n",
    "\n",
    "        # Filtra solo post tra 17 e 20 ottobre\n",
    "        if not (start_date <= created_time < end_date):\n",
    "            continue\n",
    "\n",
    "        # Filtra solo thread che menzionano Bitcoin/BTC\n",
    "        title_and_text = (submission.title + \" \" + (submission.selftext or \"\")).lower()\n",
    "        if not any(keyword in title_and_text for keyword in keywords_filter):\n",
    "            continue\n",
    "\n",
    "        # EVITA DUPLICATI\n",
    "        if submission.id in processed_threads:\n",
    "            continue\n",
    "        \n",
    "        processed_threads.add(submission.id)\n",
    "\n",
    "        # Salta thread senza commenti\n",
    "        if submission.num_comments == 0:\n",
    "            continue\n",
    "\n",
    "        # ========== FILTRO LINGUA THREAD ==========\n",
    "        stats[\"threads_checked\"] += 1\n",
    "        thread_text_to_check = submission.title + \" \" + (submission.selftext or \"\")\n",
    "        \n",
    "        if not is_english(thread_text_to_check):\n",
    "            stats[\"threads_non_english\"] += 1\n",
    "            print(f\"‚è≠Ô∏è  Thread NON inglese saltato: {submission.title[:50]}...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üß© Thread: {submission.title[:60]}... ({submission.subreddit.display_name}) - {submission.num_comments} commenti\")\n",
    "\n",
    "        # ========== SCARICA I COMMENTI ==========\n",
    "        try:\n",
    "            submission.comments.replace_more(limit=15)\n",
    "            \n",
    "            comments_count = 0\n",
    "            for comment in submission.comments.list():\n",
    "                if not comment.body or comment.body in [\"[deleted]\", \"[removed]\"]:\n",
    "                    continue\n",
    "                \n",
    "                # FILTRA I COMMENTI PER DATA\n",
    "                try:\n",
    "                    comment_created = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                # SALTA COMMENTI FUORI DAL RANGE TEMPORALE\n",
    "                if not (start_date <= comment_created < end_date):\n",
    "                    continue\n",
    "                \n",
    "                # ========== FILTRO LINGUA COMMENTO ==========\n",
    "                stats[\"comments_checked\"] += 1\n",
    "                if not is_english(comment.body):\n",
    "                    stats[\"comments_non_english\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                if not comment.author:\n",
    "                    author_name = \"[deleted]\"\n",
    "                else:\n",
    "                    author_name = comment.author.name\n",
    "\n",
    "                # Estrai hashtags e mentions\n",
    "                hashtags = extract_hashtags(comment.body)\n",
    "                mentions = extract_mentions(comment.body)\n",
    "                \n",
    "                # Ottieni l'autore del parent (per network analysis)\n",
    "                parent_author = get_parent_author(comment)\n",
    "                \n",
    "                # Conta le interazioni\n",
    "                num_replies = len(comment.replies) if comment.replies else 0\n",
    "\n",
    "                comment_info = {\n",
    "                    # Info thread\n",
    "                    \"thread_title\": submission.title,\n",
    "                    \"thread_author\": submission.author.name if submission.author else \"[deleted]\",\n",
    "                    \"thread_score\": submission.score,\n",
    "                    \"thread_num_comments\": submission.num_comments,\n",
    "                    \n",
    "                    # Info commento\n",
    "                    \"text_id\": comment.id,\n",
    "                    \"comment_parent_id\": comment.parent_id,\n",
    "                    \"text_author\": author_name,\n",
    "                    \"text\": comment.body,\n",
    "                    \"likes\": comment.score,\n",
    "                    \"text_date\": comment_created,\n",
    "                    \"text_num_replies\": num_replies,\n",
    "                    \"retweets\": None,\n",
    "                    \n",
    "                    # DATI PER NETWORK ANALYSIS\n",
    "                    \"comment_parent_author\": parent_author,\n",
    "                    \"text_mentions\": \", \".join(mentions),\n",
    "                    \"text_hashtags\": \", \".join(hashtags),\n",
    "\n",
    "                    # ARGUMENT:\n",
    "                    \"argument\": \"Bitcoin\",\n",
    "                    \"site\": \"Reddit\",\n",
    "                }\n",
    "\n",
    "                comments_data.append(comment_info)\n",
    "                comments_count += 1\n",
    "                \n",
    "                # Check obiettivo\n",
    "                if len(comments_data) >= TARGET_COMMENTS:\n",
    "                    print(f\"  üéØ Obiettivo raggiunto!\")\n",
    "                    break\n",
    "\n",
    "            print(f\"  üí¨ Commenti inglesi raccolti: {comments_count}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Errore nel thread: {e}\")\n",
    "        \n",
    "        time.sleep(1.2)\n",
    "\n",
    "# ========== SALVATAGGIO ==========\n",
    "print(f\"\\nüì¶ Totale commenti raccolti: {len(comments_data)}\")\n",
    "\n",
    "if comments_data:\n",
    "    df = pd.DataFrame(comments_data)\n",
    "    \n",
    "    # Rimuovi duplicati\n",
    "    initial_count = len(df)\n",
    "    df = df.drop_duplicates(subset=['text_id'])\n",
    "    if len(df) < initial_count:\n",
    "        print(f\"üßπ Rimossi {initial_count - len(df)} commenti duplicati\")\n",
    "    \n",
    "    # Ordina per data\n",
    "    df = df.sort_values('text_date')\n",
    "    \n",
    "    # Rimuovi timezone per Excel\n",
    "    df['text_date'] = pd.to_datetime(df['text_date']).dt.tz_localize(None)\n",
    "    \n",
    "    # Prova a salvare in Excel\n",
    "    try:\n",
    "        df.to_excel(output_filename, index=False, engine='openpyxl')\n",
    "        print(f\"‚úÖ Salvato in Excel: {output_filename}\")\n",
    "    except PermissionError:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_filename = output_filename.replace('.xlsx', f'_{timestamp}.xlsx')\n",
    "        df.to_excel(backup_filename, index=False, engine='openpyxl')\n",
    "        print(f\"‚ö†Ô∏è  File aperto! Salvato come: {backup_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Errore Excel: {e}\")\n",
    "        csv_filename = output_filename.replace('.xlsx', '.csv')\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        print(f\"‚úÖ Salvato in CSV: {csv_filename}\")\n",
    "    \n",
    "    # Mostra statistiche filtro lingua\n",
    "    print(f\"\\nüåç Statistiche Filtro Lingua:\")\n",
    "    print(f\"   Thread controllati: {stats['threads_checked']}\")\n",
    "    print(f\"   Thread NON inglesi saltati: {stats['threads_non_english']}\")\n",
    "    print(f\"   Commenti controllati: {stats['comments_checked']}\")\n",
    "    print(f\"   Commenti NON inglesi saltati: {stats['comments_non_english']}\")\n",
    "    \n",
    "    # Mostra statistiche\n",
    "    print(f\"\\nüìä Statistiche Finali:\")\n",
    "    print(f\"   Thread processati: {len(processed_threads)}\")\n",
    "    print(f\"   Autori unici: {df['text_author'].nunique()}\")\n",
    "    print(f\"   Relazioni parent-child: {df['comment_parent_author'].notna().sum()}\")\n",
    "    \n",
    "    # Distribuzione temporale\n",
    "    df['comment_date'] = pd.to_datetime(df['text_date']).dt.date\n",
    "    print(f\"\\nüìÖ Distribuzione per giorno:\")\n",
    "    print(df['comment_date'].value_counts().sort_index())\n",
    "    \n",
    "    # Distribuzione oraria\n",
    "    print(f\"\\nüïê Distribuzione per ora:\")\n",
    "    df['hour'] = pd.to_datetime(df['text_date']).dt.hour\n",
    "    print(df['hour'].value_counts().sort_index().head(10))\n",
    "    \n",
    "    # STATISTICHE PER NETWORK ANALYSIS\n",
    "    print(f\"\\nüï∏Ô∏è Metriche Network Analysis:\")\n",
    "    print(f\"   Nodi (utenti): {df['text_author'].nunique()}\")\n",
    "    print(f\"   Edges potenziali (risposte): {df['comment_parent_author'].notna().sum()}\")\n",
    "    \n",
    "    # TOP 5 thread pi√π commentati\n",
    "    print(f\"\\nüî• Top 5 Thread pi√π commentati:\")\n",
    "    top_threads = df.groupby('thread_title').size().sort_values(ascending=False).head()\n",
    "    for title, count in top_threads.items():\n",
    "        print(f\"   {count:3d} commenti - {title[:60]}...\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nessun commento trovato nel periodo indicato.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2bd2f",
   "metadata": {},
   "source": [
    "## Nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff672357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INSERISCI LE TUE CREDENZIALI QUI =====\n",
    "CLIENT_ID = \"eDLBDOYp5Dg7AZFLdCoG1Q\"\n",
    "CLIENT_SECRET = \"EhcSQzvvKqkdQ2OTTLaRn-fUyndc0w\"\n",
    "USER_AGENT = \"python:bitcoin_scraper:v1.0 (by /u/ActKey2978)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a870f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "\n",
    "# Data di inizio - dal 17 ottobre in poi (NESSUN FILTRO FINE)\n",
    "start_date = datetime(2025, 10, 17, 0, 0, 0, tzinfo=timezone.utc)\n",
    "\n",
    "\n",
    "# ========== AUTENTICAZIONE ==========\n",
    "print(\"üîê Autenticazione in corso...\")\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")\n",
    "print(\"‚úì Autenticazione riuscita! (solo lettura)\\n\")\n",
    "\n",
    "\n",
    "# ========== PARAMETRI ==========\n",
    "subreddits = \"stocks+investing+wallstreetbets+StockMarket+technology+hardware+nvidia+pcmasterrace\"\n",
    "keywords_filter = [\"nvidia\", \"nvda\"]\n",
    "output_filename = \"reddit_comments_nvidia_from_oct17_ALL.xlsx\"\n",
    "comments_data = []\n",
    "processed_threads = set()\n",
    "TARGET_COMMENTS = 15000  # Obiettivo commenti\n",
    "\n",
    "\n",
    "# Statistiche filtro lingua\n",
    "stats = {\n",
    "    \"threads_checked\": 0,\n",
    "    \"threads_non_english\": 0,\n",
    "    \"comments_checked\": 0,\n",
    "    \"comments_non_english\": 0,\n",
    "    \"comments_whitelisted\": 0\n",
    "}\n",
    "\n",
    "\n",
    "# ========== WHITELIST TERMINI NVIDIA/TECH/FINANZIARI ==========\n",
    "NVIDIA_TECH_TERMS = {\n",
    "    # NVIDIA specifici\n",
    "    'nvidia', 'nvda', 'jensen', 'huang', 'geforce', 'rtx', 'gtx', 'quadro',\n",
    "    'tesla', 'cuda', 'cudnn', 'tensor', 'tensorrt', 'dlss', 'ray tracing',\n",
    "    'ampere', 'ada', 'hopper', 'blackwell', 'grace', 'lovelace',\n",
    "    '4090', '4080', '4070', '4060', '3090', '3080', '3070', '3060',\n",
    "    'a100', 'h100', 'b100', 'l40', 'a40', 'dgx', 'hgx',\n",
    "\n",
    "    # GPU/Gaming\n",
    "    'gpu', 'graphics card', 'vram', 'memory', 'bandwidth', 'cores',\n",
    "    'fps', 'framerate', 'gaming', 'gamer', 'overclock', 'oc', 'tdp',\n",
    "    'bottleneck', 'benchmark', 'rasterization', 'shaders',\n",
    "    'vr', 'virtual reality', '4k', '1440p', '1080p', '8k',\n",
    "\n",
    "    # AI/ML/Datacenter\n",
    "    'ai', 'artificial intelligence', 'ml', 'machine learning', 'deep learning',\n",
    "    'llm', 'large language model', 'transformer', 'neural network', 'inference',\n",
    "    'training', 'model', 'pytorch', 'tensorflow', 'onnx', 'triton',\n",
    "    'datacenter', 'data center', 'hpc', 'supercomputer', 'cluster',\n",
    "    'cloud', 'aws', 'azure', 'gcp', 'hyperscaler',\n",
    "\n",
    "    # Competitori\n",
    "    'amd', 'intel', 'radeon', 'arc', 'alchemist', 'battlemage',\n",
    "    'mi300', 'instinct', 'epyc', 'ryzen', 'threadripper',\n",
    "\n",
    "    # Tech slang\n",
    "    'mobo', 'motherboard', 'psu', 'power supply', 'cpu', 'processor',\n",
    "    'ram', 'ssd', 'nvme', 'pcie', 'rgb', 'cooler', 'thermal paste',\n",
    "    'bios', 'uefi', 'driver', 'firmware', 'update', 'patch',\n",
    "\n",
    "    # Stock/Trading\n",
    "    'stock', 'stocks', 'share', 'shares', 'ticker', 'nasdaq', 'sp500',\n",
    "    'earnings', 'revenue', 'profit', 'margin', 'guidance', 'beat', 'miss',\n",
    "    'bull', 'bear', 'bullish', 'bearish', 'calls', 'puts', 'options',\n",
    "    'long', 'short', 'squeeze', 'gamma', 'theta', 'strike', 'expiry',\n",
    "    'ath', 'atl', 'pe ratio', 'market cap', 'mcap', 'valuation',\n",
    "\n",
    "    # Finance slang\n",
    "    'hodl', 'hodling', 'stonks', 'tendies', 'yolo', 'fomo', 'fud',\n",
    "    'moon', 'lambo', 'rocket', 'ape', 'diamond hands', 'paper hands',\n",
    "    'buy the dip', 'btfd', 'dca', 'rsi', 'macd', 'support', 'resistance',\n",
    "\n",
    "    # Tech companies\n",
    "    'tsmc', 'samsung', 'micron', 'sk hynix', 'broadcom', 'qualcomm',\n",
    "    'arm', 'apple', 'microsoft', 'google', 'meta', 'amazon', 'openai',\n",
    "\n",
    "    # Acronimi comuni\n",
    "    'imo', 'imho', 'tbh', 'ngl', 'af', 'rn', 'fr', 'btw', 'fyi',\n",
    "    'dyor', 'nfa', 'not financial advice', 'afaik', 'iirc',\n",
    "\n",
    "    # Gaming/PC\n",
    "    'pc', 'rig', 'build', 'prebuilt', 'custom', 'watercooling', 'aio',\n",
    "    'case', 'fans', 'airflow', 'temps', 'benchmark', 'stress test',\n",
    "    'msrp', 'scalper', 'scalping', 'shortage', 'availability',\n",
    "\n",
    "    # Numeri comuni\n",
    "    '100k', '200k', '500k', '1m', '10m', '100m', '1b', '10b', '100b',\n",
    "    'trillion', 'billion', 'million', 'thousand'\n",
    "}\n",
    "\n",
    "\n",
    "# ========== FUNZIONI DI UTILIT√Ä ==========\n",
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Verifica se il testo √® probabilmente in inglese.\n",
    "    Versione PERMISSIVA con whitelist termini NVIDIA/tech/finance.\n",
    "    \"\"\"\n",
    "    if len(text.strip()) < 30:\n",
    "        return True\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    if any(term in text_lower for term in NVIDIA_TECH_TERMS):\n",
    "        stats[\"comments_whitelisted\"] += 1\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        clean_text = re.sub(r'https?://\\S+|u/\\w+|@\\w+|\\$[A-Z]+|[0-9$%‚Ç¨¬£¬•‚Çø]|#\\w+', '', text)\n",
    "        if len(clean_text.strip()) < 30:\n",
    "            return True\n",
    "        detected_lang = detect(clean_text)\n",
    "        accepted_langs = ['en', 'nl', 'da', 'no', 'sv', 'de', 'cy']\n",
    "        return detected_lang in accepted_langs\n",
    "    except LangDetectException:\n",
    "        return True\n",
    "\n",
    "\n",
    "def extract_mentions(text):\n",
    "    \"\"\"Estrae le mention (@username o u/username) dal testo\"\"\"\n",
    "    mentions = []\n",
    "    mentions.extend(re.findall(r'u/(\\w+)', text))\n",
    "    mentions.extend(re.findall(r'@(\\w+)', text))\n",
    "    return list(set(mentions))\n",
    "\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    \"\"\"Estrae gli hashtag dal testo\"\"\"\n",
    "    return re.findall(r\"#\\w+\", text)\n",
    "\n",
    "\n",
    "def get_parent_author(comment):\n",
    "    \"\"\"Ottiene l'autore del commento parent (per network analysis)\"\"\"\n",
    "    try:\n",
    "        if comment.parent_id.startswith(\"t1_\"):\n",
    "            parent = reddit.comment(comment.parent_id.split(\"_\")[1])\n",
    "            return parent.author.name if parent.author else \"[deleted]\"\n",
    "        elif comment.parent_id.startswith(\"t3_\"):\n",
    "            parent = reddit.submission(comment.parent_id.split(\"_\")[1])\n",
    "            return parent.author.name if parent.author else \"[deleted]\"\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# ========== RICERCA THREAD ==========\n",
    "print(\"üîé Cerco TUTTI i thread e commenti su Nvidia dal 17 ottobre 2025 in poi...\")\n",
    "print(f\"üìÖ Range THREAD: dal {start_date.date()} in poi\")\n",
    "print(f\"üìÖ Range COMMENTI: dal {start_date.date()} in poi\")\n",
    "print(f\"üéØ Subreddit: {subreddits}\")\n",
    "print(f\"üîç Keywords filter: {keywords_filter}\")\n",
    "print(f\"üåç Filtro lingua: INGLESE (permissivo + whitelist NVIDIA/tech)\")\n",
    "print(f\"üéØ Obiettivo: {TARGET_COMMENTS} commenti\\n\")\n",
    "\n",
    "\n",
    "# Strategia 1: Cerca post con keywords\n",
    "print(\"üîÑ Strategia 1: Ricerca per keyword...\")\n",
    "for submission in reddit.subreddit(subreddits).search(\n",
    "    \"nvidia OR nvda\",\n",
    "    sort=\"new\",\n",
    "    time_filter=\"month\",\n",
    "    limit=None\n",
    "):\n",
    "    if len(comments_data) >= TARGET_COMMENTS:\n",
    "        print(f\"\\n‚úÖ Obiettivo raggiunto! {len(comments_data)} commenti raccolti\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        created_time = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # ‚úÖ Prendi solo thread DAL 17 ottobre IN POI\n",
    "    if created_time < start_date:\n",
    "        continue\n",
    "\n",
    "    # Filtra solo thread che menzionano Nvidia/NVDA\n",
    "    title_and_text = (submission.title + \" \" + (submission.selftext or \"\")).lower()\n",
    "    if not any(keyword in title_and_text for keyword in keywords_filter):\n",
    "        continue\n",
    "\n",
    "    # EVITA DUPLICATI\n",
    "    if submission.id in processed_threads:\n",
    "        continue\n",
    "    processed_threads.add(submission.id)\n",
    "\n",
    "    # Salta thread senza commenti\n",
    "    if submission.num_comments == 0:\n",
    "        continue\n",
    "\n",
    "    # ========== FILTRO LINGUA THREAD ==========\n",
    "    stats[\"threads_checked\"] += 1\n",
    "    thread_text_to_check = submission.title + \" \" + (submission.selftext or \"\")\n",
    "\n",
    "    if not is_english(thread_text_to_check):\n",
    "        stats[\"threads_non_english\"] += 1\n",
    "        print(f\"‚è≠Ô∏è  Thread NON inglese saltato: {submission.title[:50]}...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üß© Thread: {submission.title[:60]}... ({submission.subreddit.display_name}) - {submission.num_comments} commenti\")\n",
    "\n",
    "    # ========== SCARICA I COMMENTI ==========\n",
    "    try:\n",
    "        submission.comments.replace_more(limit=15)\n",
    "\n",
    "        comments_count = 0\n",
    "        for comment in submission.comments.list():\n",
    "            if not comment.body or comment.body in [\"[deleted]\", \"[removed]\"]:\n",
    "                continue\n",
    "\n",
    "            # ‚úÖ PRENDI SOLO COMMENTI DAL 17 OTTOBRE IN POI\n",
    "            try:\n",
    "                comment_created = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # SALTA COMMENTI PRIMA DEL 17 OTTOBRE\n",
    "            if comment_created < start_date:\n",
    "                continue\n",
    "\n",
    "            # ========== FILTRO LINGUA COMMENTO (CON WHITELIST) ==========\n",
    "            stats[\"comments_checked\"] += 1\n",
    "            if not is_english(comment.body):\n",
    "                stats[\"comments_non_english\"] += 1\n",
    "                continue\n",
    "\n",
    "            author_name = comment.author.name if comment.author else \"[deleted]\"\n",
    "\n",
    "            # Estrai hashtags e mentions\n",
    "            hashtags = extract_hashtags(comment.body)\n",
    "            mentions = extract_mentions(comment.body)\n",
    "            parent_author = get_parent_author(comment)\n",
    "            num_replies = len(comment.replies) if comment.replies else 0\n",
    "\n",
    "            comment_info = {\n",
    "                \"thread_title\": submission.title,\n",
    "                \"thread_author\": submission.author.name if submission.author else \"[deleted]\",\n",
    "                \"thread_score\": submission.score,\n",
    "                \"thread_num_comments\": submission.num_comments,\n",
    "                \"text_id\": comment.id,\n",
    "                \"comment_parent_id\": comment.parent_id,\n",
    "                \"text_author\": author_name,\n",
    "                \"text\": comment.body,\n",
    "                \"likes\": comment.score,\n",
    "                \"text_date\": comment_created,\n",
    "                \"text_num_replies\": num_replies,\n",
    "                \"retweets\": None,\n",
    "                \"comment_parent_author\": parent_author,\n",
    "                \"text_mentions\": \", \".join(mentions),\n",
    "                \"text_hashtags\": \", \".join(hashtags),\n",
    "                \"argument\": \"Nvidia\",\n",
    "                \"site\": \"Reddit\",\n",
    "            }\n",
    "\n",
    "            comments_data.append(comment_info)\n",
    "            comments_count += 1\n",
    "\n",
    "            if len(comments_data) >= TARGET_COMMENTS:\n",
    "                print(f\"  üéØ Obiettivo raggiunto!\")\n",
    "                break\n",
    "\n",
    "        print(f\"  üí¨ Commenti raccolti: {comments_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Errore nel thread: {e}\")\n",
    "\n",
    "    time.sleep(1.2)\n",
    "\n",
    "\n",
    "# Strategia 2: Scansiona post recenti\n",
    "if len(comments_data) < TARGET_COMMENTS:\n",
    "    print(f\"\\nüîÑ Strategia 2: Scansione post recenti... (raccolti: {len(comments_data)}/{TARGET_COMMENTS})\")\n",
    "\n",
    "    for submission in reddit.subreddit(subreddits).new(limit=None):\n",
    "        if len(comments_data) >= TARGET_COMMENTS:\n",
    "            print(f\"\\n‚úÖ Obiettivo raggiunto! {len(comments_data)} commenti raccolti\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            created_time = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Prendi solo thread dal 17 ottobre in poi\n",
    "        if created_time < start_date:\n",
    "            continue\n",
    "\n",
    "        # Filtra solo thread che menzionano Nvidia/NVDA\n",
    "        title_and_text = (submission.title + \" \" + (submission.selftext or \"\")).lower()\n",
    "        if not any(keyword in title_and_text for keyword in keywords_filter):\n",
    "            continue\n",
    "\n",
    "        if submission.id in processed_threads:\n",
    "            continue\n",
    "        processed_threads.add(submission.id)\n",
    "\n",
    "        if submission.num_comments == 0:\n",
    "            continue\n",
    "\n",
    "        stats[\"threads_checked\"] += 1\n",
    "        thread_text_to_check = submission.title + \" \" + (submission.selftext or \"\")\n",
    "\n",
    "        if not is_english(thread_text_to_check):\n",
    "            stats[\"threads_non_english\"] += 1\n",
    "            print(f\"‚è≠Ô∏è  Thread NON inglese saltato: {submission.title[:50]}...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üß© Thread: {submission.title[:60]}... ({submission.subreddit.display_name}) - {submission.num_comments} commenti\")\n",
    "\n",
    "        try:\n",
    "            submission.comments.replace_more(limit=15)\n",
    "\n",
    "            comments_count = 0\n",
    "            for comment in submission.comments.list():\n",
    "                if not comment.body or comment.body in [\"[deleted]\", \"[removed]\"]:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    comment_created = datetime.fromtimestamp(comment.created_utc, tz=timezone.utc)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if comment_created < start_date:\n",
    "                    continue\n",
    "\n",
    "                stats[\"comments_checked\"] += 1\n",
    "                if not is_english(comment.body):\n",
    "                    stats[\"comments_non_english\"] += 1\n",
    "                    continue\n",
    "\n",
    "                author_name = comment.author.name if comment.author else \"[deleted]\"\n",
    "                hashtags = extract_hashtags(comment.body)\n",
    "                mentions = extract_mentions(comment.body)\n",
    "                parent_author = get_parent_author(comment)\n",
    "                num_replies = len(comment.replies) if comment.replies else 0\n",
    "\n",
    "                comment_info = {\n",
    "                    \"thread_title\": submission.title,\n",
    "                    \"thread_author\": submission.author.name if submission.author else \"[deleted]\",\n",
    "                    \"thread_score\": submission.score,\n",
    "                    \"thread_num_comments\": submission.num_comments,\n",
    "                    \"text_id\": comment.id,\n",
    "                    \"comment_parent_id\": comment.parent_id,\n",
    "                    \"text_author\": author_name,\n",
    "                    \"text\": comment.body,\n",
    "                    \"likes\": comment.score,\n",
    "                    \"text_date\": comment_created,\n",
    "                    \"text_num_replies\": num_replies,\n",
    "                    \"retweets\": None,\n",
    "                    \"comment_parent_author\": parent_author,\n",
    "                    \"text_mentions\": \", \".join(mentions),\n",
    "                    \"text_hashtags\": \", \".join(hashtags),\n",
    "                    \"argument\": \"Nvidia\",\n",
    "                    \"site\": \"Reddit\",\n",
    "                }\n",
    "\n",
    "                comments_data.append(comment_info)\n",
    "                comments_count += 1\n",
    "\n",
    "                if len(comments_data) >= TARGET_COMMENTS:\n",
    "                    print(f\"  üéØ Obiettivo raggiunto!\")\n",
    "                    break\n",
    "\n",
    "            print(f\"  üí¨ Commenti raccolti: {comments_count}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Errore nel thread: {e}\")\n",
    "\n",
    "        time.sleep(1.2)\n",
    "\n",
    "\n",
    "# ========== SALVATAGGIO ==========\n",
    "print(f\"\\nüì¶ Totale commenti raccolti: {len(comments_data)}\")\n",
    "\n",
    "if comments_data:\n",
    "    df = pd.DataFrame(comments_data)\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df = df.drop_duplicates(subset=['text_id'])\n",
    "    if len(df) < initial_count:\n",
    "        print(f\"üßπ Rimossi {initial_count - len(df)} commenti duplicati\")\n",
    "\n",
    "    df = df.sort_values('text_date')\n",
    "    df['text_date'] = pd.to_datetime(df['text_date']).dt.tz_localize(None)\n",
    "\n",
    "    try:\n",
    "        df.to_excel(output_filename, index=False, engine='openpyxl')\n",
    "        print(f\"‚úÖ Salvato in Excel: {output_filename}\")\n",
    "    except PermissionError:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_filename = output_filename.replace('.xlsx', f'_{timestamp}.xlsx')\n",
    "        df.to_excel(backup_filename, index=False, engine='openpyxl')\n",
    "        print(f\"‚ö†Ô∏è  File aperto! Salvato come: {backup_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Errore Excel: {e}\")\n",
    "        csv_filename = output_filename.replace('.xlsx', '.csv')\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        print(f\"‚úÖ Salvato in CSV: {csv_filename}\")\n",
    "\n",
    "    print(f\"\\nüåç Statistiche Filtro Lingua:\")\n",
    "    print(f\"   Thread controllati: {stats['threads_checked']}\")\n",
    "    print(f\"   Thread filtrati: {stats['threads_non_english']}\")\n",
    "    print(f\"   Commenti controllati: {stats['comments_checked']}\")\n",
    "    print(f\"   Commenti filtrati: {stats['comments_non_english']}\")\n",
    "    print(f\"   ‚úÖ Commenti salvati via whitelist: {stats['comments_whitelisted']}\")\n",
    "\n",
    "    print(f\"\\nüìä Statistiche Finali:\")\n",
    "    print(f\"   Thread processati: {len(processed_threads)}\")\n",
    "    print(f\"   Autori unici: {df['text_author'].nunique()}\")\n",
    "    print(f\"   Relazioni parent-child: {df['comment_parent_author'].notna().sum()}\")\n",
    "\n",
    "    df['comment_date'] = pd.to_datetime(df['text_date']).dt.date\n",
    "    print(f\"\\nüìÖ Distribuzione per giorno:\")\n",
    "    print(df['comment_date'].value_counts().sort_index())\n",
    "\n",
    "    print(f\"\\nüïê Distribuzione per ora:\")\n",
    "    df['hour'] = pd.to_datetime(df['text_date']).dt.hour\n",
    "    print(df['hour'].value_counts().sort_index().head(10))\n",
    "\n",
    "    print(f\"\\nüï∏Ô∏è Metriche Network Analysis:\")\n",
    "    print(f\"   Nodi (utenti): {df['text_author'].nunique()}\")\n",
    "    print(f\"   Edges potenziali (risposte): {df['comment_parent_author'].notna().sum()}\")\n",
    "\n",
    "    print(f\"\\nüî• Top 5 Thread pi√π commentati:\")\n",
    "    top_threads = df.groupby('thread_title').size().sort_values(ascending=False).head()\n",
    "    for title, count in top_threads.items():\n",
    "        print(f\"   {count:3d} commenti - {title[:60]}...\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nessun commento trovato nel periodo indicato.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"reddit_comments_nvidia_oct17_22_2025.xlsx\")\n",
    "df['text_date'] = pd.to_datetime(df['text_date'])\n",
    "\n",
    "# Filtra commenti dal 17 al 20 ottobre 2025\n",
    "df_17_20 = df[(df['text_date'] >= '2025-10-17') & (df['text_date'] < '2025-10-21')]\n",
    "\n",
    "print(f\"Commenti filtrati: {len(df_17_20)}\")\n",
    "df_17_20.to_excel(\"reddit_comments_nvidia_oct17_20_2025.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7ea8d",
   "metadata": {},
   "source": [
    "## Threads df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('reddit_comments_bitcoin_oct17_20_2025.xlsx')\n",
    "df2 = pd.read_excel('reddit_comments_nvidia_oct17_20_2025.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699fbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_reddit_Bitcoin = df1.sample(n=2100, random_state=42)\n",
    "#sample_reddit_Nvidia = df2.sample(n=2100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_df = pd.concat([sample_reddit_Bitcoin, sample_reddit_Nvidia], ignore_index=True)\n",
    "threads_df.to_excel('threads_df.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a382c1a",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Modern color palette\n",
    "COLORS = {\n",
    "    'primary': '#FF6B6B',    # Red/coral\n",
    "    'secondary': '#4ECDC4',  # Turquoise\n",
    "    'accent': '#95E1D3',     # Aqua green\n",
    "    'dark': '#34495e',       # Dark gray\n",
    "    'light': '#ECF0F1',      # Light gray\n",
    "    'combined': '#9B59B6'    # Purple for combined analysis\n",
    "}\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "# ==========================================\n",
    "# 1. NETWORK CONSTRUCTION (REDDIT VERSION)\n",
    "# ==========================================\n",
    "\n",
    "def build_directed_weighted_network(df, argument_name):\n",
    "    \"\"\"\n",
    "    Main network: DIRECTED & WEIGHTED\n",
    "    Edge: text_author ‚Üí comment_parent_author (who replies to whom)\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        user = row['text_author']\n",
    "        parent = row['comment_parent_author']\n",
    "        \n",
    "        # Skip if parent is null (root comment) or if it's a self-reply\n",
    "        if pd.notna(parent) and user != parent:\n",
    "            if G.has_edge(user, parent):\n",
    "                G[user][parent]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(user, parent, weight=1)\n",
    "    \n",
    "    print(f\"\\nüîµ DIRECTED WEIGHTED NETWORK - {argument_name}\")\n",
    "    print(f\"   Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "def build_reciprocal_network(G_directed, argument_name):\n",
    "    \"\"\"Reciprocal network: UNDIRECTED & WEIGHTED\"\"\"\n",
    "    G_reciprocal = nx.Graph()\n",
    "    \n",
    "    for u, v, data in G_directed.edges(data=True):\n",
    "        if G_directed.has_edge(v, u):\n",
    "            weight = data['weight'] + G_directed[v][u]['weight']\n",
    "            if not G_reciprocal.has_edge(u, v):\n",
    "                G_reciprocal.add_edge(u, v, weight=weight)\n",
    "    \n",
    "    print(f\"\\nüü¢ RECIPROCAL UNDIRECTED NETWORK - {argument_name}\")\n",
    "    print(f\"   Nodes: {G_reciprocal.number_of_nodes()}, Edges: {G_reciprocal.number_of_edges()}\")\n",
    "    \n",
    "    return G_reciprocal\n",
    "\n",
    "def build_backbone_network(G_directed, argument_name, min_weight=3):\n",
    "    \"\"\"Backbone network: DIRECTED & UNWEIGHTED\"\"\"\n",
    "    G_backbone = nx.DiGraph()\n",
    "    \n",
    "    for u, v, data in G_directed.edges(data=True):\n",
    "        if data['weight'] >= min_weight:\n",
    "            G_backbone.add_edge(u, v)\n",
    "    \n",
    "    print(f\"\\nüî¥ BACKBONE NETWORK - {argument_name} (min weight={min_weight})\")\n",
    "    print(f\"   Nodes: {G_backbone.number_of_nodes()}, Edges: {G_backbone.number_of_edges()}\")\n",
    "    \n",
    "    return G_backbone\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. NETWORK METRICS & STATISTICS\n",
    "# ==========================================\n",
    "\n",
    "def analyze_directed_network(G, argument_name):\n",
    "    \"\"\"Directed network analysis\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìà DIRECTED NETWORK ANALYSIS - {argument_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    in_degree_weighted = dict(G.in_degree(weight='weight'))\n",
    "    top_mentioned = sorted(in_degree_weighted.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nüéØ TOP 10 MOST REPLIED TO (receive most replies):\")\n",
    "    for i, (user, replies) in enumerate(top_mentioned, 1):\n",
    "        print(f\"   {i}. u/{user}: {replies} replies received\")\n",
    "    \n",
    "    out_degree = dict(G.out_degree())\n",
    "    top_active = sorted(out_degree.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nüí¨ TOP 10 MOST ACTIVE (reply to most different users):\")\n",
    "    for i, (user, replies) in enumerate(top_active, 1):\n",
    "        print(f\"   {i}. u/{user}: replies to {replies} users\")\n",
    "    \n",
    "    pagerank = nx.pagerank(G, weight='weight')\n",
    "    top_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\n‚≠ê TOP 10 PAGERANK (overall influence):\")\n",
    "    for i, (user, score) in enumerate(top_pagerank, 1):\n",
    "        print(f\"   {i}. u/{user}: {score:.5f}\")\n",
    "    \n",
    "    try:\n",
    "        hits = nx.hits(G, max_iter=100)\n",
    "        authorities = sorted(hits[0].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        hubs = sorted(hits[1].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        \n",
    "        print(\"\\nüèÜ TOP 5 AUTHORITIES (most cited):\")\n",
    "        for i, (user, score) in enumerate(authorities, 1):\n",
    "            print(f\"   {i}. u/{user}: {score:.5f}\")\n",
    "        \n",
    "        print(\"\\nüîó TOP 5 HUBS (cite most):\")\n",
    "        for i, (user, score) in enumerate(hubs, 1):\n",
    "            print(f\"   {i}. u/{user}: {score:.5f}\")\n",
    "    except:\n",
    "        print(\"\\n‚ö†Ô∏è HITS algorithm did not converge\")\n",
    "    \n",
    "    return {\n",
    "        'in_degree_weighted': in_degree_weighted,\n",
    "        'pagerank': pagerank,\n",
    "        'out_degree': out_degree\n",
    "    }\n",
    "\n",
    "def analyze_reciprocal_network(G, argument_name):\n",
    "    \"\"\"Reciprocal network analysis\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìà RECIPROCAL NETWORK ANALYSIS - {argument_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if G.number_of_edges() == 0:\n",
    "        print(\"‚ö†Ô∏è No reciprocal interactions found!\")\n",
    "        return {}\n",
    "    \n",
    "    betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "    top_betweenness = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nüåâ TOP 10 BRIDGES (key connectors between groups):\")\n",
    "    for i, (user, score) in enumerate(top_betweenness, 1):\n",
    "        print(f\"   {i}. u/{user}: {score:.5f}\")\n",
    "    \n",
    "    clustering = nx.clustering(G, weight='weight')\n",
    "    avg_clustering = sum(clustering.values()) / len(clustering)\n",
    "    \n",
    "    print(f\"\\nüîó CLUSTERING COEFFICIENT: {avg_clustering:.4f}\")\n",
    "    \n",
    "    try:\n",
    "        import community as community_louvain\n",
    "        communities = community_louvain.best_partition(G, weight='weight')\n",
    "        n_communities = len(set(communities.values()))\n",
    "        \n",
    "        print(f\"\\nüë• COMMUNITY DETECTION: {n_communities} communities\")\n",
    "        \n",
    "        comm_sizes = Counter(communities.values())\n",
    "        for comm_id, size in comm_sizes.most_common(5):\n",
    "            members = [u for u, c in communities.items() if c == comm_id][:5]\n",
    "            print(f\"   Community {comm_id}: {size} members (e.g.: {', '.join(members)})\")\n",
    "        \n",
    "        return {\n",
    "            'betweenness': betweenness,\n",
    "            'clustering': clustering,\n",
    "            'communities': communities\n",
    "        }\n",
    "    except ImportError:\n",
    "        print(\"\\n‚ö†Ô∏è python-louvain not installed (pip install python-louvain)\")\n",
    "        return {\n",
    "            'betweenness': betweenness,\n",
    "            'clustering': clustering\n",
    "        }\n",
    "\n",
    "def analyze_backbone_network(G, argument_name):\n",
    "    \"\"\"Backbone network analysis\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìà BACKBONE NETWORK ANALYSIS - {argument_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if G.number_of_edges() == 0:\n",
    "        print(\"‚ö†Ô∏è Empty backbone - reduce min_weight\")\n",
    "        return {}\n",
    "    \n",
    "    scc = list(nx.strongly_connected_components(G))\n",
    "    print(f\"\\nüîÑ STRONGLY CONNECTED COMPONENTS: {len(scc)}\")\n",
    "    largest_scc = max(scc, key=len)\n",
    "    print(f\"   Largest component: {len(largest_scc)} nodes\")\n",
    "    \n",
    "    G_undirected = G.to_undirected()\n",
    "    G_undirected.remove_edges_from(nx.selfloop_edges(G_undirected))\n",
    "    \n",
    "    core_numbers = nx.core_number(G_undirected)\n",
    "    max_core = max(core_numbers.values())\n",
    "    \n",
    "    print(f\"\\nüíé K-CORE DECOMPOSITION:\")\n",
    "    print(f\"   Max core number: {max_core}\")\n",
    "    k_core = [u for u, k in core_numbers.items() if k == max_core]\n",
    "    print(f\"   {max_core}-core: {len(k_core)} nodes\")\n",
    "    if len(k_core) <= 10:\n",
    "        print(f\"   Members: {', '.join(k_core)}\")\n",
    "    \n",
    "    try:\n",
    "        import community as community_louvain\n",
    "        G_und_simple = nx.Graph()\n",
    "        for u, v in G.edges():\n",
    "            if u != v:\n",
    "                G_und_simple.add_edge(u, v)\n",
    "        \n",
    "        communities = community_louvain.best_partition(G_und_simple)\n",
    "        n_communities = len(set(communities.values()))\n",
    "        \n",
    "        print(f\"\\nüö∂ COMMUNITY DETECTION: {n_communities} communities\")\n",
    "        \n",
    "        return {\n",
    "            'scc': scc,\n",
    "            'core_numbers': core_numbers,\n",
    "            'communities': communities\n",
    "        }\n",
    "    except ImportError:\n",
    "        return {\n",
    "            'scc': scc,\n",
    "            'core_numbers': core_numbers\n",
    "        }\n",
    "\n",
    "# ==========================================\n",
    "# 3. VISUALIZATIONS\n",
    "# ==========================================\n",
    "\n",
    "def visualize_directed_network(G, metrics, argument_name, top_n=200):\n",
    "    \"\"\"Visualize directed network\"\"\"\n",
    "    top_nodes = sorted(metrics['pagerank'].items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_nodes = [n[0] for n in top_nodes]\n",
    "    G_sub = G.subgraph(top_nodes).copy()\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    pos = nx.spring_layout(G_sub, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    node_sizes = [metrics['pagerank'].get(n, 0) * 50000 for n in G_sub.nodes()]\n",
    "    edges = G_sub.edges()\n",
    "    weights = [G_sub[u][v]['weight'] for u, v in edges]\n",
    "    \n",
    "    # Calculate colors based on weight for better visibility\n",
    "    max_weight = max(weights) if weights else 1\n",
    "    edge_colors = [plt.cm.Blues(0.3 + 0.7 * (w / max_weight)) for w in weights]\n",
    "    \n",
    "    nx.draw_networkx_edges(G_sub, pos, edge_color=edge_colors, \n",
    "                          width=[w*0.6 for w in weights], alpha=0.7,\n",
    "                          arrows=True, arrowsize=10, \n",
    "                          arrowstyle='-|>', connectionstyle='arc3,rad=0.1')\n",
    "    \n",
    "    nx.draw_networkx_nodes(G_sub, pos, node_size=node_sizes, \n",
    "                          node_color=COLORS['secondary'], alpha=0.8, \n",
    "                          edgecolors=COLORS['dark'], linewidths=2)\n",
    "    \n",
    "    nx.draw_networkx_labels(G_sub, pos, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title(f'{argument_name} - DIRECTED NETWORK (Reddit Replies)\\nTop {top_n} users by PageRank (Node size = influence)', \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{argument_name.lower()}_directed_network.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(f\"\\nüíæ Saved: {argument_name.lower()}_directed_network.png\")\n",
    "\n",
    "def visualize_reciprocal_network(G, metrics, argument_name, top_n=300):\n",
    "    \"\"\"Visualize reciprocal network\"\"\"\n",
    "    if G.number_of_edges() == 0:\n",
    "        print(\"‚ö†Ô∏è No reciprocal interactions to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Use the entire network without filtering\n",
    "    G_sub = G.copy()\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    pos = nx.spring_layout(G_sub, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    if 'communities' in metrics:\n",
    "        communities = metrics['communities']\n",
    "        node_colors = [communities.get(n, 0) for n in G_sub.nodes()]\n",
    "        cmap = plt.cm.Set3\n",
    "    else:\n",
    "        node_colors = COLORS['accent']\n",
    "        cmap = None\n",
    "    \n",
    "    weights = [G_sub[u][v]['weight'] for u, v in G_sub.edges()]\n",
    "    \n",
    "    # Apply same style as directed network\n",
    "    max_weight = max(weights) if weights else 1\n",
    "    edge_colors = [plt.cm.Greens(0.3 + 0.7 * (w / max_weight)) for w in weights]\n",
    "    \n",
    "    nx.draw_networkx_edges(G_sub, pos, edge_color=edge_colors,\n",
    "                          width=[w*0.6 for w in weights], alpha=0.7)\n",
    "    \n",
    "    nx.draw_networkx_nodes(G_sub, pos, node_size=300, \n",
    "                          node_color=node_colors, cmap=cmap, \n",
    "                          alpha=0.9, edgecolors=COLORS['dark'], linewidths=1.5)\n",
    "    \n",
    "    nx.draw_networkx_labels(G_sub, pos, font_size=7, font_weight='bold')\n",
    "    \n",
    "    plt.title(f'{argument_name} - RECIPROCAL NETWORK\\nBidirectional conversations (Colors = Communities)', \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{argument_name.lower()}_reciprocal_network.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(f\"\\nüíæ Saved: {argument_name.lower()}_reciprocal_network.png\")\n",
    "\n",
    "def visualize_statistics(G_dir, metrics_dir, argument_name):\n",
    "    \"\"\"Compact statistical visualizations\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # 1. Degree distribution\n",
    "    in_degrees = [d for n, d in G_dir.in_degree()]\n",
    "    out_degrees = [d for n, d in G_dir.out_degree()]\n",
    "    \n",
    "    axes[0, 0].hist(in_degrees, bins=30, alpha=0.7, label='In-degree (replies received)', \n",
    "                   color=COLORS['primary'], edgecolor=COLORS['dark'])\n",
    "    axes[0, 0].hist(out_degrees, bins=30, alpha=0.7, label='Out-degree (replies given)', \n",
    "                   color=COLORS['secondary'], edgecolor=COLORS['dark'])\n",
    "    axes[0, 0].set_xlabel('Degree', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[0, 0].set_title('Degree Distribution', fontweight='bold', fontsize=12)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Weight distribution\n",
    "    weights = [d['weight'] for u, v, d in G_dir.edges(data=True)]\n",
    "    axes[0, 1].hist(weights, bins=30, color=COLORS['accent'], alpha=0.8, \n",
    "                   edgecolor=COLORS['dark'])\n",
    "    axes[0, 1].set_xlabel('Weight (# replies)', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[0, 1].set_title('Edge Weight Distribution', fontweight='bold', fontsize=12)\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Top users bar chart\n",
    "    top_pr = sorted(metrics_dir['pagerank'].items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "    users = [u for u, _ in top_pr]\n",
    "    scores = [s for _, s in top_pr]\n",
    "    \n",
    "    colors_gradient = [COLORS['primary'] if i < 5 else COLORS['secondary'] if i < 10 \n",
    "                      else COLORS['accent'] for i in range(len(users))]\n",
    "    \n",
    "    axes[1, 0].barh(users, scores, color=colors_gradient, alpha=0.8, edgecolor=COLORS['dark'])\n",
    "    axes[1, 0].set_xlabel('PageRank Score', fontweight='bold')\n",
    "    axes[1, 0].set_title('Top 15 Users by PageRank', fontweight='bold', fontsize=12)\n",
    "    axes[1, 0].invert_yaxis()\n",
    "    axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 4. Adjacency heatmap\n",
    "    top_20 = [u for u, _ in top_pr[:20]]\n",
    "    G_sub = G_dir.subgraph(top_20)\n",
    "    adj_matrix = nx.to_numpy_array(G_sub, nodelist=top_20, weight='weight')\n",
    "    \n",
    "    sns.heatmap(adj_matrix, xticklabels=top_20, yticklabels=top_20, \n",
    "                cmap='RdYlGn', ax=axes[1, 1], cbar_kws={'label': 'Replies'},\n",
    "                linewidths=0.5, linecolor=COLORS['light'])\n",
    "    axes[1, 1].set_title('Reply Heatmap (Top 20 users)', fontweight='bold', fontsize=12)\n",
    "    axes[1, 1].set_xlabel('Replies to', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Author', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'{argument_name} - Network Statistics', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{argument_name.lower()}_statistics.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(f\"\\nüíæ Saved: {argument_name.lower()}_statistics.png\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION FOR SINGLE ARGUMENT\n",
    "# ==========================================\n",
    "\n",
    "def analyze_single_argument(df, argument_name, min_weight_backbone=3, top_n_viz=200):\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline for ONE SINGLE argument\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame already filtered by argument\n",
    "    argument_name : 'bitcoin' or 'nvidia' or 'combined'\n",
    "    min_weight_backbone : threshold for backbone network\n",
    "    top_n_viz : number of nodes to visualize\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üöÄ NETWORK ANALYSIS: {argument_name.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Dataset: {len(df)} comments\")\n",
    "    \n",
    "    # Build networks\n",
    "    G_directed = build_directed_weighted_network(df, argument_name)\n",
    "    G_reciprocal = build_reciprocal_network(G_directed, argument_name)\n",
    "    G_backbone = build_backbone_network(G_directed, argument_name, min_weight=min_weight_backbone)\n",
    "    \n",
    "    # Analyze\n",
    "    metrics_dir = analyze_directed_network(G_directed, argument_name)\n",
    "    metrics_recip = analyze_reciprocal_network(G_reciprocal, argument_name)\n",
    "    metrics_backbone = analyze_backbone_network(G_backbone, argument_name)\n",
    "    \n",
    "    # Visualize\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üé® GENERATING VISUALIZATIONS - {argument_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    visualize_directed_network(G_directed, metrics_dir, argument_name, top_n=top_n_viz)\n",
    "    visualize_reciprocal_network(G_reciprocal, metrics_recip, argument_name, top_n=top_n_viz)\n",
    "    visualize_statistics(G_directed, metrics_dir, argument_name)\n",
    "    \n",
    "    print(f\"\\n‚úÖ ANALYSIS COMPLETED: {argument_name}\")\n",
    "    \n",
    "    return {\n",
    "        'networks': {\n",
    "            'directed': G_directed,\n",
    "            'reciprocal': G_reciprocal,\n",
    "            'backbone': G_backbone\n",
    "        },\n",
    "        'metrics': {\n",
    "            'directed': metrics_dir,\n",
    "            'reciprocal': metrics_recip,\n",
    "            'backbone': metrics_backbone\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# 5. COMPARATIVE ANALYSIS\n",
    "# ==========================================\n",
    "\n",
    "def compare_networks(results_bitcoin, results_nvidia, results_combined):\n",
    "    \"\"\"Compare metrics across Bitcoin and NVIDIA networks\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä COMPARATIVE NETWORK ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_data = {\n",
    "        'Metric': [],\n",
    "        'Bitcoin': [],\n",
    "        'NVIDIA': []\n",
    "    }\n",
    "    \n",
    "    # Network size metrics\n",
    "    comparison_data['Metric'].extend(['Nodes', 'Edges', 'Avg Degree', 'Density'])\n",
    "    \n",
    "    for name, results in [('Bitcoin', results_bitcoin), ('NVIDIA', results_nvidia)]:\n",
    "        G = results['networks']['directed']\n",
    "        comparison_data[name].append(G.number_of_nodes())\n",
    "        comparison_data[name].append(G.number_of_edges())\n",
    "        comparison_data[name].append(f\"{2*G.number_of_edges()/G.number_of_nodes():.2f}\" if G.number_of_nodes() > 0 else \"0\")\n",
    "        comparison_data[name].append(f\"{nx.density(G):.4f}\")\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\nüìà NETWORK SIZE COMPARISON:\")\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # 1. Network size comparison\n",
    "    metrics = ['Nodes', 'Edges']\n",
    "    btc_values = [results_bitcoin['networks']['directed'].number_of_nodes(),\n",
    "                  results_bitcoin['networks']['directed'].number_of_edges()]\n",
    "    nvda_values = [results_nvidia['networks']['directed'].number_of_nodes(),\n",
    "                   results_nvidia['networks']['directed'].number_of_edges()]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 0].bar(x - width/2, btc_values, width, label='Bitcoin', color=COLORS['primary'], alpha=0.8)\n",
    "    axes[0, 0].bar(x + width/2, nvda_values, width, label='NVIDIA', color=COLORS['secondary'], alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Metric', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Count', fontweight='bold')\n",
    "    axes[0, 0].set_title('Network Size Comparison', fontweight='bold', fontsize=12)\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(metrics)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Degree distribution comparison\n",
    "    btc_degrees = [d for n, d in results_bitcoin['networks']['directed'].degree()]\n",
    "    nvda_degrees = [d for n, d in results_nvidia['networks']['directed'].degree()]\n",
    "    \n",
    "    axes[0, 1].hist(btc_degrees, bins=30, alpha=0.6, label='Bitcoin', color=COLORS['primary'], edgecolor='black')\n",
    "    axes[0, 1].hist(nvda_degrees, bins=30, alpha=0.6, label='NVIDIA', color=COLORS['secondary'], edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Degree', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[0, 1].set_title('Degree Distribution Comparison', fontweight='bold', fontsize=12)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Top users PageRank comparison\n",
    "    btc_pr = results_bitcoin['metrics']['directed']['pagerank']\n",
    "    nvda_pr = results_nvidia['metrics']['directed']['pagerank']\n",
    "    \n",
    "    top_btc = sorted(btc_pr.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    top_nvda = sorted(nvda_pr.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    users_btc = [u[:15] for u, _ in top_btc]\n",
    "    scores_btc = [s for _, s in top_btc]\n",
    "    users_nvda = [u[:15] for u, _ in top_nvda]\n",
    "    scores_nvda = [s for _, s in top_nvda]\n",
    "    \n",
    "    y_btc = np.arange(len(users_btc))\n",
    "    y_nvda = np.arange(len(users_nvda))\n",
    "    \n",
    "    axes[1, 0].barh(y_btc, scores_btc, color=COLORS['primary'], alpha=0.8, edgecolor='black')\n",
    "    axes[1, 0].set_yticks(y_btc)\n",
    "    axes[1, 0].set_yticklabels(users_btc, fontsize=8)\n",
    "    axes[1, 0].set_xlabel('PageRank Score', fontweight='bold')\n",
    "    axes[1, 0].set_title('Top 10 Users - Bitcoin', fontweight='bold', fontsize=12)\n",
    "    axes[1, 0].invert_yaxis()\n",
    "    axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].barh(y_nvda, scores_nvda, color=COLORS['secondary'], alpha=0.8, edgecolor='black')\n",
    "    axes[1, 1].set_yticks(y_nvda)\n",
    "    axes[1, 1].set_yticklabels(users_nvda, fontsize=8)\n",
    "    axes[1, 1].set_xlabel('PageRank Score', fontweight='bold')\n",
    "    axes[1, 1].set_title('Top 10 Users - NVIDIA', fontweight='bold', fontsize=12)\n",
    "    axes[1, 1].invert_yaxis()\n",
    "    axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Comparative Network Analysis: Bitcoin vs NVIDIA', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparative_analysis.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(f\"\\nüíæ Saved: comparative_analysis.png\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. COMPLETE EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\\n\" + \"üåü\"*35)\n",
    "    print(\"  REDDIT NETWORK ANALYSIS BY ARGUMENT\")\n",
    "    print(\"üåü\"*35)\n",
    "    \n",
    "    # Load DataFrame (modify filename if needed)\n",
    "    df = pd.read_excel(\"threads_df.xlsx\")  # Excel file\n",
    "    # df = pd.read_csv(\"reddit_data.csv\")  # If CSV\n",
    "    \n",
    "    print(f\"\\nüìä Total dataset: {len(df)} comments\")\n",
    "    \n",
    "    # Check argument column\n",
    "    if 'argument' not in df.columns:\n",
    "        print(\"\\n‚ö†Ô∏è  Column 'argument' not found in DataFrame!\")\n",
    "        print(\"    Make sure the file contains the 'argument' column with values 'nvidia' and 'bitcoin'\")\n",
    "        exit()\n",
    "    \n",
    "    # Remove root comments (without parent) and self-replies\n",
    "    df_filtered = df[pd.notna(df['comment_parent_author'])].copy()\n",
    "    df_filtered = df_filtered[df_filtered['text_author'] != df_filtered['comment_parent_author']]\n",
    "    \n",
    "    print(f\"üìä Comments with valid interactions: {len(df_filtered)}\")\n",
    "    \n",
    "    # Show distribution\n",
    "    arg_counts = df_filtered['argument'].value_counts()\n",
    "    print(f\"\\nüìà Argument distribution:\")\n",
    "    for arg, count in arg_counts.items():\n",
    "        print(f\"   {arg}: {count} comments\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # BITCOIN ANALYSIS\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\\n\" + \"üü†\"*35)\n",
    "    print(\"  BITCOIN ANALYSIS\")\n",
    "    print(\"üü†\"*35)\n",
    "    \n",
    "    df_bitcoin = df_filtered[df_filtered['argument'].str.lower() == 'bitcoin'].copy()\n",
    "    \n",
    "    if len(df_bitcoin) == 0:\n",
    "        print(\"‚ö†Ô∏è  No comments found for Bitcoin!\")\n",
    "        results_bitcoin = None\n",
    "    else:\n",
    "        results_bitcoin = analyze_single_argument(\n",
    "            df=df_bitcoin,\n",
    "            argument_name='Bitcoin',\n",
    "            min_weight_backbone=2,\n",
    "            top_n_viz=200\n",
    "        )\n",
    "    \n",
    "    # ==========================================\n",
    "    # NVIDIA ANALYSIS\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\\n\" + \"üü¢\"*35)\n",
    "    print(\"  NVIDIA ANALYSIS\")\n",
    "    print(\"üü¢\"*35)\n",
    "    \n",
    "    df_nvidia = df_filtered[df_filtered['argument'].str.lower() == 'nvidia'].copy()\n",
    "    \n",
    "    if len(df_nvidia) == 0:\n",
    "        print(\"‚ö†Ô∏è  No comments found for NVIDIA!\")\n",
    "        results_nvidia = None\n",
    "    else:\n",
    "        results_nvidia = analyze_single_argument(\n",
    "            df=df_nvidia,\n",
    "            argument_name='NVIDIA',\n",
    "            min_weight_backbone=2,\n",
    "            top_n_viz=200\n",
    "        )\n",
    "    \n",
    "    # ==========================================\n",
    "    # COMBINED ANALYSIS (BITCOIN + NVIDIA)\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\\n\" + \"üü£\"*35)\n",
    "    print(\"  COMBINED ANALYSIS (BITCOIN + NVIDIA)\")\n",
    "    print(\"üü£\"*35)\n",
    "    \n",
    "    # Use all filtered data for combined analysis\n",
    "    if len(df_filtered) == 0:\n",
    "        print(\"‚ö†Ô∏è  No comments found for combined analysis!\")\n",
    "        results_combined = None\n",
    "    else:\n",
    "        results_combined = analyze_single_argument(\n",
    "            df=df_filtered,\n",
    "            argument_name='Combined',\n",
    "            min_weight_backbone=2,\n",
    "            top_n_viz=200\n",
    "        )\n",
    "    \n",
    "    # ==========================================\n",
    "    # COMPARATIVE ANALYSIS\n",
    "    # ==========================================\n",
    "    \n",
    "    if results_bitcoin and results_nvidia and results_combined:\n",
    "        print(\"\\n\\n\" + \"üìä\"*35)\n",
    "        print(\"  COMPARATIVE ANALYSIS\")\n",
    "        print(\"üìä\"*35)\n",
    "        compare_networks(results_bitcoin, results_nvidia, results_combined)\n",
    "    \n",
    "    # ==========================================\n",
    "    # FINAL SUMMARY\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"üéâ COMPLETE ANALYSIS FINISHED!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìÅ GENERATED FILES:\")\n",
    "    print(f\"\\n   üü† Bitcoin:\")\n",
    "    print(f\"      - bitcoin_directed_network.png\")\n",
    "    print(f\"      - bitcoin_reciprocal_network.png\")\n",
    "    print(f\"      - bitcoin_statistics.png\")\n",
    "    \n",
    "    print(f\"\\n   üü¢ NVIDIA:\")\n",
    "    print(f\"      - nvidia_directed_network.png\")\n",
    "    print(f\"      - nvidia_reciprocal_network.png\")\n",
    "    print(f\"      - nvidia_statistics.png\")\n",
    "    \n",
    "    print(f\"\\n   üü£ Combined:\")\n",
    "    print(f\"      - combined_directed_network.png\")\n",
    "    print(f\"      - combined_reciprocal_network.png\")\n",
    "    print(f\"      - combined_statistics.png\")\n",
    "    \n",
    "    print(f\"\\n   üìä Comparative:\")\n",
    "    print(f\"      - comparative_analysis.png\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ú® All analyses completed successfully! ‚ú®\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.spatial import ConvexHull\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verify and import required libraries\n",
    "try:\n",
    "    import igraph as ig\n",
    "    import leidenalg as la\n",
    "    LEIDEN_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    LEIDEN_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  ERROR: Missing libraries!\")\n",
    "    print(\"   Install with: pip install python-igraph leidenalg\")\n",
    "    print(f\"   Error details: {e}\")\n",
    "    print(\"\\n   Code cannot proceed without these libraries.\")\n",
    "    raise ImportError(\"Install python-igraph and leidenalg to continue\")\n",
    "\n",
    "# ==========================================\n",
    "# COMMUNITY DETECTION WITH LEIDEN\n",
    "# ==========================================\n",
    "\n",
    "def networkx_to_igraph(G_nx):\n",
    "    \"\"\"Convert NetworkX network to iGraph for Leiden\"\"\"\n",
    "    nodes = list(G_nx.nodes())\n",
    "    node_map = {node: idx for idx, node in enumerate(nodes)}\n",
    "    \n",
    "    edges = []\n",
    "    weights = []\n",
    "    \n",
    "    for u, v, data in G_nx.edges(data=True):\n",
    "        edges.append((node_map[u], node_map[v]))\n",
    "        weights.append(data.get('weight', 1))\n",
    "    \n",
    "    g = ig.Graph(directed=G_nx.is_directed())\n",
    "    g.add_vertices(len(nodes))\n",
    "    g.add_edges(edges)\n",
    "    g.es['weight'] = weights\n",
    "    g.vs['name'] = nodes\n",
    "    \n",
    "    return g, node_map, nodes\n",
    "\n",
    "def detect_communities_leiden(G, resolution=1.0):\n",
    "    \"\"\"\n",
    "    Apply Leiden algorithm for community detection\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G : NetworkX graph (directed or undirected)\n",
    "    resolution : float, resolution parameter (default 1.0)\n",
    "                 - lower values = larger communities\n",
    "                 - higher values = smaller communities\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç LEIDEN COMMUNITY DETECTION (resolution={resolution})\")\n",
    "    print(f\"   Network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Convert to undirected for community detection\n",
    "    if G.is_directed():\n",
    "        G_undirected = G.to_undirected()\n",
    "        print(\"   ‚Üí Converted to undirected for community detection\")\n",
    "    else:\n",
    "        G_undirected = G.copy()\n",
    "    \n",
    "    # Remove self-loops\n",
    "    G_undirected.remove_edges_from(nx.selfloop_edges(G_undirected))\n",
    "    \n",
    "    # Take only the largest connected component\n",
    "    largest_cc = max(nx.connected_components(G_undirected), key=len)\n",
    "    G_clean = G_undirected.subgraph(largest_cc).copy()\n",
    "    \n",
    "    print(f\"   ‚Üí Largest connected component: {G_clean.number_of_nodes()} nodes\")\n",
    "    \n",
    "    # Convert to iGraph\n",
    "    g_igraph, node_map, nodes = networkx_to_igraph(G_clean)\n",
    "    \n",
    "    # Apply Leiden\n",
    "    print(\"   ‚Üí Running Leiden algorithm...\")\n",
    "    partition = la.find_partition(\n",
    "        g_igraph,\n",
    "        la.RBConfigurationVertexPartition,\n",
    "        weights='weight',\n",
    "        resolution_parameter=resolution,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Extract communities\n",
    "    communities = {}\n",
    "    for comm_id, members in enumerate(partition):\n",
    "        for node_idx in members:\n",
    "            user = nodes[node_idx]\n",
    "            communities[user] = comm_id\n",
    "    \n",
    "    modularity = partition.modularity\n",
    "    n_communities = len(partition)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Community detection completed!\")\n",
    "    print(f\"   Communities found: {n_communities}\")\n",
    "    print(f\"   Modularity: {modularity:.4f}\")\n",
    "    \n",
    "    return communities, modularity, partition, G_clean\n",
    "\n",
    "# ==========================================\n",
    "# IMPROVED VISUALIZATIONS\n",
    "# ==========================================\n",
    "\n",
    "def get_community_layout(G, communities, top_n=200):\n",
    "    \"\"\"\n",
    "    Create layout where nodes of same community are grouped together\n",
    "    Uses a hierarchical approach: first place communities, then nodes within\n",
    "    \"\"\"\n",
    "    # Filter top nodes by degree\n",
    "    degrees = dict(G.degree(weight='weight'))\n",
    "    top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_nodes_set = set([n for n, _ in top_nodes])\n",
    "    \n",
    "    # Filter communities to include only top nodes\n",
    "    filtered_communities = {n: c for n, c in communities.items() if n in top_nodes_set}\n",
    "    \n",
    "    # Create subgraph\n",
    "    G_sub = G.subgraph(top_nodes_set).copy()\n",
    "    \n",
    "    # Group nodes by community\n",
    "    comm_nodes = {}\n",
    "    for node, comm in filtered_communities.items():\n",
    "        if comm not in comm_nodes:\n",
    "            comm_nodes[comm] = []\n",
    "        comm_nodes[comm].append(node)\n",
    "    \n",
    "    # Calculate community positions in a circle\n",
    "    n_communities = len(comm_nodes)\n",
    "    comm_positions = {}\n",
    "    \n",
    "    angle_step = 2 * np.pi / n_communities\n",
    "    radius = 10  # Distance from center for communities\n",
    "    \n",
    "    for i, comm_id in enumerate(sorted(comm_nodes.keys())):\n",
    "        angle = i * angle_step\n",
    "        comm_positions[comm_id] = (radius * np.cos(angle), radius * np.sin(angle))\n",
    "    \n",
    "    # Calculate node positions within each community\n",
    "    pos = {}\n",
    "    for comm_id, nodes in comm_nodes.items():\n",
    "        comm_center = comm_positions[comm_id]\n",
    "        \n",
    "        # Create subgraph for this community\n",
    "        G_comm = G_sub.subgraph(nodes).copy()\n",
    "        \n",
    "        # Use spring layout for internal structure, scaled down\n",
    "        if len(nodes) > 1:\n",
    "            pos_comm = nx.spring_layout(G_comm, k=0.5, iterations=50, scale=2, seed=42)\n",
    "        else:\n",
    "            pos_comm = {nodes[0]: (0, 0)}\n",
    "        \n",
    "        # Offset by community center\n",
    "        for node, (x, y) in pos_comm.items():\n",
    "            pos[node] = (comm_center[0] + x, comm_center[1] + y)\n",
    "    \n",
    "    return G_sub, pos, filtered_communities, comm_nodes\n",
    "\n",
    "def draw_community_hulls(ax, pos, comm_nodes, communities, colors):\n",
    "    \"\"\"Draw convex hulls around communities\"\"\"\n",
    "    for comm_id, nodes in comm_nodes.items():\n",
    "        if len(nodes) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Get positions for this community\n",
    "        points = np.array([pos[n] for n in nodes if n in pos])\n",
    "        \n",
    "        if len(points) < 3:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Calculate convex hull\n",
    "            hull = ConvexHull(points)\n",
    "            \n",
    "            # Get hull vertices\n",
    "            hull_points = points[hull.vertices]\n",
    "            \n",
    "            # Add first point at end to close the polygon\n",
    "            hull_points = np.vstack([hull_points, hull_points[0]])\n",
    "            \n",
    "            # Draw filled polygon\n",
    "            color = colors[comm_id % len(colors)]\n",
    "            ax.fill(hull_points[:, 0], hull_points[:, 1], \n",
    "                   color=color, alpha=0.2, zorder=0)\n",
    "            ax.plot(hull_points[:, 0], hull_points[:, 1], \n",
    "                   color=color, linewidth=2, alpha=0.6, zorder=1)\n",
    "        except:\n",
    "            # If hull fails, just skip\n",
    "            pass\n",
    "\n",
    "def visualize_leiden_communities(G, communities, argument_map=None, top_n=200, use_topic_colors=False):\n",
    "    \"\"\"\n",
    "    Visualize network with Leiden communities properly grouped\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    use_topic_colors : bool\n",
    "        If True and argument_map exists, color by dominant topic (Bitcoin/NVIDIA)\n",
    "        If False, use diverse colors for each community\n",
    "    \"\"\"\n",
    "    print(\"\\nüé® Generating Leiden community visualization...\")\n",
    "    \n",
    "    # Get community-based layout\n",
    "    G_sub, pos, filtered_communities, comm_nodes = get_community_layout(G, communities, top_n)\n",
    "    \n",
    "    print(f\"   Visualizing {G_sub.number_of_nodes()} nodes in {len(comm_nodes)} communities\")\n",
    "    \n",
    "    # Calculate dominant topic for each community\n",
    "    comm_dominant_topic = {}\n",
    "    if argument_map:\n",
    "        for comm_id, nodes in comm_nodes.items():\n",
    "            topics = [argument_map.get(n, 'Unknown') for n in nodes if n in argument_map]\n",
    "            if topics:\n",
    "                topic_counts = Counter(topics)\n",
    "                dominant = topic_counts.most_common(1)[0]\n",
    "                comm_dominant_topic[comm_id] = {\n",
    "                    'topic': dominant[0],\n",
    "                    'purity': dominant[1] / len(topics),\n",
    "                    'count': dominant[1],\n",
    "                    'total': len(topics)\n",
    "                }\n",
    "    \n",
    "    # Prepare colors\n",
    "    n_communities = len(comm_nodes)\n",
    "    colors = {}\n",
    "    \n",
    "    # Check if we have mixed topics (both Bitcoin and NVIDIA)\n",
    "    has_mixed_topics = False\n",
    "    if argument_map and comm_dominant_topic:\n",
    "        topics_present = set(info['topic'].lower() for info in comm_dominant_topic.values())\n",
    "        has_mixed_topics = ('bitcoin' in str(topics_present) and 'nvidia' in str(topics_present))\n",
    "    \n",
    "    # Use topic-based colors only if explicitly requested AND we have mixed topics\n",
    "    if use_topic_colors and argument_map and comm_dominant_topic and has_mixed_topics:\n",
    "        # Use different color schemes for Bitcoin vs NVIDIA communities\n",
    "        bitcoin_colors = plt.cm.Oranges(np.linspace(0.4, 0.9, n_communities))\n",
    "        nvidia_colors = plt.cm.Greens(np.linspace(0.4, 0.9, n_communities))\n",
    "        mixed_colors = plt.cm.Purples(np.linspace(0.4, 0.9, n_communities))\n",
    "        \n",
    "        bitcoin_idx = 0\n",
    "        nvidia_idx = 0\n",
    "        mixed_idx = 0\n",
    "        \n",
    "        for comm_id in sorted(comm_nodes.keys()):\n",
    "            if comm_id in comm_dominant_topic:\n",
    "                topic = comm_dominant_topic[comm_id]['topic'].lower()\n",
    "                purity = comm_dominant_topic[comm_id]['purity']\n",
    "                \n",
    "                if 'bitcoin' in topic and purity > 0.6:\n",
    "                    colors[comm_id] = bitcoin_colors[bitcoin_idx % len(bitcoin_colors)]\n",
    "                    bitcoin_idx += 1\n",
    "                elif 'nvidia' in topic and purity > 0.6:\n",
    "                    colors[comm_id] = nvidia_colors[nvidia_idx % len(nvidia_colors)]\n",
    "                    nvidia_idx += 1\n",
    "                else:\n",
    "                    colors[comm_id] = mixed_colors[mixed_idx % len(mixed_colors)]\n",
    "                    mixed_idx += 1\n",
    "            else:\n",
    "                colors[comm_id] = plt.cm.Greys(0.5)\n",
    "        \n",
    "        color_mode = 'topic'\n",
    "    else:\n",
    "        # Use diverse colors for each community\n",
    "        if n_communities <= 20:\n",
    "            color_array = plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "        else:\n",
    "            color_array = plt.cm.gist_rainbow(np.linspace(0, 1, n_communities))\n",
    "        \n",
    "        for i, comm_id in enumerate(sorted(comm_nodes.keys())):\n",
    "            colors[comm_id] = color_array[i % len(color_array)]\n",
    "        \n",
    "        color_mode = 'community'\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(20, 16))\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    # Draw community hulls first\n",
    "    draw_community_hulls(ax, pos, comm_nodes, filtered_communities, colors)\n",
    "    \n",
    "    # Draw edges\n",
    "    edge_list = list(G_sub.edges())\n",
    "    if edge_list:\n",
    "        weights = [G_sub[u][v].get('weight', 1) for u, v in edge_list]\n",
    "        max_weight = max(weights) if weights else 1\n",
    "        \n",
    "        for (u, v), weight in zip(edge_list, weights):\n",
    "            x = [pos[u][0], pos[v][0]]\n",
    "            y = [pos[u][1], pos[v][1]]\n",
    "            alpha = 0.1 + 0.3 * (weight / max_weight)\n",
    "            width = 0.3 + 1.0 * (weight / max_weight)\n",
    "            ax.plot(x, y, color='gray', alpha=alpha, linewidth=width, zorder=2)\n",
    "    \n",
    "    # Draw nodes\n",
    "    degrees = dict(G.degree(weight='weight'))\n",
    "    \n",
    "    for node in G_sub.nodes():\n",
    "        if node not in pos or node not in filtered_communities:\n",
    "            continue\n",
    "        \n",
    "        x, y = pos[node]\n",
    "        comm_id = filtered_communities[node]\n",
    "        color = colors[comm_id]\n",
    "        \n",
    "        # Node size based on degree\n",
    "        size = 50 + degrees.get(node, 0) * 3\n",
    "        \n",
    "        ax.scatter(x, y, s=size, c=[color], alpha=0.9, \n",
    "                  edgecolors='black', linewidths=1.5, zorder=3)\n",
    "    \n",
    "    # Draw labels for larger nodes only\n",
    "    top_degree_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "    for node, _ in top_degree_nodes:\n",
    "        if node in pos:\n",
    "            x, y = pos[node]\n",
    "            ax.text(x, y, node, fontsize=7, fontweight='bold',\n",
    "                   ha='center', va='center', zorder=4)\n",
    "    \n",
    "    # Create legend with topic information\n",
    "    comm_sizes = Counter(filtered_communities.values())\n",
    "    top_comms = sorted(comm_sizes.keys(), key=lambda c: comm_sizes[c], reverse=True)[:10]\n",
    "    \n",
    "    legend_elements = []\n",
    "    for comm_id in top_comms:\n",
    "        color = colors[comm_id]\n",
    "        size = comm_sizes[comm_id]\n",
    "        \n",
    "        # Add topic info if in topic color mode\n",
    "        if color_mode == 'topic' and comm_id in comm_dominant_topic:\n",
    "            topic_info = comm_dominant_topic[comm_id]\n",
    "            topic = topic_info['topic']\n",
    "            purity = topic_info['purity']\n",
    "            label = f'Community {comm_id}: {size} users - {topic} ({purity*100:.0f}% pure)'\n",
    "        else:\n",
    "            label = f'Community {comm_id} ({size} users)'\n",
    "        \n",
    "        legend_elements.append(\n",
    "            plt.Line2D([0], [0], marker='o', color='w',\n",
    "                      markerfacecolor=color, markersize=12,\n",
    "                      label=label,\n",
    "                      markeredgecolor='black', markeredgewidth=1.5)\n",
    "        )\n",
    "    \n",
    "    # Add topic color legend only if in topic color mode\n",
    "    if color_mode == 'topic':\n",
    "        legend_elements.append(plt.Line2D([0], [0], linestyle='none', label=''))  # Spacer\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='s', color='w',\n",
    "                              markerfacecolor='orange', markersize=12,\n",
    "                              label='Bitcoin-dominant (>60%)',\n",
    "                              markeredgecolor='black', markeredgewidth=1.5))\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='s', color='w',\n",
    "                              markerfacecolor='green', markersize=12,\n",
    "                              label='NVIDIA-dominant (>60%)',\n",
    "                              markeredgecolor='black', markeredgewidth=1.5))\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='s', color='w',\n",
    "                              markerfacecolor='purple', markersize=12,\n",
    "                              label='Mixed topics',\n",
    "                              markeredgecolor='black', markeredgewidth=1.5))\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=9,\n",
    "             title='Top 10 Communities', framealpha=0.95, \n",
    "             title_fontsize=11, edgecolor='black')\n",
    "    \n",
    "    # Update title\n",
    "    if color_mode == 'topic':\n",
    "        bitcoin_comms = sum(1 for c in comm_dominant_topic.values() \n",
    "                           if 'bitcoin' in c['topic'].lower() and c['purity'] > 0.6)\n",
    "        nvidia_comms = sum(1 for c in comm_dominant_topic.values() \n",
    "                          if 'nvidia' in c['topic'].lower() and c['purity'] > 0.6)\n",
    "        mixed_comms = n_communities - bitcoin_comms - nvidia_comms\n",
    "        \n",
    "        title_text = (f'LEIDEN COMMUNITY DETECTION\\n'\n",
    "                     f'{G_sub.number_of_nodes()} users grouped into {n_communities} communities\\n'\n",
    "                     f'üü† {bitcoin_comms} Bitcoin-dominant | üü¢ {nvidia_comms} NVIDIA-dominant | üü£ {mixed_comms} Mixed\\n'\n",
    "                     f'Node size = weighted degree | Colored areas = community boundaries')\n",
    "    else:\n",
    "        title_text = (f'LEIDEN COMMUNITY DETECTION\\n'\n",
    "                     f'{G_sub.number_of_nodes()} users grouped into {n_communities} communities\\n'\n",
    "                     f'Node size = weighted degree | Colored areas = community boundaries')\n",
    "    \n",
    "    ax.set_title(title_text, fontsize=18, fontweight='bold', pad=20)\n",
    "    \n",
    "    ax.axis('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('leiden_communities_grouped.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(\"üíæ Saved: leiden_communities_grouped.png\")\n",
    "\n",
    "def analyze_communities(G, communities, argument_map=None):\n",
    "    \"\"\"Analyze detected communities\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä COMMUNITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    comm_sizes = Counter(communities.values())\n",
    "    n_communities = len(comm_sizes)\n",
    "    \n",
    "    print(f\"\\nTotal communities: {n_communities}\")\n",
    "    print(f\"Largest community: {max(comm_sizes.values())} members\")\n",
    "    print(f\"Smallest community: {min(comm_sizes.values())} members\")\n",
    "    print(f\"Average members per community: {np.mean(list(comm_sizes.values())):.1f}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ TOP 10 COMMUNITIES BY SIZE:\")\n",
    "    for i, (comm_id, size) in enumerate(comm_sizes.most_common(10), 1):\n",
    "        members = [u for u, c in communities.items() if c == comm_id]\n",
    "        \n",
    "        G_comm = G.subgraph(members).copy()\n",
    "        n_edges = G_comm.number_of_edges()\n",
    "        density = nx.density(G_comm) if len(members) > 1 else 0\n",
    "        \n",
    "        if n_edges > 0:\n",
    "            degrees = dict(G_comm.degree())\n",
    "            top_members = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            top_names = [f\"u/{u}\" for u, d in top_members]\n",
    "        else:\n",
    "            top_names = [f\"u/{members[0]}\"] if members else []\n",
    "        \n",
    "        print(f\"\\n   {i}. Community {comm_id}: {size} members\")\n",
    "        print(f\"      Internal interactions: {n_edges}, Density: {density:.4f}\")\n",
    "        print(f\"      Top members: {', '.join(top_names)}\")\n",
    "        \n",
    "        if argument_map:\n",
    "            comm_arguments = [argument_map.get(u, 'Unknown') for u in members if u in argument_map]\n",
    "            if comm_arguments:\n",
    "                arg_dist = Counter(comm_arguments)\n",
    "                dominant_arg = arg_dist.most_common(1)[0]\n",
    "                print(f\"      Dominant topic: {dominant_arg[0]} ({dominant_arg[1]}/{len(comm_arguments)} users)\")\n",
    "    \n",
    "    return comm_sizes\n",
    "\n",
    "def visualize_community_stats(comm_sizes, communities, argument_map=None):\n",
    "    \"\"\"Dashboard with community statistics\"\"\"\n",
    "    print(\"\\nüé® Generating community statistics...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # 1. Community size distribution\n",
    "    sizes = list(comm_sizes.values())\n",
    "    axes[0, 0].hist(sizes, bins=min(30, len(set(sizes))), color='#4ECDC4', alpha=0.8, edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Community Size (# users)', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[0, 0].set_title('Community Size Distribution', fontweight='bold', fontsize=12)\n",
    "    if max(sizes) / min(sizes) > 10:\n",
    "        axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Top 15 communities bar chart\n",
    "    top_15 = comm_sizes.most_common(15)\n",
    "    comm_ids = [f'C{c}' for c, _ in top_15]\n",
    "    sizes_top = [s for _, s in top_15]\n",
    "    \n",
    "    colors_gradient = plt.cm.viridis(np.linspace(0.3, 0.9, len(sizes_top)))\n",
    "    axes[0, 1].barh(comm_ids, sizes_top, color=colors_gradient, alpha=0.8, edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Number of Users', fontweight='bold')\n",
    "    axes[0, 1].set_title('Top 15 Communities by Size', fontweight='bold', fontsize=12)\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 3. Topic purity (if available)\n",
    "    if argument_map:\n",
    "        community_purity = {}\n",
    "        \n",
    "        for comm_id in set(communities.values()):\n",
    "            members = [u for u, c in communities.items() if c == comm_id]\n",
    "            arguments = [argument_map.get(u, 'Unknown') for u in members if u in argument_map]\n",
    "            \n",
    "            if arguments:\n",
    "                arg_counts = Counter(arguments)\n",
    "                dominant = arg_counts.most_common(1)[0]\n",
    "                purity = dominant[1] / len(arguments)\n",
    "                community_purity[comm_id] = {\n",
    "                    'purity': purity,\n",
    "                    'dominant_arg': dominant[0],\n",
    "                    'size': len(members)\n",
    "                }\n",
    "        \n",
    "        top_comms = [c for c, _ in comm_sizes.most_common(20)]\n",
    "        purities = [community_purity.get(c, {}).get('purity', 0) for c in top_comms]\n",
    "        args = [community_purity.get(c, {}).get('dominant_arg', 'N/A') for c in top_comms]\n",
    "        \n",
    "        colors_by_arg = ['#FF6B6B' if 'bitcoin' in str(a).lower() else '#4ECDC4' if 'nvidia' in str(a).lower() else '#95E1D3' \n",
    "                        for a in args]\n",
    "        \n",
    "        axes[1, 0].barh([f'C{c}' for c in top_comms], purities,\n",
    "                       color=colors_by_arg, alpha=0.8, edgecolor='black')\n",
    "        axes[1, 0].set_xlabel('Topic Purity', fontweight='bold')\n",
    "        axes[1, 0].set_title('Community Purity (Top 20)', fontweight='bold', fontsize=12)\n",
    "        axes[1, 0].invert_yaxis()\n",
    "        axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "        axes[1, 0].set_xlim([0, 1])\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='#FF6B6B', label='Bitcoin-dominant'),\n",
    "            Patch(facecolor='#4ECDC4', label='NVIDIA-dominant'),\n",
    "            Patch(facecolor='#95E1D3', label='Mixed')\n",
    "        ]\n",
    "        axes[1, 0].legend(handles=legend_elements, loc='lower right')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'Topic data not available',\n",
    "                       ha='center', va='center', fontsize=12)\n",
    "        axes[1, 0].axis('off')\n",
    "    \n",
    "    # 4. Cumulative coverage\n",
    "    sorted_sizes = sorted(sizes, reverse=True)\n",
    "    cumsum = np.cumsum(sorted_sizes)\n",
    "    total_users = sum(comm_sizes.values())\n",
    "    cumsum_pct = cumsum / total_users * 100\n",
    "    \n",
    "    axes[1, 1].plot(range(1, len(cumsum_pct)+1), cumsum_pct,\n",
    "                   color='#FF6B6B', linewidth=3, marker='o', markersize=4)\n",
    "    axes[1, 1].axhline(y=80, color='gray', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "    axes[1, 1].set_xlabel('Number of Communities', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Cumulative % of Users', fontweight='bold')\n",
    "    axes[1, 1].set_title('Cumulative User Coverage', fontweight='bold', fontsize=12)\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].set_ylim([0, 105])\n",
    "    \n",
    "    plt.suptitle('LEIDEN COMMUNITY DETECTION - Statistics', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('leiden_statistics.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(\"üíæ Saved: leiden_statistics.png\")\n",
    "\n",
    "# ==========================================\n",
    "# MAIN PIPELINE\n",
    "# ==========================================\n",
    "\n",
    "def analyze_reddit_communities(df, argument_name='All', resolution=1.0, top_n_viz=200, use_topic_colors=False):\n",
    "    \"\"\"\n",
    "    Complete pipeline for community detection on Reddit data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    use_topic_colors : bool\n",
    "        If True, color communities by dominant topic (for combined analysis)\n",
    "        If False, use diverse colors for each community (for single-topic analysis)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üöÄ LEIDEN COMMUNITY DETECTION - {argument_name.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Dataset: {len(df)} comments\")\n",
    "    \n",
    "    # 1. Build network\n",
    "    print(\"\\nüìä Building network...\")\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        user = row['text_author']\n",
    "        parent = row['comment_parent_author']\n",
    "        \n",
    "        if pd.notna(parent) and user != parent:\n",
    "            if G.has_edge(user, parent):\n",
    "                G[user][parent]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(user, parent, weight=1)\n",
    "    \n",
    "    print(f\"   Network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # 2. Detect communities\n",
    "    communities, modularity, partition, G_clean = detect_communities_leiden(G, resolution=resolution)\n",
    "    \n",
    "    # 3. Analyze communities\n",
    "    if 'argument' in df.columns:\n",
    "        argument_map = df.groupby('text_author')['argument'].first().to_dict()\n",
    "    else:\n",
    "        argument_map = None\n",
    "    \n",
    "    comm_sizes = analyze_communities(G_clean, communities, argument_map)\n",
    "    \n",
    "    # 4. Visualize\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üé® GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    visualize_leiden_communities(G_clean, communities, argument_map, top_n=top_n_viz, use_topic_colors=use_topic_colors)\n",
    "    visualize_community_stats(comm_sizes, communities, argument_map)\n",
    "    \n",
    "    print(f\"\\n‚úÖ COMMUNITY DETECTION COMPLETED: {argument_name}\")\n",
    "    \n",
    "    return {\n",
    "        'network': G_clean,\n",
    "        'communities': communities,\n",
    "        'modularity': modularity,\n",
    "        'comm_sizes': comm_sizes,\n",
    "        'partition': partition\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\\n\" + \"üåü\"*35)\n",
    "    print(\"  REDDIT LEIDEN COMMUNITY DETECTION\")\n",
    "    print(\"üåü\"*35)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_excel(\"threads_df.xlsx\")\n",
    "    \n",
    "    print(f\"\\nüìä Total dataset: {len(df)} comments\")\n",
    "    \n",
    "    # Filter valid comments\n",
    "    df_filtered = df[pd.notna(df['comment_parent_author'])].copy()\n",
    "    df_filtered = df_filtered[df_filtered['text_author'] != df_filtered['comment_parent_author']]\n",
    "    \n",
    "    print(f\"üìä Comments with valid interactions: {len(df_filtered)}\")\n",
    "    \n",
    "    # Check if argument column exists\n",
    "    if 'argument' in df_filtered.columns:\n",
    "        arg_counts = df_filtered['argument'].value_counts()\n",
    "        print(f\"\\nüìà Argument distribution:\")\n",
    "        for arg, count in arg_counts.items():\n",
    "            print(f\"   {arg}: {count} comments\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. COMBINED ANALYSIS (ALL DATA)\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\\n\" + \"üü£\"*35)\n",
    "    print(\"  COMBINED ANALYSIS (BITCOIN + NVIDIA)\")\n",
    "    print(\"üü£\"*35)\n",
    "    \n",
    "    results_combined = analyze_reddit_communities(\n",
    "        df=df_filtered,\n",
    "        argument_name='Combined',\n",
    "        resolution=1.0,\n",
    "        top_n_viz=200,\n",
    "        use_topic_colors=True  # Use topic-based colors for combined\n",
    "    )\n",
    "    \n",
    "    # Rename output files for combined\n",
    "    import os\n",
    "    if os.path.exists('leiden_communities_grouped.png'):\n",
    "        if os.path.exists('combined_leiden_communities.png'):\n",
    "            os.remove('combined_leiden_communities.png')\n",
    "        os.rename('leiden_communities_grouped.png', 'combined_leiden_communities.png')\n",
    "    if os.path.exists('leiden_statistics.png'):\n",
    "        if os.path.exists('combined_leiden_statistics.png'):\n",
    "            os.remove('combined_leiden_statistics.png')\n",
    "        os.rename('leiden_statistics.png', 'combined_leiden_statistics.png')\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. BITCOIN ANALYSIS\n",
    "    # ==========================================\n",
    "    \n",
    "    if 'argument' in df_filtered.columns:\n",
    "        print(\"\\n\\n\" + \"üü†\"*35)\n",
    "        print(\"  BITCOIN ANALYSIS\")\n",
    "        print(\"üü†\"*35)\n",
    "        \n",
    "        df_bitcoin = df_filtered[df_filtered['argument'].str.lower() == 'bitcoin'].copy()\n",
    "        \n",
    "        if len(df_bitcoin) > 0:\n",
    "            results_bitcoin = analyze_reddit_communities(\n",
    "                df=df_bitcoin,\n",
    "                argument_name='Bitcoin',\n",
    "                resolution=1.0,\n",
    "                top_n_viz=200,\n",
    "                use_topic_colors=False  # Use diverse colors for single topic\n",
    "            )\n",
    "            \n",
    "            # Rename output files for bitcoin\n",
    "            if os.path.exists('leiden_communities_grouped.png'):\n",
    "                if os.path.exists('bitcoin_leiden_communities.png'):\n",
    "                    os.remove('bitcoin_leiden_communities.png')\n",
    "                os.rename('leiden_communities_grouped.png', 'bitcoin_leiden_communities.png')\n",
    "            if os.path.exists('leiden_statistics.png'):\n",
    "                if os.path.exists('bitcoin_leiden_statistics.png'):\n",
    "                    os.remove('bitcoin_leiden_statistics.png')\n",
    "                os.rename('leiden_statistics.png', 'bitcoin_leiden_statistics.png')\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No comments found for Bitcoin!\")\n",
    "        \n",
    "        # ==========================================\n",
    "        # 3. NVIDIA ANALYSIS\n",
    "        # ==========================================\n",
    "        \n",
    "        print(\"\\n\\n\" + \"üü¢\"*35)\n",
    "        print(\"  NVIDIA ANALYSIS\")\n",
    "        print(\"üü¢\"*35)\n",
    "        \n",
    "        df_nvidia = df_filtered[df_filtered['argument'].str.lower() == 'nvidia'].copy()\n",
    "        \n",
    "        if len(df_nvidia) > 0:\n",
    "            results_nvidia = analyze_reddit_communities(\n",
    "                df=df_nvidia,\n",
    "                argument_name='NVIDIA',\n",
    "                resolution=1.0,\n",
    "                top_n_viz=200,\n",
    "                use_topic_colors=False  # Use diverse colors for single topic\n",
    "            )\n",
    "            \n",
    "            # Rename output files for nvidia\n",
    "            if os.path.exists('leiden_communities_grouped.png'):\n",
    "                if os.path.exists('nvidia_leiden_communities.png'):\n",
    "                    os.remove('nvidia_leiden_communities.png')\n",
    "                os.rename('leiden_communities_grouped.png', 'nvidia_leiden_communities.png')\n",
    "            if os.path.exists('leiden_statistics.png'):\n",
    "                if os.path.exists('nvidia_leiden_statistics.png'):\n",
    "                    os.remove('nvidia_leiden_statistics.png')\n",
    "                os.rename('leiden_statistics.png', 'nvidia_leiden_statistics.png')\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No comments found for NVIDIA!\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # FINAL SUMMARY\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"üéâ COMPLETE ANALYSIS FINISHED!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìÅ GENERATED FILES:\")\n",
    "    \n",
    "    print(f\"\\n   üü£ Combined (Bitcoin + NVIDIA):\")\n",
    "    print(f\"      - combined_leiden_communities.png\")\n",
    "    print(f\"      - combined_leiden_statistics.png\")\n",
    "    \n",
    "    if 'argument' in df_filtered.columns:\n",
    "        print(f\"\\n   üü† Bitcoin:\")\n",
    "        print(f\"      - bitcoin_leiden_communities.png\")\n",
    "        print(f\"      - bitcoin_leiden_statistics.png\")\n",
    "        \n",
    "        print(f\"\\n   üü¢ NVIDIA:\")\n",
    "        print(f\"      - nvidia_leiden_communities.png\")\n",
    "        print(f\"      - nvidia_leiden_statistics.png\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ú® All analyses completed successfully! ‚ú®\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfcf4b7",
   "metadata": {},
   "source": [
    "# Threads and Tweets df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_excel('tweets_df.xlsx')\n",
    "threads_df = pd.read_excel('threads_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053abfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnt_df = pd.concat([tweets_df, threads_df], ignore_index=True)\n",
    "tnt_df.to_excel('tnt_df.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74382f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0b85c",
   "metadata": {},
   "source": [
    "## Text cleaning, lemmatization , vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec22ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnt_df = pd.read_excel(\"tnt_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REQUIRED LIBRARIES\n",
    "# ============================================================================\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download NLTK stopwords\n",
    "try:\n",
    "    nltk_stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. TEXT CLEANING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text(text, remove_hashtag_symbol=True, remove_cashtag_before_words=True, \n",
    "               remove_emojis=True, keep_dollar_numbers=True, lowercase=True):\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"(f|ht)(tp)(s?)(://)(.*?)([\\s]|$)\", \" \", text)\n",
    "    \n",
    "    # Remove RT patterns\n",
    "    text = re.sub(r\"(RT|rt|via)((?:\\b\\W*@\\w+)+)\", \" \", text)\n",
    "    \n",
    "    # Remove HTML entities\n",
    "    html_entities = [\"&copy;\", \"&reg;\", \"&trade;\", \"&ldquo;\", \"&lsquo;\", \"&rsquo;\", \n",
    "                     \"&bull;\", \"&middot;\", \"&ndash;\", \"&mdash;\", \"&nbsp;\", \"&lt;\", \n",
    "                     \"&gt;\", \"&amp;\", \"&quot;\"]\n",
    "    for entity in html_entities:\n",
    "        text = text.replace(entity, \" \")\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r\"@\\S+\", \" \", text)\n",
    "    \n",
    "    # Remove emojis (including flag emojis like üáÆüá≥)\n",
    "    if remove_emojis:\n",
    "        # Remove emoji characters (comprehensive pattern)\n",
    "        text = re.sub(r\"[^\\w\\s,.\\'!?-]\", \"\", text)\n",
    "    \n",
    "    # Handle $ symbol:\n",
    "    # Remove $ only before letters (not before numbers)\n",
    "    if remove_cashtag_before_words:\n",
    "        text = re.sub(r'\\$(?=[A-Za-z])', '', text)\n",
    "    \n",
    "    # Handle # symbol:\n",
    "    # Remove '#' but keep the word after it\n",
    "    if remove_hashtag_symbol:\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Lowercase if requested\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. SPACY UTILITIES & COLLOCATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    \"\"\"Load spaCy model\"\"\"\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "    except OSError:\n",
    "        raise FileNotFoundError(f\"Cannot load spaCy model: {model_name}. \"\n",
    "                                f\"Run: python -m spacy download {model_name}\")\n",
    "    return nlp\n",
    "\n",
    "\n",
    "def annotate_texts(nlp, texts, show_progress=True):\n",
    "    \"\"\"Annotate list of texts with spaCy\"\"\"\n",
    "    out = []\n",
    "    iterable = tqdm(texts, desc=\"Annotating\") if show_progress else texts\n",
    "    \n",
    "    for text in iterable:\n",
    "        doc = nlp(str(text) if pd.notna(text) else \"\")\n",
    "        tokens = [{\n",
    "            'form': token.text,\n",
    "            'lemma': token.lemma_,\n",
    "            'upos': token.pos_\n",
    "        } for token in doc]\n",
    "        out.append(tokens)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_collocations_POS(texts, nlp, pos_patterns=[('ADJ','NOUN'), ('NOUN','NOUN'), \n",
    "                                                        ('NOUN','PROPN'), ('PROPN','PROPN')], \n",
    "                             min_freq=2, save_file=\"colloc_POS.xlsx\", verbose=True):\n",
    "    \n",
    "    annotated = annotate_texts(nlp, texts, show_progress=verbose)\n",
    "    counts = Counter()\n",
    "    pattern_map = {}\n",
    "    \n",
    "    for doc in annotated:\n",
    "        for i in range(len(doc)-1):\n",
    "            t1, t2 = doc[i], doc[i+1]\n",
    "            pattern = (t1['upos'], t2['upos'])\n",
    "            \n",
    "            if pattern in pos_patterns:\n",
    "                w1 = (t1['lemma'] if t1['lemma'] != '_' else t1['form']).lower()\n",
    "                w2 = (t2['lemma'] if t2['lemma'] != '_' else t2['form']).lower()\n",
    "                colloc = f\"{w1} {w2}\"\n",
    "                counts[colloc] += 1\n",
    "                pattern_map[colloc] = f\"{pattern[0]} {pattern[1]}\"\n",
    "    \n",
    "    rows = [{'collocation': c, 'freq': f, 'pos_pattern': pattern_map[c]}\n",
    "            for c, f in counts.items() if f >= min_freq]\n",
    "    \n",
    "    df = pd.DataFrame(rows).sort_values('freq', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    if save_file:\n",
    "        df.to_excel(save_file, index=False)\n",
    "        if verbose: \n",
    "            print(f\"‚úì Saved {save_file} ({len(df)} collocations)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_collocations_PMI(texts, nlp, top_n=200, min_freq=2, \n",
    "                             save_file=\"colloc_PMI.xlsx\", verbose=True):\n",
    "    \n",
    "    annotated = annotate_texts(nlp, texts, show_progress=verbose)\n",
    "    unigram = Counter()\n",
    "    bigram = Counter()\n",
    "    total_unigrams = 0\n",
    "    \n",
    "    for doc in annotated:\n",
    "        lemmas = []\n",
    "        for tok in doc:\n",
    "            if tok['upos'] == 'PUNCT':\n",
    "                continue\n",
    "            lemma = (tok['lemma'] if tok['lemma'] != '_' else tok['form']).lower()\n",
    "            lemmas.append(lemma)\n",
    "            unigram[lemma] += 1\n",
    "            total_unigrams += 1\n",
    "        \n",
    "        for i in range(len(lemmas)-1):\n",
    "            bigram[f\"{lemmas[i]} {lemmas[i+1]}\"] += 1\n",
    "    \n",
    "    N = max(total_unigrams, 1)\n",
    "    rows = []\n",
    "    \n",
    "    for big, freq in bigram.items():\n",
    "        if freq < min_freq:\n",
    "            continue\n",
    "        \n",
    "        w1, w2 = big.split(\" \", 1)\n",
    "        p_w1 = unigram[w1] / N\n",
    "        p_w2 = unigram[w2] / N\n",
    "        p_w1w2 = freq / max(1, N-1)\n",
    "        \n",
    "        if p_w1 > 0 and p_w2 > 0 and p_w1w2 > 0:\n",
    "            pmi = math.log2(p_w1w2 / (p_w1 * p_w2))\n",
    "        else:\n",
    "            pmi = float('-inf')\n",
    "        \n",
    "        rows.append({'collocation': big, 'freq': freq, 'pmi': pmi})\n",
    "    \n",
    "    df = pd.DataFrame(rows).sort_values(['pmi', 'freq'], ascending=[False, False])\n",
    "    \n",
    "    if top_n:\n",
    "        df = df.head(top_n)\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    if save_file:\n",
    "        df.to_excel(save_file, index=False)\n",
    "        if verbose:\n",
    "            print(f\"‚úì Saved {save_file} ({len(df)} collocations)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_collocations(texts, colloc_file, verbose=True):\n",
    "    \"\"\"Replace multi-word collocations with underscores\"\"\"\n",
    "    \n",
    "    if not os.path.exists(colloc_file):\n",
    "        if verbose:\n",
    "            print(f\"‚ö† File not found: {colloc_file}. Skipping.\")\n",
    "        return texts\n",
    "    \n",
    "    df = pd.read_excel(colloc_file)\n",
    "    collocations = sorted(df['collocation'].dropna().unique(), \n",
    "                         key=lambda s: len(s.split()), reverse=True)\n",
    "    \n",
    "    patterns = []\n",
    "    for colloc in collocations:\n",
    "        escaped = r'\\s+'.join(re.escape(word) for word in colloc.split())\n",
    "        pattern = re.compile(rf'\\b{escaped}\\b', flags=re.IGNORECASE)\n",
    "        replacement = \"_\".join(colloc.split())\n",
    "        patterns.append((pattern, replacement))\n",
    "    \n",
    "    result = []\n",
    "    iterator = tqdm(texts, desc=f\"Applying {os.path.basename(colloc_file)}\") if verbose else texts\n",
    "    \n",
    "    for text in iterator:\n",
    "        if pd.isna(text):\n",
    "            result.append(text)\n",
    "            continue\n",
    "        \n",
    "        text_str = str(text)\n",
    "        for pattern, repl in patterns:\n",
    "            text_str = pattern.sub(repl, text_str)\n",
    "        result.append(text_str)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def apply_collocation_pipeline(df, text_col='text_cleaned', nlp=None, verbose=True):\n",
    "    \"\"\"Apply complete collocation extraction and substitution pipeline\"\"\"\n",
    "    \n",
    "    if nlp is None:\n",
    "        nlp = load_spacy_model()\n",
    "    \n",
    "    # Step 1: POS collocations\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 1: POS-based collocations\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    pos_df = extract_collocations_POS(df[text_col], nlp, min_freq=1, verbose=verbose)\n",
    "    df[text_col] = apply_collocations(df[text_col], \"colloc_POS.xlsx\", verbose=verbose)\n",
    "    \n",
    "    # Step 2: PMI collocations\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 2: PMI-based collocations\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    pmi_df = extract_collocations_PMI(df[text_col], nlp, top_n=200, min_freq=2, verbose=verbose)\n",
    "    df[text_col] = apply_collocations(df[text_col], \"colloc_PMI.xlsx\", verbose=verbose)\n",
    "    \n",
    "    return df, pos_df, pmi_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. LEMMATIZATION WITH STOPWORDS\n",
    "# ============================================================================\n",
    "\n",
    "def lemmatize_texts(texts, nlp=None, stopwords_list=None, verbose=True):\n",
    "    \"\"\"Lemmatize texts and identify stopwords\"\"\"\n",
    "    \n",
    "    if nlp is None:\n",
    "        nlp = load_spacy_model()\n",
    "    \n",
    "    if stopwords_list is None:\n",
    "        stopwords_list = list(nltk_stopwords.words('english'))\n",
    "    \n",
    "    stopwords_lower = set(w.lower() for w in stopwords_list)\n",
    "    \n",
    "    results = []\n",
    "    iterator = tqdm(enumerate(texts), total=len(texts), desc=\"Lemmatizing\") if verbose else enumerate(texts)\n",
    "    \n",
    "    for doc_idx, text in iterator:\n",
    "        doc = nlp(str(text) if pd.notna(text) else \"\")\n",
    "        doc_id = f\"doc_{doc_idx}\"\n",
    "        \n",
    "        for token_idx, token in enumerate(doc):\n",
    "            if token.is_punct or token.is_space:\n",
    "                continue\n",
    "            \n",
    "            is_stopword = (token.text.lower() in stopwords_lower or \n",
    "                          token.lemma_.lower() in stopwords_lower)\n",
    "            \n",
    "            results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'token_id': token_idx + 1,\n",
    "                'token': token.text,\n",
    "                'lemma': token.lemma_,\n",
    "                'upos': token.pos_,\n",
    "                'STOP': is_stopword\n",
    "            })\n",
    "    \n",
    "    df_lem = pd.DataFrame(results)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n‚úì Lemmatization complete: {len(df_lem)} tokens from {len(texts)} documents\")\n",
    "        print(f\"  Stopwords: {df_lem['STOP'].sum()}\")\n",
    "        print(f\"  Content words: {(~df_lem['STOP']).sum()}\")\n",
    "    \n",
    "    return df_lem\n",
    "\n",
    "\n",
    "def create_text_withstop(df, lemmatized_df, verbose=True):\n",
    "    \"\"\"Create text_lemmatized column WITH stopwords (all tokens)\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nCreating text_lemmatized column (with stopwords)...\")\n",
    "    \n",
    "    def reconstruct_withstop(doc_idx):\n",
    "        doc_id = f\"doc_{doc_idx}\"\n",
    "        content_lemmas = lemmatized_df[lemmatized_df['doc_id'] == doc_id]['lemma'].tolist()\n",
    "        return ' '.join(content_lemmas)\n",
    "    \n",
    "    df['text_lemmatized'] = [reconstruct_withstop(i) for i in range(len(df))]\n",
    "    \n",
    "    if verbose:\n",
    "        avg_tokens = df['text_lemmatized'].str.split().str.len().mean()\n",
    "        print(f\"‚úì Created text_lemmatized column\")\n",
    "        print(f\"  Avg tokens: {avg_tokens:.1f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_text_nostop(df, lemmatized_df, verbose=True):\n",
    "    \"\"\"Create text_nostop column WITHOUT stopwords (content words only)\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nCreating text_nostop column (without stopwords)...\")\n",
    "    \n",
    "    def reconstruct_nostop(doc_idx):\n",
    "        doc_id = f\"doc_{doc_idx}\"\n",
    "        # Filter only content words (STOP == False)\n",
    "        content_lemmas = lemmatized_df[\n",
    "            (lemmatized_df['doc_id'] == doc_id) & \n",
    "            (~lemmatized_df['STOP'])\n",
    "        ]['lemma'].tolist()\n",
    "        return ' '.join(content_lemmas)\n",
    "    \n",
    "    df['text_nostop'] = [reconstruct_nostop(i) for i in range(len(df))]\n",
    "    \n",
    "    if verbose:\n",
    "        avg_tokens = df['text_nostop'].str.split().str.len().mean()\n",
    "        print(f\"‚úì Created text_nostop column\")\n",
    "        print(f\"  Avg tokens: {avg_tokens:.1f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. TERM-DOCUMENT MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "def create_document_term_matrix(texts, save_file=\"document_term_matrix.csv\", verbose=True):\n",
    "    \n",
    "    vectorizer = CountVectorizer(\n",
    "        lowercase=True,\n",
    "        token_pattern=r'(?u)\\b\\w+\\b',\n",
    "        min_df=1\n",
    "    )\n",
    "    \n",
    "    dtm_sparse = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Transpose: rows=terms, columns=documents\n",
    "    dtm_df = pd.DataFrame(\n",
    "        dtm_sparse.toarray().T,\n",
    "        index=vectorizer.get_feature_names_out(),\n",
    "        columns=[f'doc_{i}' for i in range(len(texts))]\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        sparsity = (dtm_sparse.nnz / (dtm_sparse.shape[0] * dtm_sparse.shape[1]) * 100)\n",
    "        print(f\"\\n‚úì DTM created: {dtm_df.shape[0]} terms √ó {dtm_df.shape[1]} documents\")\n",
    "        print(f\"  Sparsity: {sparsity:.2f}%\")\n",
    "    \n",
    "    if save_file:\n",
    "        dtm_df.to_csv(save_file)\n",
    "        if verbose:\n",
    "            print(f\"  Saved to: {save_file}\")\n",
    "    \n",
    "    return dtm_df, vectorizer\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def process_tweets(tnt_df, verbose=True):\n",
    "    \"\"\"Complete processing pipeline for tweet data\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NLP TEXT PROCESSING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df = tnt_df.copy()\n",
    "    \n",
    "    # STEP 1: Text Cleaning\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: Text Cleaning\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df['text_cleaned'] = df['text'].apply(\n",
    "        lambda x: clean_text(x, remove_hashtag_symbol=True, remove_cashtag_before_words=True, \n",
    "               remove_emojis=True, keep_dollar_numbers=True, lowercase=True)\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n‚úì Cleaned {len(df)} documents\")\n",
    "        print(f\"  Avg length before: {df['text'].str.len().mean():.0f} chars\")\n",
    "        print(f\"  Avg length after: {df['text_cleaned'].str.len().mean():.0f} chars\")\n",
    "    \n",
    "    # STEP 2: Collocations\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: Collocations\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    nlp = load_spacy_model()\n",
    "    df, pos_df, pmi_df = apply_collocation_pipeline(df, text_col='text_cleaned', nlp=nlp, verbose=verbose)\n",
    "    \n",
    "    # STEP 3: Lemmatization\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: Lemmatization\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    lemmatized_df = lemmatize_texts(df['text_cleaned'].tolist(), nlp=nlp, verbose=verbose)\n",
    "    lemmatized_df.to_csv('lemmatized_tokens.csv', index=False)\n",
    "    if verbose:\n",
    "        print(\"‚úì Saved: lemmatized_tokens.csv\")\n",
    "    \n",
    "    # STEP 4: Create text columns (with and without stopwords)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 4: Creating Text Columns\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df = create_text_withstop(df, lemmatized_df, verbose=verbose)\n",
    "    df = create_text_nostop(df, lemmatized_df, verbose=verbose)\n",
    "    \n",
    "    # STEP 5: Document-Term Matrix (using text_nostop)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 5: Document-Term Matrix\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    dtm_df, vectorizer = create_document_term_matrix(df['text_nostop'].tolist(), verbose=verbose)\n",
    "    \n",
    "    # Summary\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETE!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"  Documents processed: {len(df)}\")\n",
    "        print(f\"  Total tokens: {len(lemmatized_df)}\")\n",
    "        print(f\"  Content words: {(~lemmatized_df['STOP']).sum()}\")\n",
    "        print(f\"  POS collocations: {len(pos_df)}\")\n",
    "        print(f\"  PMI collocations: {len(pmi_df)}\")\n",
    "        print(f\"  Vocabulary size: {dtm_df.shape[0]}\")\n",
    "        \n",
    "        print(f\"\\nüíæ Files created:\")\n",
    "        print(f\"  ‚úì colloc_POS.xlsx\")\n",
    "        print(f\"  ‚úì colloc_PMI.xlsx\")\n",
    "        print(f\"  ‚úì lemmatized_tokens.csv\")\n",
    "        print(f\"  ‚úì document_term_matrix.csv\")\n",
    "        \n",
    "        print(f\"\\nüìà Dataframe columns:\")\n",
    "        print(f\"  ‚Ä¢ text_cleaned: cleaned text\")\n",
    "        print(f\"  ‚Ä¢ text_lemmatized: lemmatized with stopwords\")\n",
    "        print(f\"  ‚Ä¢ text_nostop: lemmatized without stopwords\")\n",
    "        \n",
    "        print(f\"\\nüìà Top 10 content words:\")\n",
    "        top_words = lemmatized_df[~lemmatized_df['STOP']]['lemma'].value_counts().head(10)\n",
    "        for word, freq in top_words.items():\n",
    "            print(f\"  ‚Ä¢ {word}: {freq}\")\n",
    "    \n",
    "    return df, lemmatized_df, dtm_df, pos_df, pmi_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RUN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming tnt_df is already loaded from your scraping script\n",
    "    df_processed, lemmatized_tokens, dtm, pos_collocations, pmi_collocations = process_tweets(tnt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_excel('df_processed.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d66223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.read_excel('df_processed.xlsx')\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f250f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7583f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from nrclex import NRCLex\n",
    "\n",
    "# ==========================================\n",
    "# 1. ANALISI SENTIMENT\n",
    "# ==========================================\n",
    "\n",
    "def analyze_vader(df, text_col='text_lemmatized'):\n",
    "    \"\"\"VADER: Ottimizzato per social media\"\"\"\n",
    "    print(\"\\nüîµ VADER Sentiment Analysis...\")\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    results = []\n",
    "    for text in tqdm(df[text_col], desc=\"VADER\"):\n",
    "        scores = analyzer.polarity_scores(str(text))\n",
    "        results.append({\n",
    "            'vader_compound': scores['compound'],\n",
    "            'vader_pos': scores['pos'],\n",
    "            'vader_neu': scores['neu'],\n",
    "            'vader_neg': scores['neg'],\n",
    "            'vader_label': 'positive' if scores['compound'] >= 0.05 \n",
    "                          else 'negative' if scores['compound'] <= -0.05 \n",
    "                          else 'neutral'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_textblob(df, text_col='text_lemmatized'):\n",
    "    \"\"\"TextBlob: Include subjectivity\"\"\"\n",
    "    print(\"\\nüîµ TextBlob Sentiment Analysis...\")\n",
    "    \n",
    "    results = []\n",
    "    for text in tqdm(df[text_col], desc=\"TextBlob\"):\n",
    "        try:\n",
    "            blob = TextBlob(str(text))\n",
    "            results.append({\n",
    "                'textblob_polarity': blob.sentiment.polarity,\n",
    "                'textblob_subjectivity': blob.sentiment.subjectivity,\n",
    "                'textblob_label': 'positive' if blob.sentiment.polarity > 0.1\n",
    "                                 else 'negative' if blob.sentiment.polarity < -0.1\n",
    "                                 else 'neutral'\n",
    "            })\n",
    "        except:\n",
    "            results.append({\n",
    "                'textblob_polarity': 0,\n",
    "                'textblob_subjectivity': 0,\n",
    "                'textblob_label': 'neutral'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_nrclex(df, text_col='text_lemmatized'):\n",
    "    \"\"\"NRCLex: 8 emozioni base\"\"\"\n",
    "    print(\"\\nüòä NRCLex Emotion Analysis...\")\n",
    "    \n",
    "    results = []\n",
    "    for text in tqdm(df[text_col], desc=\"NRCLex\"):\n",
    "        try:\n",
    "            emotion = NRCLex(str(text))\n",
    "            freq = emotion.affect_frequencies\n",
    "            \n",
    "            results.append({\n",
    "                'nrc_fear': freq.get('fear', 0),\n",
    "                'nrc_anger': freq.get('anger', 0),\n",
    "                'nrc_anticipation': freq.get('anticipation', 0),\n",
    "                'nrc_trust': freq.get('trust', 0),\n",
    "                'nrc_surprise': freq.get('surprise', 0),\n",
    "                'nrc_sadness': freq.get('sadness', 0),\n",
    "                'nrc_joy': freq.get('joy', 0),\n",
    "                'nrc_disgust': freq.get('disgust', 0),\n",
    "                'nrc_positive': freq.get('positive', 0),\n",
    "                'nrc_negative': freq.get('negative', 0),\n",
    "                'nrc_dominant_emotion': max(freq.items(), key=lambda x: x[1])[0] if freq else 'neutral'\n",
    "            })\n",
    "        except:\n",
    "            results.append({\n",
    "                'nrc_fear': 0, 'nrc_anger': 0, 'nrc_anticipation': 0,\n",
    "                'nrc_trust': 0, 'nrc_surprise': 0, 'nrc_sadness': 0,\n",
    "                'nrc_joy': 0, 'nrc_disgust': 0, 'nrc_positive': 0,\n",
    "                'nrc_negative': 0, 'nrc_dominant_emotion': 'neutral'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ==========================================\n",
    "# 2. AGGREGAZIONE\n",
    "# ==========================================\n",
    "\n",
    "def aggregate_sentiments(df):\n",
    "    \"\"\"Crea sentiment ensemble\"\"\"\n",
    "    print(\"\\nüìä Aggregazione sentiment scores...\")\n",
    "    \n",
    "    # Ensemble polarity (media di VADER e TextBlob)\n",
    "    df['polarity_ensemble'] = (df['vader_compound'] + df['textblob_polarity']) / 2\n",
    "    \n",
    "    # Majority vote\n",
    "    def majority_vote(row):\n",
    "        labels = [row['vader_label'], row['textblob_label']]\n",
    "        return max(set(labels), key=labels.count)\n",
    "    \n",
    "    df['sentiment_ensemble'] = df.apply(majority_vote, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# 3. VISUALIZZAZIONI\n",
    "# ==========================================\n",
    "\n",
    "def plot_sentiment_distribution(df, title=\"Sentiment Distribution\"):\n",
    "    \"\"\"Visualizza distribuzioni sentiment\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # VADER\n",
    "    axes[0, 0].hist(df['vader_compound'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 0].set_title('VADER Compound Score')\n",
    "    axes[0, 0].set_xlabel('Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # TextBlob\n",
    "    axes[0, 1].hist(df['textblob_polarity'], bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 1].set_title('TextBlob Polarity')\n",
    "    axes[0, 1].set_xlabel('Score')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Ensemble\n",
    "    axes[1, 0].hist(df['polarity_ensemble'], bins=50, color='mediumseagreen', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1, 0].set_title('Ensemble Polarity')\n",
    "    axes[1, 0].set_xlabel('Score')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Sentiment labels\n",
    "    sentiment_counts = df['sentiment_ensemble'].value_counts()\n",
    "    colors = {'positive': 'green', 'neutral': 'gray', 'negative': 'red'}\n",
    "    axes[1, 1].bar(sentiment_counts.index, sentiment_counts.values, \n",
    "                   color=[colors.get(x, 'gray') for x in sentiment_counts.index], alpha=0.7)\n",
    "    axes[1, 1].set_title('Sentiment Labels Distribution')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\").replace(\"/\", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_emotions_distribution(df, title=\"Emotions Distribution\"):\n",
    "    \"\"\"Visualizza distribuzioni emozioni\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # NRC Emotions Media\n",
    "    nrc_cols = ['nrc_fear', 'nrc_anger', 'nrc_anticipation', 'nrc_trust', \n",
    "                'nrc_surprise', 'nrc_sadness', 'nrc_joy', 'nrc_disgust']\n",
    "    nrc_means = df[nrc_cols].mean().sort_values(ascending=False)\n",
    "    \n",
    "    axes[0].barh(nrc_means.index, nrc_means.values, color='skyblue', alpha=0.8)\n",
    "    axes[0].set_title('NRCLex Emotions (Average Frequency)')\n",
    "    axes[0].set_xlabel('Frequency')\n",
    "    \n",
    "    # Emozioni dominanti\n",
    "    nrc_dominant_counts = df['nrc_dominant_emotion'].value_counts().head(10)\n",
    "    axes[1].bar(nrc_dominant_counts.index, nrc_dominant_counts.values, \n",
    "                color='mediumseagreen', alpha=0.8)\n",
    "    axes[1].set_title('NRCLex Dominant Emotion')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\").replace(\"/\", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_temporal_sentiment(df, date_col='text_date', title=\"Temporal Sentiment\"):\n",
    "    \"\"\"Analisi temporale del sentiment\"\"\"\n",
    "    if date_col not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Colonna {date_col} non trovata\")\n",
    "        return\n",
    "    \n",
    "    df_temp = df.copy()\n",
    "    df_temp[date_col] = pd.to_datetime(df_temp[date_col], errors='coerce')\n",
    "    df_temp = df_temp.dropna(subset=[date_col])\n",
    "    \n",
    "    if len(df_temp) == 0:\n",
    "        print(\"‚ö†Ô∏è Nessuna data valida trovata\")\n",
    "        return\n",
    "    \n",
    "    df_temp['date_only'] = df_temp[date_col].dt.date\n",
    "    \n",
    "    daily = df_temp.groupby('date_only').agg({\n",
    "        'vader_compound': 'mean',\n",
    "        'textblob_polarity': 'mean',\n",
    "        'polarity_ensemble': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    ax.plot(daily['date_only'], daily['vader_compound'], label='VADER', marker='o', alpha=0.7)\n",
    "    ax.plot(daily['date_only'], daily['textblob_polarity'], label='TextBlob', marker='s', alpha=0.7)\n",
    "    ax.plot(daily['date_only'], daily['polarity_ensemble'], label='Ensemble', marker='^', linewidth=2)\n",
    "    ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "    ax.set_ylabel('Polarity Score')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\").replace(\"/\", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def print_statistics(df, category_name=\"Dataset\"):\n",
    "    \"\"\"Stampa statistiche descrittive\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìä STATISTICS - {category_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüéØ SENTIMENT DISTRIBUTION:\")\n",
    "    sentiment_pct = df['sentiment_ensemble'].value_counts(normalize=True) * 100\n",
    "    for label, pct in sentiment_pct.items():\n",
    "        print(f\"   {label.capitalize()}: {pct:.2f}%\")\n",
    "    \n",
    "    print(\"\\nüìà POLARITY SCORES:\")\n",
    "    print(f\"   VADER: {df['vader_compound'].mean():.3f} (std: {df['vader_compound'].std():.3f})\")\n",
    "    print(f\"   TextBlob: {df['textblob_polarity'].mean():.3f} (std: {df['textblob_polarity'].std():.3f})\")\n",
    "    print(f\"   Ensemble: {df['polarity_ensemble'].mean():.3f} (std: {df['polarity_ensemble'].std():.3f})\")\n",
    "    \n",
    "    agreement = (df['vader_label'] == df['textblob_label']).mean() * 100\n",
    "    print(f\"\\nü§ù MODEL AGREEMENT: {agreement:.2f}%\")\n",
    "    \n",
    "    print(\"\\nüí≠ SUBJECTIVITY:\")\n",
    "    print(f\"   Mean: {df['textblob_subjectivity'].mean():.3f}\")\n",
    "    print(f\"   Median: {df['textblob_subjectivity'].median():.3f}\")\n",
    "    \n",
    "    print(\"\\nüòä TOP 5 EMOTIONS (NRCLex):\")\n",
    "    top_emotions = df['nrc_dominant_emotion'].value_counts().head(5)\n",
    "    for emotion, count in top_emotions.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"   {emotion.capitalize()}: {count} ({pct:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# ==========================================\n",
    "# 4. PIPELINE COMPLETA\n",
    "# ==========================================\n",
    "\n",
    "def analyze_complete_sentiment(df, category_name=\"Dataset\"):\n",
    "    \"\"\"Pipeline completa sentiment analysis (solo lessicale)\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ SENTIMENT ANALYSIS: {category_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Dataset: {len(df)} post/commenti\")\n",
    "    print(\"Metodi: VADER + TextBlob + NRCLex\")\n",
    "    \n",
    "    # Analisi\n",
    "    vader_results = analyze_vader(df)\n",
    "    textblob_results = analyze_textblob(df)\n",
    "    nrc_results = analyze_nrclex(df)\n",
    "    \n",
    "    # Merge\n",
    "    results_df = pd.concat([\n",
    "        df.reset_index(drop=True),\n",
    "        vader_results, \n",
    "        textblob_results,\n",
    "        nrc_results\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Aggregazione\n",
    "    results_df = aggregate_sentiments(results_df)\n",
    "    \n",
    "    # Output\n",
    "    print_statistics(results_df, category_name)\n",
    "    plot_sentiment_distribution(results_df, f\"{category_name} - Sentiment\")\n",
    "    plot_emotions_distribution(results_df, f\"{category_name} - Emotions\")\n",
    "    \n",
    "    if 'text_date' in results_df.columns:\n",
    "        plot_temporal_sentiment(results_df, date_col='text_date', title=f\"{category_name} - Temporal\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ COMPLETATO: {category_name}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ==========================================\n",
    "# 5. CONFRONTI\n",
    "# ==========================================\n",
    "\n",
    "def compare_two_groups(df1, df2, name1=\"Group 1\", name2=\"Group 2\", save_prefix=\"comparison\"):\n",
    "    \"\"\"Confronta due gruppi generici\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚öñÔ∏è  {name1.upper()} vs {name2.upper()} COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle(f'{name1} vs {name2} - Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Polarity\n",
    "    data_polarity = pd.DataFrame({\n",
    "        name1: df1['polarity_ensemble'],\n",
    "        name2: df2['polarity_ensemble']\n",
    "    })\n",
    "    data_polarity.boxplot(ax=axes[0, 0])\n",
    "    axes[0, 0].axhline(0, color='red', linestyle='--')\n",
    "    axes[0, 0].set_title('Polarity Comparison')\n",
    "    axes[0, 0].set_ylabel('Ensemble Score')\n",
    "    \n",
    "    # Sentiment labels\n",
    "    sent1 = df1['sentiment_ensemble'].value_counts(normalize=True) * 100\n",
    "    sent2 = df2['sentiment_ensemble'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    x = np.arange(3)\n",
    "    width = 0.35\n",
    "    labels = ['positive', 'neutral', 'negative']\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, [sent1.get(l, 0) for l in labels], width, label=name1, alpha=0.8)\n",
    "    axes[0, 1].bar(x + width/2, [sent2.get(l, 0) for l in labels], width, label=name2, alpha=0.8)\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(labels)\n",
    "    axes[0, 1].set_ylabel('Percentage (%)')\n",
    "    axes[0, 1].set_title('Sentiment Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Emotions\n",
    "    emo1 = df1['nrc_dominant_emotion'].value_counts().head(5)\n",
    "    emo2 = df2['nrc_dominant_emotion'].value_counts().head(5)\n",
    "    \n",
    "    all_emotions = list(set(emo1.index) | set(emo2.index))\n",
    "    x_emo = np.arange(len(all_emotions))\n",
    "    \n",
    "    axes[1, 0].barh(x_emo - width/2, [emo1.get(e, 0) for e in all_emotions], width, label=name1)\n",
    "    axes[1, 0].barh(x_emo + width/2, [emo2.get(e, 0) for e in all_emotions], width, label=name2)\n",
    "    axes[1, 0].set_yticks(x_emo)\n",
    "    axes[1, 0].set_yticklabels(all_emotions)\n",
    "    axes[1, 0].set_xlabel('Count')\n",
    "    axes[1, 0].set_title('Top Emotions')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Stats table\n",
    "    stats_data = {\n",
    "        'Metric': ['Polarity Mean', 'Polarity Std', '% Positive', '% Negative', 'Subjectivity'],\n",
    "        name1: [\n",
    "            f\"{df1['polarity_ensemble'].mean():.3f}\",\n",
    "            f\"{df1['polarity_ensemble'].std():.3f}\",\n",
    "            f\"{(df1['sentiment_ensemble']=='positive').mean()*100:.1f}%\",\n",
    "            f\"{(df1['sentiment_ensemble']=='negative').mean()*100:.1f}%\",\n",
    "            f\"{df1['textblob_subjectivity'].mean():.3f}\"\n",
    "        ],\n",
    "        name2: [\n",
    "            f\"{df2['polarity_ensemble'].mean():.3f}\",\n",
    "            f\"{df2['polarity_ensemble'].std():.3f}\",\n",
    "            f\"{(df2['sentiment_ensemble']=='positive').mean()*100:.1f}%\",\n",
    "            f\"{(df2['sentiment_ensemble']=='negative').mean()*100:.1f}%\",\n",
    "            f\"{df2['textblob_subjectivity'].mean():.3f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    axes[1, 1].axis('off')\n",
    "    table = axes[1, 1].table(cellText=stats_df.values, colLabels=stats_df.columns,\n",
    "                            cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 2)\n",
    "    axes[1, 1].set_title('Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_prefix}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Confronto {name1} vs {name2} completato!\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. ESECUZIONE MULTI-PIATTAFORMA\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nüöÄ SENTIMENT ANALYSIS - MULTI-PLATFORM (Twitter + Reddit)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CARICA IL TUO DATASET PROCESSATO\n",
    "df = df_processed.copy()\n",
    "\n",
    "# Pulizia dati\n",
    "print(f\"\\nüìä Dataset totale: {len(df)} righe\")\n",
    "df['text_lemmatized'] = df['text_lemmatized'].fillna(\"\").astype(str)\n",
    "df['text_nostop'] = df['text_nostop'].fillna(\"\").astype(str)\n",
    "df = df[df['text_lemmatized'].str.strip() != \"\"]\n",
    "print(f\"üìä Dopo pulizia: {len(df)} righe\")\n",
    "\n",
    "# Verifica colonna site\n",
    "if 'site' not in df.columns:\n",
    "    print(\"‚ö†Ô∏è ATTENZIONE: Colonna 'site' non trovata! Assumo tutti i dati siano da Twitter.\")\n",
    "    df['site'] = 'twitter'\n",
    "\n",
    "# Statistiche piattaforme\n",
    "print(f\"\\nüì± DISTRIBUZIONE PER PIATTAFORMA:\")\n",
    "print(df['site'].value_counts())\n",
    "\n",
    "# Dividi per PIATTAFORMA\n",
    "df_twitter = df[df['site'].str.lower().isin(['nitter', 'twitter'])].copy()\n",
    "df_reddit = df[df['site'].str.lower() == 'reddit'].copy()\n",
    "\n",
    "print(f\"\\nüìä Twitter/Nitter posts: {len(df_twitter)}\")\n",
    "print(f\"üìä Reddit posts: {len(df_reddit)}\")\n",
    "\n",
    "# Dividi per ARGOMENTO\n",
    "df_bitcoin = df[df['argument'] == 'Bitcoin'].copy()\n",
    "df_nvidia = df[df['argument'] == 'Nvidia'].copy()\n",
    "\n",
    "print(f\"\\nüìä Bitcoin posts (totali): {len(df_bitcoin)}\")\n",
    "print(f\"üìä Nvidia posts (totali): {len(df_nvidia)}\")\n",
    "\n",
    "# Dividi per ARGOMENTO + PIATTAFORMA\n",
    "df_bitcoin_twitter = df[(df['argument'] == 'Bitcoin') & (df['site'].str.lower().isin(['nitter', 'twitter']))].copy()\n",
    "df_bitcoin_reddit = df[(df['argument'] == 'Bitcoin') & (df['site'].str.lower() == 'reddit')].copy()\n",
    "df_nvidia_twitter = df[(df['argument'] == 'Nvidia') & (df['site'].str.lower().isin(['nitter', 'twitter']))].copy()\n",
    "df_nvidia_reddit = df[(df['argument'] == 'Nvidia') & (df['site'].str.lower() == 'reddit')].copy()\n",
    "\n",
    "print(f\"\\nüìä Bitcoin Twitter: {len(df_bitcoin_twitter)}\")\n",
    "print(f\"üìä Bitcoin Reddit: {len(df_bitcoin_reddit)}\")\n",
    "print(f\"üìä Nvidia Twitter: {len(df_nvidia_twitter)}\")\n",
    "print(f\"üìä Nvidia Reddit: {len(df_nvidia_reddit)}\")\n",
    "\n",
    "# ==========================================\n",
    "# ANALISI COMPLETE\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INIZIO ANALISI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Analisi per ARGOMENTO (tutti i dati)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1Ô∏è‚É£ ANALISI PER ARGOMENTO (Twitter + Reddit)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_bitcoin_all = analyze_complete_sentiment(df_bitcoin, \"Bitcoin (All Platforms)\")\n",
    "results_nvidia_all = analyze_complete_sentiment(df_nvidia, \"Nvidia (All Platforms)\")\n",
    "\n",
    "# 2. Analisi per PIATTAFORMA (tutti gli argomenti)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2Ô∏è‚É£ ANALISI PER PIATTAFORMA (Bitcoin + Nvidia)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df_twitter) > 0:\n",
    "    results_twitter_all = analyze_complete_sentiment(df_twitter, \"Twitter (All Topics)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nessun dato Twitter trovato\")\n",
    "    \n",
    "if len(df_reddit) > 0:\n",
    "    results_reddit_all = analyze_complete_sentiment(df_reddit, \"Reddit (All Topics)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nessun dato Reddit trovato\")\n",
    "\n",
    "# 3. Analisi DETTAGLIATE (Argomento x Piattaforma)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3Ô∏è‚É£ ANALISI DETTAGLIATE (Argomento x Piattaforma)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df_bitcoin_twitter) > 0:\n",
    "    results_bitcoin_twitter = analyze_complete_sentiment(df_bitcoin_twitter, \"Bitcoin/Twitter\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nessun dato Bitcoin/Twitter\")\n",
    "    \n",
    "if len(df_bitcoin_reddit) > 0:\n",
    "    results_bitcoin_reddit = analyze_complete_sentiment(df_bitcoin_reddit, \"Bitcoin/Reddit\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nessun dato Bitcoin/Reddit\")\n",
    "    \n",
    "if len(df_nvidia_twitter) > 0:\n",
    "    results_nvidia_twitter = analyze_complete_sentiment(df_nvidia_twitter, \"Nvidia/Twitter\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nessun dato Nvidia/Twitter\")\n",
    "    \n",
    "if len(df_nvidia_reddit) > 0:\n",
    "    results_nvidia_reddit = analyze_complete_sentiment(df_nvidia_reddit, \"Nvidia/Reddit\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nessun dato Nvidia/Reddit\")\n",
    "\n",
    "# ==========================================\n",
    "# CONFRONTI\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4Ô∏è‚É£ CONFRONTI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Confronto Bitcoin vs Nvidia (tutti i dati)\n",
    "compare_two_groups(results_bitcoin_all, results_nvidia_all, \n",
    "                   \"Bitcoin\", \"Nvidia\", \"comparison_bitcoin_vs_nvidia\")\n",
    "\n",
    "# Confronto Twitter vs Reddit (tutti i dati)\n",
    "if len(df_twitter) > 0 and len(df_reddit) > 0:\n",
    "    compare_two_groups(results_twitter_all, results_reddit_all, \n",
    "                       \"Twitter\", \"Reddit\", \"comparison_twitter_vs_reddit\")\n",
    "\n",
    "# Confronto Bitcoin: Twitter vs Reddit\n",
    "if len(df_bitcoin_twitter) > 0 and len(df_bitcoin_reddit) > 0:\n",
    "    compare_two_groups(results_bitcoin_twitter, results_bitcoin_reddit, \n",
    "                       \"Bitcoin/Twitter\", \"Bitcoin/Reddit\", \"comparison_bitcoin_twitter_vs_reddit\")\n",
    "\n",
    "# Confronto Nvidia: Twitter vs Reddit\n",
    "if len(df_nvidia_twitter) > 0 and len(df_nvidia_reddit) > 0:\n",
    "    compare_two_groups(results_nvidia_twitter, results_nvidia_reddit, \n",
    "                       \"Nvidia/Twitter\", \"Nvidia/Reddit\", \"comparison_nvidia_twitter_vs_reddit\")\n",
    "\n",
    "# ==========================================\n",
    "# SALVATAGGIO RISULTATI\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nüíæ Salvataggio risultati in formato CSV (pi√π leggero)...\")\n",
    "\n",
    "# Salva tutti i risultati in CSV\n",
    "try:\n",
    "    results_bitcoin_all.to_csv(\"results_bitcoin_all.csv\", index=False)\n",
    "    print(\"   ‚úÖ Salvato: results_bitcoin_all.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Errore salvando Bitcoin: {e}\")\n",
    "\n",
    "try:\n",
    "    results_nvidia_all.to_csv(\"results_nvidia_all.csv\", index=False)\n",
    "    print(\"   ‚úÖ Salvato: results_nvidia_all.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Errore salvando Nvidia: {e}\")\n",
    "\n",
    "if len(df_twitter) > 0:\n",
    "    try:\n",
    "        results_twitter_all.to_csv(\"results_twitter_all.csv\", index=False)\n",
    "        print(\"   ‚úÖ Salvato: results_twitter_all.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Errore salvando Twitter: {e}\")\n",
    "\n",
    "if len(df_reddit) > 0:\n",
    "    try:\n",
    "        results_reddit_all.to_csv(\"results_reddit_all.csv\", index=False)\n",
    "        print(\"   ‚úÖ Salvato: results_reddit_all.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Errore salvando Reddit: {e}\")\n",
    "\n",
    "if len(df_bitcoin_twitter) > 0:\n",
    "    try:\n",
    "        results_bitcoin_twitter.to_csv(\"results_bitcoin_twitter.csv\", index=False)\n",
    "        print(\"   ‚úÖ Salvato: results_bitcoin_twitter.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Errore salvando Bitcoin/Twitter: {e}\")\n",
    "\n",
    "if len(df_bitcoin_reddit) > 0:\n",
    "    try:\n",
    "        results_bitcoin_reddit.to_csv(\"results_bitcoin_reddit.csv\", index=False)\n",
    "        print(\"   ‚úÖ Salvato: results_bitcoin_reddit.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Errore salvando Bitcoin/Reddit: {e}\")\n",
    "\n",
    "if len(df_nvidia_twitter) > 0:\n",
    "    try:\n",
    "        results_nvidia_twitter.to_csv(\"results_nvidia_twitter.csv\", index=False)\n",
    "        print(\"   ‚úÖ Salvato: results_nvidia_twitter.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Errore salvando Nvidia/Twitter: {e}\")\n",
    "\n",
    "if len(df_nvidia_reddit) > 0:\n",
    "    try:\n",
    "        results_nvidia_reddit.to_csv(\"results_nvidia_reddit.csv\", index=False)\n",
    "        print(\"   ‚úÖ Salvato: results_nvidia_reddit.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Errore salvando Nvidia/Reddit: {e}\")\n",
    "\n",
    "print(\"\\nüéâ ANALISI COMPLETA TERMINATA!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìÅ FILE SALVATI (CSV):\")\n",
    "print(\"\\nüìä RISULTATI PRINCIPALI:\")\n",
    "print(\"   ‚úÖ results_bitcoin_all.csv\")\n",
    "print(\"   ‚úÖ results_nvidia_all.csv\")\n",
    "\n",
    "print(\"\\nüåê RISULTATI PER PIATTAFORMA:\")\n",
    "if len(df_twitter) > 0:\n",
    "    print(\"   ‚úÖ results_twitter_all.csv\")\n",
    "if len(df_reddit) > 0:\n",
    "    print(\"   ‚úÖ results_reddit_all.csv\")\n",
    "\n",
    "print(\"\\nüîç RISULTATI DETTAGLIATI:\")\n",
    "if len(df_bitcoin_twitter) > 0:\n",
    "    print(\"   ‚úÖ results_bitcoin_twitter.csv\")\n",
    "if len(df_bitcoin_reddit) > 0:\n",
    "    print(\"   ‚úÖ results_bitcoin_reddit.csv\")\n",
    "if len(df_nvidia_twitter) > 0:\n",
    "    print(\"   ‚úÖ results_nvidia_twitter.csv\")\n",
    "if len(df_nvidia_reddit) > 0:\n",
    "    print(\"   ‚úÖ results_nvidia_reddit.csv\")\n",
    "\n",
    "print(\"\\nüìà GRAFICI PNG:\")\n",
    "print(\"   ‚úÖ Tutti i grafici sentiment/emotions/temporal per ogni categoria\")\n",
    "print(\"   ‚úÖ comparison_bitcoin_vs_nvidia.png\")\n",
    "if len(df_twitter) > 0 and len(df_reddit) > 0:\n",
    "    print(\"   ‚úÖ comparison_twitter_vs_reddit.png\")\n",
    "if len(df_bitcoin_twitter) > 0 and len(df_bitcoin_reddit) > 0:\n",
    "    print(\"   ‚úÖ comparison_bitcoin_twitter_vs_reddit.png\")\n",
    "if len(df_nvidia_twitter) > 0 and len(df_nvidia_reddit) > 0:\n",
    "    print(\"   ‚úÖ comparison_nvidia_twitter_vs_reddit.png\")\n",
    "\n",
    "print(\"\\nüí° INFO:\")\n",
    "print(\"   üìÅ I file CSV occupano ~70% meno spazio degli Excel\")\n",
    "print(\"   üìñ Apribili con Excel, Google Sheets, o pandas\")\n",
    "print(\"   üîÑ Per convertire in Excel: pd.read_csv('file.csv').to_excel('file.xlsx')\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34078cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBITO DOPO aver caricato df_processed (nel primo script)\n",
    "print(\"\\nüîç DIAGNOSI CARICAMENTO:\")\n",
    "print(f\"Totale righe caricate: {len(df)}\")\n",
    "print(\"\\nDistribuzione ORIGINALE:\")\n",
    "print(df.groupby(['argument', 'site']).size())\n",
    "\n",
    "# DOPO il fillna e il filtro delle righe vuote\n",
    "df['text_lemmatized'] = df['text_lemmatized'].fillna(\"\")\n",
    "df['text_nostop'] = df['text_nostop'].fillna(\"\")\n",
    "df = df[df['text_lemmatized'].str.strip() != \"\"]\n",
    "\n",
    "print(\"\\nDistribuzione DOPO filtro righe vuote:\")\n",
    "print(df.groupby(['argument', 'site']).size())\n",
    "\n",
    "# DOPO aver diviso per categoria\n",
    "df_bitcoin = df[df['argument'] == 'Bitcoin'].copy()\n",
    "df_nvidia = df[df['argument'] == 'Nvidia'].copy()\n",
    "\n",
    "print(f\"\\nüìä Bitcoin: {len(df_bitcoin)}\")\n",
    "print(df_bitcoin['site'].value_counts())\n",
    "print(f\"\\nüìä Nvidia: {len(df_nvidia)}\")\n",
    "print(df_nvidia['site'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
